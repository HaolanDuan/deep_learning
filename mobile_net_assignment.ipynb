{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc75787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f79a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([40000, 3, 32, 32])\n",
      "Shape of y_train: torch.Size([40000])\n",
      "Shape of X_val:  torch.Size([10000, 3, 32, 32])\n",
      "Shape of y_val:  torch.Size([10000])\n",
      "Shape of X_test:  torch.Size([10000, 3, 32, 32])\n",
      "Shape of y_test:  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "import data_process\n",
    "data_dict = data_process.preprocess_cifar100(\n",
    "    cuda=True,dtype=torch.float64, flatten=False\n",
    ")\n",
    "\n",
    "print(\"Shape of X_train:\", data_dict[\"X_train\"].shape)\n",
    "print(\"Shape of y_train:\", data_dict[\"y_train\"].shape)\n",
    "print(\"Shape of X_val: \", data_dict[\"X_val\"].shape)\n",
    "print(\"Shape of y_val: \", data_dict[\"y_val\"].shape)\n",
    "print(\"Shape of X_test: \", data_dict[\"X_test\"].shape)\n",
    "print(\"Shape of y_test: \", data_dict[\"y_test\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3af69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of RGB of training set is:  tensor([0.5071, 0.4865, 0.4409], device='cuda:0', dtype=torch.float64)\n",
      "The std of RGB of training set is:  tensor([0.2673, 0.2564, 0.2762], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean and std of the whole training set \n",
    "# Before splitting into train and val\n",
    "\n",
    "origin_train = torch.cat((data_dict[\"X_train\"], data_dict[\"X_val\"]), dim=0)\n",
    "mean, std = data_process.compute_mean_std(origin_train)\n",
    "print(\"The mean of RGB of training set is: \", mean)\n",
    "print(\"The std of RGB of training set is: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c39d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  2.0000,  3.0000,  4.0000,  5.0000,  6.0000,  7.0000,\n",
      "          8.0000,  9.0000, 10.0000, 11.0000, 12.0000, 13.0000, 14.0000, 15.0000,\n",
      "         16.0000, 17.0000, 18.0000, 19.0000, 20.0000, 21.0000, 22.0000, 23.0000,\n",
      "         24.0000, 25.0000, 26.0000, 27.0000, 28.0000, 29.0000, 30.0000, 31.0000,\n",
      "         32.0000, 33.0000, 34.0000, 35.0000, 36.0000, 37.0000, 38.0000, 39.0000,\n",
      "         40.0000, 41.0000, 42.0000, 43.0000, 44.0000, 45.0000, 46.0000, 47.0000,\n",
      "         48.0000, 49.0000, 50.0000, 51.0000, 52.0000, 53.0000, 54.0000, 55.0000,\n",
      "         56.0000, 57.0000, 58.0000, 59.0000, 60.0000, 61.0000, 62.0000, 63.0000,\n",
      "         64.0000, 65.0000, 66.0000, 67.0000, 68.0000, 69.0000, 70.0000, 71.0000,\n",
      "         72.0000, 73.0000, 74.0000, 75.0000, 76.0000, 77.0000, 78.0000, 79.0000,\n",
      "         80.0000, 81.0000, 82.0000, 83.0000, 84.0000, 85.0000, 86.0000, 87.0000,\n",
      "         88.0000, 89.0000, 90.0000, 91.0000, 92.0000, 93.0000, 94.0000, 95.0000,\n",
      "         96.0000, 97.0000, 98.0000, 99.0000],\n",
      "        [ 0.9925,  0.9675,  1.0050,  0.9950,  0.9725,  0.9775,  1.0100,  0.9775,\n",
      "          1.0575,  0.9950,  0.9350,  0.9950,  0.9950,  0.9825,  1.0375,  0.9825,\n",
      "          1.0350,  1.0200,  1.0325,  1.0400,  0.9950,  1.0225,  1.0300,  0.9900,\n",
      "          0.9875,  0.9650,  0.9900,  0.9950,  0.9550,  0.9750,  0.9775,  1.0325,\n",
      "          1.0425,  1.0300,  1.0225,  1.0025,  1.0025,  0.9750,  1.0200,  0.9775,\n",
      "          1.0225,  0.9900,  1.0250,  1.0325,  0.9900,  0.9850,  0.9850,  0.9700,\n",
      "          0.9975,  1.0075,  0.9700,  0.9875,  0.9575,  0.9725,  1.0375,  0.9900,\n",
      "          0.9950,  0.9900,  1.0325,  1.0350,  1.0400,  1.0350,  0.9975,  1.0050,\n",
      "          1.0050,  1.0200,  1.0325,  1.0275,  1.0150,  1.0250,  1.0125,  1.0425,\n",
      "          1.0300,  0.9950,  0.9875,  0.9850,  0.9875,  0.9875,  1.0200,  0.9850,\n",
      "          0.9675,  1.0200,  0.9875,  1.0025,  0.9775,  0.9925,  0.9725,  1.0125,\n",
      "          0.9825,  1.0125,  0.9800,  0.9650,  1.0350,  1.0125,  0.9600,  0.9800,\n",
      "          0.9650,  0.9650,  0.9950,  1.0150]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show the proportion of each class in the new X_train\n",
    "counts = torch.bincount(data_dict[\"y_train\"])\n",
    "proportions = counts / 40000 * 100.0\n",
    "labels = data_dict[\"y_train\"].unique()\n",
    "proportion_result = torch.stack((labels, proportions), dim=0)\n",
    "print(proportion_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed1bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Try the DataLoaders for MobileNet training (Refer to the ResNet notebook)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='.', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='.', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# we can use a larger batch size during test, because we do not save \n",
    "# intermediate variables for gradient computation, which leaves more memory\n",
    "\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "trainset_new, valset = torch.utils.data.random_split(trainset, (0.8, 0.2), generator=generator)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset_new, batch_size=128, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159639bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet Model\n",
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "287739ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index,:]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c325547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net, criterion, trainloader, scheduler, alpha):\n",
    "    device = 'cuda'\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        train_loss += loss.data.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += (lam * predicted.eq(targets_a.data).sum().item()\n",
    "                    + (1 - lam) * predicted.eq(targets_b.data).sum().item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
    "\n",
    "    scheduler.step()\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a8f49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch, net, criterion, valloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return val_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7a6ef017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net, criterion, testloader):\n",
    "    device = 'cuda'\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e8611ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(net, acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, ('./checkpoint/ckpt%3d.pth') % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3822fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "iteration :  50, loss : 2.3264, accuracy : 15.37\n",
      "iteration : 100, loss : 2.1711, accuracy : 20.48\n",
      "iteration : 150, loss : 2.0759, accuracy : 23.96\n",
      "iteration : 200, loss : 2.0219, accuracy : 26.21\n",
      "iteration : 250, loss : 1.9807, accuracy : 27.77\n",
      "iteration : 300, loss : 1.9467, accuracy : 29.23\n",
      "Saving..\n",
      "Epoch :   0, training loss : 1.9380, training accuracy : 29.53, val loss : 1.5947, val accuracy : 41.29\n",
      "\n",
      "Epoch: 1\n",
      "iteration :  50, loss : 1.7357, accuracy : 38.41\n",
      "iteration : 100, loss : 1.7034, accuracy : 39.41\n",
      "iteration : 150, loss : 1.6834, accuracy : 40.16\n",
      "iteration : 200, loss : 1.6724, accuracy : 40.64\n",
      "iteration : 250, loss : 1.6681, accuracy : 40.89\n",
      "iteration : 300, loss : 1.6553, accuracy : 41.52\n",
      "Saving..\n",
      "Epoch :   1, training loss : 1.6476, training accuracy : 41.76, val loss : 1.4430, val accuracy : 48.06\n",
      "\n",
      "Epoch: 2\n",
      "iteration :  50, loss : 1.6147, accuracy : 43.67\n",
      "iteration : 100, loss : 1.6303, accuracy : 43.36\n",
      "iteration : 150, loss : 1.6091, accuracy : 44.24\n",
      "iteration : 200, loss : 1.5869, accuracy : 44.95\n",
      "iteration : 250, loss : 1.5710, accuracy : 45.64\n",
      "iteration : 300, loss : 1.5657, accuracy : 45.95\n",
      "Saving..\n",
      "Epoch :   2, training loss : 1.5604, training accuracy : 46.12, val loss : 1.2923, val accuracy : 53.58\n",
      "\n",
      "Epoch: 3\n",
      "iteration :  50, loss : 1.4450, accuracy : 50.73\n",
      "iteration : 100, loss : 1.4282, accuracy : 51.33\n",
      "iteration : 150, loss : 1.4283, accuracy : 51.43\n",
      "iteration : 200, loss : 1.4255, accuracy : 51.62\n",
      "iteration : 250, loss : 1.4127, accuracy : 52.14\n",
      "iteration : 300, loss : 1.4101, accuracy : 52.38\n",
      "Saving..\n",
      "Epoch :   3, training loss : 1.4025, training accuracy : 52.60, val loss : 1.1721, val accuracy : 59.11\n",
      "\n",
      "Epoch: 4\n",
      "iteration :  50, loss : 1.3312, accuracy : 56.07\n",
      "iteration : 100, loss : 1.3040, accuracy : 56.89\n",
      "iteration : 150, loss : 1.3238, accuracy : 56.19\n",
      "iteration : 200, loss : 1.3451, accuracy : 55.81\n",
      "iteration : 250, loss : 1.3390, accuracy : 56.00\n",
      "iteration : 300, loss : 1.3166, accuracy : 56.68\n",
      "Saving..\n",
      "Epoch :   4, training loss : 1.3129, training accuracy : 56.71, val loss : 1.0596, val accuracy : 63.32\n",
      "\n",
      "Epoch: 5\n",
      "iteration :  50, loss : 1.2075, accuracy : 60.33\n",
      "iteration : 100, loss : 1.2831, accuracy : 58.16\n",
      "iteration : 150, loss : 1.2682, accuracy : 58.69\n",
      "iteration : 200, loss : 1.2635, accuracy : 58.90\n",
      "iteration : 250, loss : 1.2682, accuracy : 58.84\n",
      "iteration : 300, loss : 1.2549, accuracy : 59.33\n",
      "Saving..\n",
      "Epoch :   5, training loss : 1.2538, training accuracy : 59.39, val loss : 0.9680, val accuracy : 67.44\n",
      "\n",
      "Epoch: 6\n",
      "iteration :  50, loss : 1.2107, accuracy : 61.33\n",
      "iteration : 100, loss : 1.1731, accuracy : 62.45\n",
      "iteration : 150, loss : 1.1846, accuracy : 62.15\n",
      "iteration : 200, loss : 1.1679, accuracy : 62.61\n",
      "iteration : 250, loss : 1.1714, accuracy : 62.51\n",
      "iteration : 300, loss : 1.1494, accuracy : 63.18\n",
      "Saving..\n",
      "Epoch :   6, training loss : 1.1509, training accuracy : 63.13, val loss : 0.9587, val accuracy : 68.02\n",
      "\n",
      "Epoch: 7\n",
      "iteration :  50, loss : 1.1675, accuracy : 63.25\n",
      "iteration : 100, loss : 1.1634, accuracy : 63.66\n",
      "iteration : 150, loss : 1.1239, accuracy : 64.91\n",
      "iteration : 200, loss : 1.1327, accuracy : 64.57\n",
      "iteration : 250, loss : 1.1363, accuracy : 64.46\n",
      "iteration : 300, loss : 1.1251, accuracy : 64.78\n",
      "Saving..\n",
      "Epoch :   7, training loss : 1.1231, training accuracy : 64.83, val loss : 0.8655, val accuracy : 71.37\n",
      "\n",
      "Epoch: 8\n",
      "iteration :  50, loss : 1.0269, accuracy : 67.93\n",
      "iteration : 100, loss : 1.0746, accuracy : 65.97\n",
      "iteration : 150, loss : 1.0763, accuracy : 66.16\n",
      "iteration : 200, loss : 1.1014, accuracy : 65.51\n",
      "iteration : 250, loss : 1.0843, accuracy : 66.07\n",
      "iteration : 300, loss : 1.0731, accuracy : 66.31\n",
      "Saving..\n",
      "Epoch :   8, training loss : 1.0638, training accuracy : 66.60, val loss : 0.8266, val accuracy : 72.01\n",
      "\n",
      "Epoch: 9\n",
      "iteration :  50, loss : 1.0535, accuracy : 66.82\n",
      "iteration : 100, loss : 1.0091, accuracy : 68.58\n",
      "iteration : 150, loss : 1.0202, accuracy : 68.19\n",
      "iteration : 200, loss : 1.0166, accuracy : 68.18\n",
      "iteration : 250, loss : 1.0143, accuracy : 68.20\n",
      "iteration : 300, loss : 1.0054, accuracy : 68.44\n",
      "Saving..\n",
      "Epoch :   9, training loss : 1.0043, training accuracy : 68.45, val loss : 0.7494, val accuracy : 75.18\n",
      "\n",
      "Epoch: 10\n",
      "iteration :  50, loss : 1.0115, accuracy : 69.10\n",
      "iteration : 100, loss : 1.0406, accuracy : 68.15\n",
      "iteration : 150, loss : 1.0530, accuracy : 67.56\n",
      "iteration : 200, loss : 1.0555, accuracy : 67.61\n",
      "iteration : 250, loss : 1.0500, accuracy : 67.74\n",
      "iteration : 300, loss : 1.0231, accuracy : 68.52\n",
      "Saving..\n",
      "Epoch :  10, training loss : 1.0156, training accuracy : 68.73, val loss : 0.7296, val accuracy : 75.78\n",
      "\n",
      "Epoch: 11\n",
      "iteration :  50, loss : 1.0191, accuracy : 68.90\n",
      "iteration : 100, loss : 0.9786, accuracy : 69.94\n",
      "iteration : 150, loss : 0.9689, accuracy : 70.27\n",
      "iteration : 200, loss : 0.9705, accuracy : 70.28\n",
      "iteration : 250, loss : 0.9729, accuracy : 70.11\n",
      "iteration : 300, loss : 0.9630, accuracy : 70.44\n",
      "Saving..\n",
      "Epoch :  11, training loss : 0.9646, training accuracy : 70.43, val loss : 0.7267, val accuracy : 75.84\n",
      "\n",
      "Epoch: 12\n",
      "iteration :  50, loss : 0.8939, accuracy : 72.24\n",
      "iteration : 100, loss : 0.9430, accuracy : 70.58\n",
      "iteration : 150, loss : 0.9278, accuracy : 71.13\n",
      "iteration : 200, loss : 0.9332, accuracy : 71.02\n",
      "iteration : 250, loss : 0.9337, accuracy : 71.13\n",
      "iteration : 300, loss : 0.9197, accuracy : 71.65\n",
      "Saving..\n",
      "Epoch :  12, training loss : 0.9246, training accuracy : 71.52, val loss : 0.6801, val accuracy : 77.75\n",
      "\n",
      "Epoch: 13\n",
      "iteration :  50, loss : 0.9438, accuracy : 70.74\n",
      "iteration : 100, loss : 0.9034, accuracy : 72.19\n",
      "iteration : 150, loss : 0.9164, accuracy : 71.86\n",
      "iteration : 200, loss : 0.9139, accuracy : 71.94\n",
      "iteration : 250, loss : 0.9266, accuracy : 71.54\n",
      "iteration : 300, loss : 0.9318, accuracy : 71.33\n",
      "Saving..\n",
      "Epoch :  13, training loss : 0.9272, training accuracy : 71.54, val loss : 0.6509, val accuracy : 78.63\n",
      "\n",
      "Epoch: 14\n",
      "iteration :  50, loss : 0.7987, accuracy : 76.24\n",
      "iteration : 100, loss : 0.8492, accuracy : 74.27\n",
      "iteration : 150, loss : 0.8478, accuracy : 74.07\n",
      "iteration : 200, loss : 0.8605, accuracy : 73.86\n",
      "iteration : 250, loss : 0.8715, accuracy : 73.62\n",
      "iteration : 300, loss : 0.8794, accuracy : 73.18\n",
      "Saving..\n",
      "Epoch :  14, training loss : 0.8788, training accuracy : 73.21, val loss : 0.6516, val accuracy : 78.67\n",
      "\n",
      "Epoch: 15\n",
      "iteration :  50, loss : 0.9430, accuracy : 71.73\n",
      "iteration : 100, loss : 0.9241, accuracy : 72.00\n",
      "iteration : 150, loss : 0.9164, accuracy : 72.35\n",
      "iteration : 200, loss : 0.9175, accuracy : 72.11\n",
      "iteration : 250, loss : 0.9124, accuracy : 72.22\n",
      "iteration : 300, loss : 0.8990, accuracy : 72.63\n",
      "Saving..\n",
      "Epoch :  15, training loss : 0.8895, training accuracy : 72.88, val loss : 0.6031, val accuracy : 80.03\n",
      "\n",
      "Epoch: 16\n",
      "iteration :  50, loss : 0.8940, accuracy : 72.92\n",
      "iteration : 100, loss : 0.8767, accuracy : 73.14\n",
      "iteration : 150, loss : 0.8782, accuracy : 73.29\n",
      "iteration : 200, loss : 0.8894, accuracy : 73.08\n",
      "iteration : 250, loss : 0.8816, accuracy : 73.37\n",
      "iteration : 300, loss : 0.8768, accuracy : 73.54\n",
      "Epoch :  16, training loss : 0.8739, training accuracy : 73.60, val loss : 0.6271, val accuracy : 79.34\n",
      "\n",
      "Epoch: 17\n",
      "iteration :  50, loss : 0.9444, accuracy : 71.30\n",
      "iteration : 100, loss : 0.8988, accuracy : 72.52\n",
      "iteration : 150, loss : 0.8705, accuracy : 73.23\n",
      "iteration : 200, loss : 0.8694, accuracy : 73.38\n",
      "iteration : 250, loss : 0.8506, accuracy : 74.05\n",
      "iteration : 300, loss : 0.8445, accuracy : 74.33\n",
      "Epoch :  17, training loss : 0.8446, training accuracy : 74.30, val loss : 0.6254, val accuracy : 79.57\n",
      "\n",
      "Epoch: 18\n",
      "iteration :  50, loss : 0.8012, accuracy : 75.70\n",
      "iteration : 100, loss : 0.8147, accuracy : 75.33\n",
      "iteration : 150, loss : 0.7909, accuracy : 76.20\n",
      "iteration : 200, loss : 0.8102, accuracy : 75.50\n",
      "iteration : 250, loss : 0.8403, accuracy : 74.50\n",
      "iteration : 300, loss : 0.8468, accuracy : 74.42\n",
      "Saving..\n",
      "Epoch :  18, training loss : 0.8498, training accuracy : 74.35, val loss : 0.5694, val accuracy : 81.64\n",
      "\n",
      "Epoch: 19\n",
      "iteration :  50, loss : 0.7957, accuracy : 75.65\n",
      "iteration : 100, loss : 0.8253, accuracy : 74.55\n",
      "iteration : 150, loss : 0.8384, accuracy : 74.40\n",
      "iteration : 200, loss : 0.8362, accuracy : 74.50\n",
      "iteration : 250, loss : 0.8414, accuracy : 74.39\n",
      "iteration : 300, loss : 0.8524, accuracy : 74.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  19, training loss : 0.8601, training accuracy : 73.77, val loss : 0.6380, val accuracy : 79.72\n",
      "\n",
      "Epoch: 20\n",
      "iteration :  50, loss : 0.7311, accuracy : 77.86\n",
      "iteration : 100, loss : 0.7039, accuracy : 78.74\n",
      "iteration : 150, loss : 0.7200, accuracy : 78.55\n",
      "iteration : 200, loss : 0.7333, accuracy : 78.08\n",
      "iteration : 250, loss : 0.7649, accuracy : 76.99\n",
      "iteration : 300, loss : 0.7683, accuracy : 76.70\n",
      "Epoch :  20, training loss : 0.7670, training accuracy : 76.74, val loss : 0.5658, val accuracy : 81.48\n",
      "\n",
      "Epoch: 21\n",
      "iteration :  50, loss : 0.8000, accuracy : 75.88\n",
      "iteration : 100, loss : 0.7416, accuracy : 77.61\n",
      "iteration : 150, loss : 0.7528, accuracy : 77.28\n",
      "iteration : 200, loss : 0.7760, accuracy : 76.64\n",
      "iteration : 250, loss : 0.8039, accuracy : 75.60\n",
      "iteration : 300, loss : 0.8108, accuracy : 75.45\n",
      "Epoch :  21, training loss : 0.8142, training accuracy : 75.31, val loss : 0.5961, val accuracy : 81.14\n",
      "\n",
      "Epoch: 22\n",
      "iteration :  50, loss : 0.7373, accuracy : 78.78\n",
      "iteration : 100, loss : 0.7335, accuracy : 78.79\n",
      "iteration : 150, loss : 0.7352, accuracy : 78.49\n",
      "iteration : 200, loss : 0.7606, accuracy : 77.50\n",
      "iteration : 250, loss : 0.7623, accuracy : 77.40\n",
      "iteration : 300, loss : 0.7651, accuracy : 77.16\n",
      "Saving..\n",
      "Epoch :  22, training loss : 0.7683, training accuracy : 77.12, val loss : 0.5579, val accuracy : 81.99\n",
      "\n",
      "Epoch: 23\n",
      "iteration :  50, loss : 0.6864, accuracy : 79.00\n",
      "iteration : 100, loss : 0.7385, accuracy : 77.93\n",
      "iteration : 150, loss : 0.7487, accuracy : 77.61\n",
      "iteration : 200, loss : 0.7570, accuracy : 77.39\n",
      "iteration : 250, loss : 0.7517, accuracy : 77.56\n",
      "iteration : 300, loss : 0.7437, accuracy : 77.89\n",
      "Saving..\n",
      "Epoch :  23, training loss : 0.7417, training accuracy : 78.03, val loss : 0.5286, val accuracy : 82.95\n",
      "\n",
      "Epoch: 24\n",
      "iteration :  50, loss : 0.6689, accuracy : 79.50\n",
      "iteration : 100, loss : 0.7207, accuracy : 78.09\n",
      "iteration : 150, loss : 0.7297, accuracy : 77.94\n",
      "iteration : 200, loss : 0.7584, accuracy : 77.05\n",
      "iteration : 250, loss : 0.7606, accuracy : 77.19\n",
      "iteration : 300, loss : 0.7578, accuracy : 77.29\n",
      "Epoch :  24, training loss : 0.7513, training accuracy : 77.48, val loss : 0.6175, val accuracy : 79.55\n",
      "\n",
      "Epoch: 25\n",
      "iteration :  50, loss : 0.7473, accuracy : 77.33\n",
      "iteration : 100, loss : 0.7751, accuracy : 76.92\n",
      "iteration : 150, loss : 0.7347, accuracy : 78.00\n",
      "iteration : 200, loss : 0.7397, accuracy : 77.89\n",
      "iteration : 250, loss : 0.7264, accuracy : 78.31\n",
      "iteration : 300, loss : 0.7269, accuracy : 78.25\n",
      "Epoch :  25, training loss : 0.7296, training accuracy : 78.15, val loss : 0.5310, val accuracy : 82.60\n",
      "\n",
      "Epoch: 26\n",
      "iteration :  50, loss : 0.6955, accuracy : 78.72\n",
      "iteration : 100, loss : 0.7402, accuracy : 77.16\n",
      "iteration : 150, loss : 0.6889, accuracy : 79.14\n",
      "iteration : 200, loss : 0.6935, accuracy : 79.13\n",
      "iteration : 250, loss : 0.7102, accuracy : 78.69\n",
      "iteration : 300, loss : 0.7041, accuracy : 78.91\n",
      "Epoch :  26, training loss : 0.7067, training accuracy : 78.86, val loss : 0.5526, val accuracy : 82.22\n",
      "\n",
      "Epoch: 27\n",
      "iteration :  50, loss : 0.8451, accuracy : 75.65\n",
      "iteration : 100, loss : 0.8561, accuracy : 74.65\n",
      "iteration : 150, loss : 0.8573, accuracy : 74.38\n",
      "iteration : 200, loss : 0.8537, accuracy : 74.48\n",
      "iteration : 250, loss : 0.8125, accuracy : 75.76\n",
      "iteration : 300, loss : 0.8109, accuracy : 75.75\n",
      "Saving..\n",
      "Epoch :  27, training loss : 0.8154, training accuracy : 75.72, val loss : 0.5344, val accuracy : 83.22\n",
      "\n",
      "Epoch: 28\n",
      "iteration :  50, loss : 0.6610, accuracy : 80.92\n",
      "iteration : 100, loss : 0.7147, accuracy : 78.50\n",
      "iteration : 150, loss : 0.7276, accuracy : 78.07\n",
      "iteration : 200, loss : 0.7588, accuracy : 77.30\n",
      "iteration : 250, loss : 0.7497, accuracy : 77.61\n",
      "iteration : 300, loss : 0.7469, accuracy : 77.73\n",
      "Saving..\n",
      "Epoch :  28, training loss : 0.7592, training accuracy : 77.38, val loss : 0.5689, val accuracy : 83.41\n",
      "\n",
      "Epoch: 29\n",
      "iteration :  50, loss : 0.8089, accuracy : 76.15\n",
      "iteration : 100, loss : 0.7922, accuracy : 76.26\n",
      "iteration : 150, loss : 0.7945, accuracy : 76.50\n",
      "iteration : 200, loss : 0.7553, accuracy : 77.71\n",
      "iteration : 250, loss : 0.7539, accuracy : 77.75\n",
      "iteration : 300, loss : 0.7388, accuracy : 78.15\n",
      "Saving..\n",
      "Epoch :  29, training loss : 0.7368, training accuracy : 78.14, val loss : 0.4901, val accuracy : 84.40\n",
      "\n",
      "Epoch: 30\n",
      "iteration :  50, loss : 0.6826, accuracy : 80.11\n",
      "iteration : 100, loss : 0.6848, accuracy : 79.89\n",
      "iteration : 150, loss : 0.7178, accuracy : 79.13\n",
      "iteration : 200, loss : 0.7163, accuracy : 79.13\n",
      "iteration : 250, loss : 0.7235, accuracy : 78.81\n",
      "iteration : 300, loss : 0.7277, accuracy : 78.66\n",
      "Epoch :  30, training loss : 0.7338, training accuracy : 78.46, val loss : 0.5479, val accuracy : 82.39\n",
      "\n",
      "Epoch: 31\n",
      "iteration :  50, loss : 0.6576, accuracy : 81.61\n",
      "iteration : 100, loss : 0.6796, accuracy : 80.67\n",
      "iteration : 150, loss : 0.6945, accuracy : 79.87\n",
      "iteration : 200, loss : 0.6915, accuracy : 79.86\n",
      "iteration : 250, loss : 0.7041, accuracy : 79.38\n",
      "iteration : 300, loss : 0.7238, accuracy : 78.67\n",
      "Epoch :  31, training loss : 0.7141, training accuracy : 78.93, val loss : 0.4891, val accuracy : 83.58\n",
      "\n",
      "Epoch: 32\n",
      "iteration :  50, loss : 0.6614, accuracy : 81.03\n",
      "iteration : 100, loss : 0.7425, accuracy : 78.17\n",
      "iteration : 150, loss : 0.7271, accuracy : 78.48\n",
      "iteration : 200, loss : 0.7231, accuracy : 78.67\n",
      "iteration : 250, loss : 0.7219, accuracy : 78.76\n",
      "iteration : 300, loss : 0.7326, accuracy : 78.44\n",
      "Saving..\n",
      "Epoch :  32, training loss : 0.7229, training accuracy : 78.73, val loss : 0.4779, val accuracy : 84.42\n",
      "\n",
      "Epoch: 33\n",
      "iteration :  50, loss : 0.7293, accuracy : 78.75\n",
      "iteration : 100, loss : 0.7220, accuracy : 78.80\n",
      "iteration : 150, loss : 0.7354, accuracy : 78.52\n",
      "iteration : 200, loss : 0.7410, accuracy : 78.26\n",
      "iteration : 250, loss : 0.7107, accuracy : 79.18\n",
      "iteration : 300, loss : 0.7009, accuracy : 79.37\n",
      "Epoch :  33, training loss : 0.7053, training accuracy : 79.30, val loss : 0.5160, val accuracy : 84.28\n",
      "\n",
      "Epoch: 34\n",
      "iteration :  50, loss : 0.6875, accuracy : 79.75\n",
      "iteration : 100, loss : 0.6636, accuracy : 80.95\n",
      "iteration : 150, loss : 0.6837, accuracy : 80.19\n",
      "iteration : 200, loss : 0.6731, accuracy : 80.28\n",
      "iteration : 250, loss : 0.6764, accuracy : 80.13\n",
      "iteration : 300, loss : 0.6713, accuracy : 80.26\n",
      "Epoch :  34, training loss : 0.6711, training accuracy : 80.28, val loss : 0.4873, val accuracy : 84.31\n",
      "\n",
      "Epoch: 35\n",
      "iteration :  50, loss : 0.7117, accuracy : 79.71\n",
      "iteration : 100, loss : 0.6644, accuracy : 80.84\n",
      "iteration : 150, loss : 0.6898, accuracy : 79.85\n",
      "iteration : 200, loss : 0.6893, accuracy : 79.76\n",
      "iteration : 250, loss : 0.7169, accuracy : 78.84\n",
      "iteration : 300, loss : 0.7205, accuracy : 78.68\n",
      "Epoch :  35, training loss : 0.7108, training accuracy : 78.96, val loss : 0.4898, val accuracy : 84.10\n",
      "\n",
      "Epoch: 36\n",
      "iteration :  50, loss : 0.8086, accuracy : 75.56\n",
      "iteration : 100, loss : 0.6886, accuracy : 79.74\n",
      "iteration : 150, loss : 0.7253, accuracy : 78.62\n",
      "iteration : 200, loss : 0.7357, accuracy : 78.18\n",
      "iteration : 250, loss : 0.7494, accuracy : 77.73\n",
      "iteration : 300, loss : 0.7422, accuracy : 78.12\n",
      "Epoch :  36, training loss : 0.7448, training accuracy : 78.08, val loss : 0.5258, val accuracy : 83.51\n",
      "\n",
      "Epoch: 37\n",
      "iteration :  50, loss : 0.6736, accuracy : 80.43\n",
      "iteration : 100, loss : 0.7020, accuracy : 79.71\n",
      "iteration : 150, loss : 0.6612, accuracy : 80.88\n",
      "iteration : 200, loss : 0.6497, accuracy : 81.17\n",
      "iteration : 250, loss : 0.6372, accuracy : 81.54\n",
      "iteration : 300, loss : 0.6352, accuracy : 81.53\n",
      "Epoch :  37, training loss : 0.6449, training accuracy : 81.30, val loss : 0.5028, val accuracy : 84.05\n",
      "\n",
      "Epoch: 38\n",
      "iteration :  50, loss : 0.7226, accuracy : 79.17\n",
      "iteration : 100, loss : 0.6626, accuracy : 80.67\n",
      "iteration : 150, loss : 0.6833, accuracy : 80.17\n",
      "iteration : 200, loss : 0.6794, accuracy : 80.34\n",
      "iteration : 250, loss : 0.6715, accuracy : 80.47\n",
      "iteration : 300, loss : 0.6754, accuracy : 80.45\n",
      "Saving..\n",
      "Epoch :  38, training loss : 0.6691, training accuracy : 80.66, val loss : 0.4735, val accuracy : 84.45\n",
      "\n",
      "Epoch: 39\n",
      "iteration :  50, loss : 0.7266, accuracy : 79.60\n",
      "iteration : 100, loss : 0.7034, accuracy : 79.83\n",
      "iteration : 150, loss : 0.7045, accuracy : 79.75\n",
      "iteration : 200, loss : 0.6893, accuracy : 80.01\n",
      "iteration : 250, loss : 0.6857, accuracy : 80.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 300, loss : 0.6840, accuracy : 80.29\n",
      "Saving..\n",
      "Epoch :  39, training loss : 0.6728, training accuracy : 80.59, val loss : 0.4643, val accuracy : 84.78\n",
      "\n",
      "Epoch: 40\n",
      "iteration :  50, loss : 0.5186, accuracy : 85.32\n",
      "iteration : 100, loss : 0.6395, accuracy : 81.31\n",
      "iteration : 150, loss : 0.6626, accuracy : 80.64\n",
      "iteration : 200, loss : 0.6884, accuracy : 80.01\n",
      "iteration : 250, loss : 0.7080, accuracy : 79.54\n",
      "iteration : 300, loss : 0.6996, accuracy : 79.79\n",
      "Epoch :  40, training loss : 0.6880, training accuracy : 80.11, val loss : 0.5164, val accuracy : 83.66\n",
      "\n",
      "Epoch: 41\n",
      "iteration :  50, loss : 0.7763, accuracy : 77.13\n",
      "iteration : 100, loss : 0.7662, accuracy : 77.39\n",
      "iteration : 150, loss : 0.7142, accuracy : 78.64\n",
      "iteration : 200, loss : 0.7360, accuracy : 78.23\n",
      "iteration : 250, loss : 0.7072, accuracy : 79.14\n",
      "iteration : 300, loss : 0.7164, accuracy : 78.83\n",
      "Saving..\n",
      "Epoch :  41, training loss : 0.7179, training accuracy : 78.86, val loss : 0.4729, val accuracy : 85.37\n",
      "\n",
      "Epoch: 42\n",
      "iteration :  50, loss : 0.6710, accuracy : 79.93\n",
      "iteration : 100, loss : 0.7166, accuracy : 78.72\n",
      "iteration : 150, loss : 0.6798, accuracy : 79.78\n",
      "iteration : 200, loss : 0.6845, accuracy : 79.85\n",
      "iteration : 250, loss : 0.6876, accuracy : 79.76\n",
      "iteration : 300, loss : 0.6978, accuracy : 79.40\n",
      "Saving..\n",
      "Epoch :  42, training loss : 0.7073, training accuracy : 79.10, val loss : 0.5097, val accuracy : 85.56\n",
      "\n",
      "Epoch: 43\n",
      "iteration :  50, loss : 0.7639, accuracy : 77.09\n",
      "iteration : 100, loss : 0.7110, accuracy : 78.49\n",
      "iteration : 150, loss : 0.7344, accuracy : 78.13\n",
      "iteration : 200, loss : 0.7158, accuracy : 78.70\n",
      "iteration : 250, loss : 0.7153, accuracy : 78.76\n",
      "iteration : 300, loss : 0.6875, accuracy : 79.67\n",
      "Saving..\n",
      "Epoch :  43, training loss : 0.6875, training accuracy : 79.63, val loss : 0.4434, val accuracy : 85.93\n",
      "\n",
      "Epoch: 44\n",
      "iteration :  50, loss : 0.5678, accuracy : 83.86\n",
      "iteration : 100, loss : 0.6450, accuracy : 81.69\n",
      "iteration : 150, loss : 0.6696, accuracy : 80.96\n",
      "iteration : 200, loss : 0.6970, accuracy : 79.91\n",
      "iteration : 250, loss : 0.6866, accuracy : 80.35\n",
      "iteration : 300, loss : 0.6889, accuracy : 80.16\n",
      "Epoch :  44, training loss : 0.6888, training accuracy : 80.13, val loss : 0.4892, val accuracy : 84.38\n",
      "\n",
      "Epoch: 45\n",
      "iteration :  50, loss : 0.6572, accuracy : 80.95\n",
      "iteration : 100, loss : 0.6450, accuracy : 81.08\n",
      "iteration : 150, loss : 0.6582, accuracy : 80.46\n",
      "iteration : 200, loss : 0.6469, accuracy : 80.85\n",
      "iteration : 250, loss : 0.6483, accuracy : 80.88\n",
      "iteration : 300, loss : 0.6564, accuracy : 80.68\n",
      "Epoch :  45, training loss : 0.6689, training accuracy : 80.25, val loss : 0.5226, val accuracy : 84.73\n",
      "\n",
      "Epoch: 46\n",
      "iteration :  50, loss : 0.7725, accuracy : 76.79\n",
      "iteration : 100, loss : 0.6968, accuracy : 79.16\n",
      "iteration : 150, loss : 0.6525, accuracy : 80.64\n",
      "iteration : 200, loss : 0.6515, accuracy : 80.71\n",
      "iteration : 250, loss : 0.6537, accuracy : 80.82\n",
      "iteration : 300, loss : 0.6646, accuracy : 80.38\n",
      "Saving..\n",
      "Epoch :  46, training loss : 0.6554, training accuracy : 80.67, val loss : 0.4324, val accuracy : 86.06\n",
      "\n",
      "Epoch: 47\n",
      "iteration :  50, loss : 0.5160, accuracy : 84.86\n",
      "iteration : 100, loss : 0.6438, accuracy : 81.08\n",
      "iteration : 150, loss : 0.6769, accuracy : 80.22\n",
      "iteration : 200, loss : 0.6473, accuracy : 80.98\n",
      "iteration : 250, loss : 0.6650, accuracy : 80.38\n",
      "iteration : 300, loss : 0.6619, accuracy : 80.45\n",
      "Epoch :  47, training loss : 0.6622, training accuracy : 80.44, val loss : 0.4636, val accuracy : 85.46\n",
      "\n",
      "Epoch: 48\n",
      "iteration :  50, loss : 0.5616, accuracy : 83.49\n",
      "iteration : 100, loss : 0.6186, accuracy : 81.96\n",
      "iteration : 150, loss : 0.6239, accuracy : 82.05\n",
      "iteration : 200, loss : 0.6335, accuracy : 81.89\n",
      "iteration : 250, loss : 0.6324, accuracy : 81.77\n",
      "iteration : 300, loss : 0.6421, accuracy : 81.56\n",
      "Epoch :  48, training loss : 0.6403, training accuracy : 81.57, val loss : 0.4459, val accuracy : 85.76\n",
      "\n",
      "Epoch: 49\n",
      "iteration :  50, loss : 0.5972, accuracy : 83.94\n",
      "iteration : 100, loss : 0.6281, accuracy : 82.37\n",
      "iteration : 150, loss : 0.6303, accuracy : 82.38\n",
      "iteration : 200, loss : 0.6415, accuracy : 81.93\n",
      "iteration : 250, loss : 0.6311, accuracy : 82.26\n",
      "iteration : 300, loss : 0.6309, accuracy : 82.21\n",
      "Epoch :  49, training loss : 0.6360, training accuracy : 82.07, val loss : 0.4727, val accuracy : 85.10\n",
      "\n",
      "Epoch: 50\n",
      "iteration :  50, loss : 0.6618, accuracy : 80.46\n",
      "iteration : 100, loss : 0.6845, accuracy : 80.10\n",
      "iteration : 150, loss : 0.6418, accuracy : 81.70\n",
      "iteration : 200, loss : 0.6449, accuracy : 81.45\n",
      "iteration : 250, loss : 0.6594, accuracy : 81.05\n",
      "iteration : 300, loss : 0.6752, accuracy : 80.54\n",
      "Epoch :  50, training loss : 0.6676, training accuracy : 80.81, val loss : 0.4728, val accuracy : 84.70\n",
      "\n",
      "Epoch: 51\n",
      "iteration :  50, loss : 0.6411, accuracy : 81.81\n",
      "iteration : 100, loss : 0.6223, accuracy : 82.07\n",
      "iteration : 150, loss : 0.6496, accuracy : 81.10\n",
      "iteration : 200, loss : 0.6672, accuracy : 80.56\n",
      "iteration : 250, loss : 0.6727, accuracy : 80.43\n",
      "iteration : 300, loss : 0.6585, accuracy : 80.87\n",
      "Epoch :  51, training loss : 0.6574, training accuracy : 80.83, val loss : 0.4957, val accuracy : 84.35\n",
      "\n",
      "Epoch: 52\n",
      "iteration :  50, loss : 0.6403, accuracy : 80.94\n",
      "iteration : 100, loss : 0.6223, accuracy : 81.74\n",
      "iteration : 150, loss : 0.6149, accuracy : 82.11\n",
      "iteration : 200, loss : 0.6093, accuracy : 82.20\n",
      "iteration : 250, loss : 0.6263, accuracy : 81.78\n",
      "iteration : 300, loss : 0.6234, accuracy : 81.91\n",
      "Epoch :  52, training loss : 0.6234, training accuracy : 81.95, val loss : 0.4563, val accuracy : 85.88\n",
      "\n",
      "Epoch: 53\n",
      "iteration :  50, loss : 0.4934, accuracy : 85.87\n",
      "iteration : 100, loss : 0.5657, accuracy : 83.71\n",
      "iteration : 150, loss : 0.5864, accuracy : 83.12\n",
      "iteration : 200, loss : 0.6002, accuracy : 82.67\n",
      "iteration : 250, loss : 0.6379, accuracy : 81.59\n",
      "iteration : 300, loss : 0.6414, accuracy : 81.31\n",
      "Epoch :  53, training loss : 0.6427, training accuracy : 81.30, val loss : 0.4539, val accuracy : 85.59\n",
      "\n",
      "Epoch: 54\n",
      "iteration :  50, loss : 0.5861, accuracy : 83.40\n",
      "iteration : 100, loss : 0.6152, accuracy : 82.53\n",
      "iteration : 150, loss : 0.5755, accuracy : 83.76\n",
      "iteration : 200, loss : 0.5766, accuracy : 83.54\n",
      "iteration : 250, loss : 0.5927, accuracy : 82.94\n",
      "iteration : 300, loss : 0.6136, accuracy : 82.35\n",
      "Epoch :  54, training loss : 0.6152, training accuracy : 82.37, val loss : 0.4726, val accuracy : 84.99\n",
      "\n",
      "Epoch: 55\n",
      "iteration :  50, loss : 0.6805, accuracy : 80.63\n",
      "iteration : 100, loss : 0.6526, accuracy : 81.62\n",
      "iteration : 150, loss : 0.6628, accuracy : 81.32\n",
      "iteration : 200, loss : 0.6351, accuracy : 81.92\n",
      "iteration : 250, loss : 0.6200, accuracy : 82.33\n",
      "iteration : 300, loss : 0.6189, accuracy : 82.32\n",
      "Epoch :  55, training loss : 0.6159, training accuracy : 82.44, val loss : 0.4591, val accuracy : 85.65\n",
      "\n",
      "Epoch: 56\n",
      "iteration :  50, loss : 0.5300, accuracy : 85.26\n",
      "iteration : 100, loss : 0.6120, accuracy : 83.13\n",
      "iteration : 150, loss : 0.6451, accuracy : 81.94\n",
      "iteration : 200, loss : 0.6369, accuracy : 82.12\n",
      "iteration : 250, loss : 0.6289, accuracy : 82.29\n",
      "iteration : 300, loss : 0.6277, accuracy : 82.24\n",
      "Epoch :  56, training loss : 0.6355, training accuracy : 82.10, val loss : 0.4917, val accuracy : 85.61\n",
      "\n",
      "Epoch: 57\n",
      "iteration :  50, loss : 0.7733, accuracy : 77.73\n",
      "iteration : 100, loss : 0.6026, accuracy : 82.93\n",
      "iteration : 150, loss : 0.6352, accuracy : 81.96\n",
      "iteration : 200, loss : 0.6214, accuracy : 82.23\n",
      "iteration : 250, loss : 0.6161, accuracy : 82.51\n",
      "iteration : 300, loss : 0.5922, accuracy : 83.12\n",
      "Saving..\n",
      "Epoch :  57, training loss : 0.5909, training accuracy : 83.11, val loss : 0.4215, val accuracy : 86.37\n",
      "\n",
      "Epoch: 58\n",
      "iteration :  50, loss : 0.5982, accuracy : 82.70\n",
      "iteration : 100, loss : 0.6201, accuracy : 81.48\n",
      "iteration : 150, loss : 0.6020, accuracy : 82.18\n",
      "iteration : 200, loss : 0.6017, accuracy : 82.46\n",
      "iteration : 250, loss : 0.5959, accuracy : 82.64\n",
      "iteration : 300, loss : 0.6088, accuracy : 82.27\n",
      "Epoch :  58, training loss : 0.6051, training accuracy : 82.46, val loss : 0.4629, val accuracy : 85.51\n",
      "\n",
      "Epoch: 59\n",
      "iteration :  50, loss : 0.5507, accuracy : 83.56\n",
      "iteration : 100, loss : 0.6325, accuracy : 81.64\n",
      "iteration : 150, loss : 0.5985, accuracy : 82.61\n",
      "iteration : 200, loss : 0.6053, accuracy : 82.58\n",
      "iteration : 250, loss : 0.5945, accuracy : 82.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 300, loss : 0.5920, accuracy : 83.10\n",
      "Epoch :  59, training loss : 0.6010, training accuracy : 82.83, val loss : 0.4822, val accuracy : 85.73\n",
      "\n",
      "Epoch: 60\n",
      "iteration :  50, loss : 0.5024, accuracy : 85.69\n",
      "iteration : 100, loss : 0.5811, accuracy : 82.91\n",
      "iteration : 150, loss : 0.6152, accuracy : 82.10\n",
      "iteration : 200, loss : 0.6419, accuracy : 81.36\n",
      "iteration : 250, loss : 0.6340, accuracy : 81.64\n",
      "iteration : 300, loss : 0.6155, accuracy : 82.26\n",
      "Epoch :  60, training loss : 0.6177, training accuracy : 82.15, val loss : 0.4406, val accuracy : 86.24\n",
      "\n",
      "Epoch: 61\n",
      "iteration :  50, loss : 0.6062, accuracy : 82.79\n",
      "iteration : 100, loss : 0.6174, accuracy : 82.23\n",
      "iteration : 150, loss : 0.6040, accuracy : 82.87\n",
      "iteration : 200, loss : 0.6209, accuracy : 82.54\n",
      "iteration : 250, loss : 0.6290, accuracy : 82.27\n",
      "iteration : 300, loss : 0.6134, accuracy : 82.73\n",
      "Epoch :  61, training loss : 0.6064, training accuracy : 82.93, val loss : 0.4378, val accuracy : 86.13\n",
      "\n",
      "Epoch: 62\n",
      "iteration :  50, loss : 0.6439, accuracy : 81.63\n",
      "iteration : 100, loss : 0.6731, accuracy : 80.96\n",
      "iteration : 150, loss : 0.6041, accuracy : 82.90\n",
      "iteration : 200, loss : 0.6328, accuracy : 81.97\n",
      "iteration : 250, loss : 0.6288, accuracy : 82.13\n",
      "iteration : 300, loss : 0.6391, accuracy : 81.77\n",
      "Epoch :  62, training loss : 0.6408, training accuracy : 81.77, val loss : 0.4832, val accuracy : 84.99\n",
      "\n",
      "Epoch: 63\n",
      "iteration :  50, loss : 0.5737, accuracy : 83.97\n",
      "iteration : 100, loss : 0.6558, accuracy : 81.30\n",
      "iteration : 150, loss : 0.6302, accuracy : 82.03\n",
      "iteration : 200, loss : 0.6239, accuracy : 82.20\n",
      "iteration : 250, loss : 0.6343, accuracy : 81.89\n",
      "iteration : 300, loss : 0.6268, accuracy : 82.18\n",
      "Saving..\n",
      "Epoch :  63, training loss : 0.6265, training accuracy : 82.23, val loss : 0.4351, val accuracy : 86.58\n",
      "\n",
      "Epoch: 64\n",
      "iteration :  50, loss : 0.6800, accuracy : 80.65\n",
      "iteration : 100, loss : 0.6020, accuracy : 82.94\n",
      "iteration : 150, loss : 0.5985, accuracy : 83.06\n",
      "iteration : 200, loss : 0.5983, accuracy : 83.07\n",
      "iteration : 250, loss : 0.5901, accuracy : 83.31\n",
      "iteration : 300, loss : 0.6205, accuracy : 82.26\n",
      "Epoch :  64, training loss : 0.6094, training accuracy : 82.57, val loss : 0.4304, val accuracy : 86.50\n",
      "\n",
      "Epoch: 65\n",
      "iteration :  50, loss : 0.6697, accuracy : 81.67\n",
      "iteration : 100, loss : 0.7466, accuracy : 78.96\n",
      "iteration : 150, loss : 0.7072, accuracy : 79.81\n",
      "iteration : 200, loss : 0.7119, accuracy : 79.51\n",
      "iteration : 250, loss : 0.6785, accuracy : 80.53\n",
      "iteration : 300, loss : 0.6732, accuracy : 80.73\n",
      "Epoch :  65, training loss : 0.6639, training accuracy : 80.99, val loss : 0.4737, val accuracy : 85.02\n",
      "\n",
      "Epoch: 66\n",
      "iteration :  50, loss : 0.5313, accuracy : 85.12\n",
      "iteration : 100, loss : 0.5851, accuracy : 83.50\n",
      "iteration : 150, loss : 0.5461, accuracy : 84.45\n",
      "iteration : 200, loss : 0.5644, accuracy : 83.87\n",
      "iteration : 250, loss : 0.5668, accuracy : 83.90\n",
      "iteration : 300, loss : 0.5670, accuracy : 83.92\n",
      "Epoch :  66, training loss : 0.5641, training accuracy : 84.00, val loss : 0.4432, val accuracy : 85.99\n",
      "\n",
      "Epoch: 67\n",
      "iteration :  50, loss : 0.6237, accuracy : 81.50\n",
      "iteration : 100, loss : 0.5909, accuracy : 82.85\n",
      "iteration : 150, loss : 0.5609, accuracy : 84.08\n",
      "iteration : 200, loss : 0.5866, accuracy : 83.51\n",
      "iteration : 250, loss : 0.5919, accuracy : 83.30\n",
      "iteration : 300, loss : 0.5825, accuracy : 83.58\n",
      "Epoch :  67, training loss : 0.5867, training accuracy : 83.45, val loss : 0.4975, val accuracy : 84.86\n",
      "\n",
      "Epoch: 68\n",
      "iteration :  50, loss : 0.5343, accuracy : 84.86\n",
      "iteration : 100, loss : 0.5621, accuracy : 83.73\n",
      "iteration : 150, loss : 0.5733, accuracy : 83.70\n",
      "iteration : 200, loss : 0.5730, accuracy : 83.79\n",
      "iteration : 250, loss : 0.6064, accuracy : 82.79\n",
      "iteration : 300, loss : 0.6079, accuracy : 82.80\n",
      "Epoch :  68, training loss : 0.6070, training accuracy : 82.86, val loss : 0.4673, val accuracy : 85.93\n",
      "\n",
      "Epoch: 69\n",
      "iteration :  50, loss : 0.5527, accuracy : 84.18\n",
      "iteration : 100, loss : 0.6042, accuracy : 82.80\n",
      "iteration : 150, loss : 0.5613, accuracy : 84.10\n",
      "iteration : 200, loss : 0.5562, accuracy : 84.32\n",
      "iteration : 250, loss : 0.5518, accuracy : 84.40\n",
      "iteration : 300, loss : 0.5762, accuracy : 83.76\n",
      "Epoch :  69, training loss : 0.5779, training accuracy : 83.81, val loss : 0.4610, val accuracy : 86.46\n",
      "\n",
      "Epoch: 70\n",
      "iteration :  50, loss : 0.6236, accuracy : 82.11\n",
      "iteration : 100, loss : 0.5865, accuracy : 83.48\n",
      "iteration : 150, loss : 0.5777, accuracy : 83.86\n",
      "iteration : 200, loss : 0.6033, accuracy : 82.94\n",
      "iteration : 250, loss : 0.5984, accuracy : 83.10\n",
      "iteration : 300, loss : 0.6005, accuracy : 82.97\n",
      "Epoch :  70, training loss : 0.6005, training accuracy : 82.97, val loss : 0.4469, val accuracy : 86.25\n",
      "\n",
      "Epoch: 71\n",
      "iteration :  50, loss : 0.6416, accuracy : 81.98\n",
      "iteration : 100, loss : 0.6406, accuracy : 82.05\n",
      "iteration : 150, loss : 0.6152, accuracy : 82.67\n",
      "iteration : 200, loss : 0.6416, accuracy : 81.84\n",
      "iteration : 250, loss : 0.6314, accuracy : 82.17\n",
      "iteration : 300, loss : 0.6220, accuracy : 82.40\n",
      "Epoch :  71, training loss : 0.6196, training accuracy : 82.43, val loss : 0.4356, val accuracy : 86.30\n",
      "\n",
      "Epoch: 72\n",
      "iteration :  50, loss : 0.5745, accuracy : 83.40\n",
      "iteration : 100, loss : 0.5631, accuracy : 84.24\n",
      "iteration : 150, loss : 0.5332, accuracy : 85.15\n",
      "iteration : 200, loss : 0.5764, accuracy : 83.83\n",
      "iteration : 250, loss : 0.5637, accuracy : 84.16\n",
      "iteration : 300, loss : 0.5685, accuracy : 83.96\n",
      "Saving..\n",
      "Epoch :  72, training loss : 0.5683, training accuracy : 83.91, val loss : 0.4290, val accuracy : 86.70\n",
      "\n",
      "Epoch: 73\n",
      "iteration :  50, loss : 0.4495, accuracy : 87.21\n",
      "iteration : 100, loss : 0.4596, accuracy : 86.95\n",
      "iteration : 150, loss : 0.5522, accuracy : 84.11\n",
      "iteration : 200, loss : 0.5533, accuracy : 84.31\n",
      "iteration : 250, loss : 0.5487, accuracy : 84.45\n",
      "iteration : 300, loss : 0.5590, accuracy : 84.16\n",
      "Epoch :  73, training loss : 0.5571, training accuracy : 84.23, val loss : 0.4626, val accuracy : 85.26\n",
      "\n",
      "Epoch: 74\n",
      "iteration :  50, loss : 0.5821, accuracy : 82.56\n",
      "iteration : 100, loss : 0.5225, accuracy : 84.72\n",
      "iteration : 150, loss : 0.5695, accuracy : 83.54\n",
      "iteration : 200, loss : 0.5419, accuracy : 84.44\n",
      "iteration : 250, loss : 0.5438, accuracy : 84.51\n",
      "iteration : 300, loss : 0.5715, accuracy : 83.76\n",
      "Epoch :  74, training loss : 0.5732, training accuracy : 83.67, val loss : 0.4388, val accuracy : 86.67\n",
      "\n",
      "Epoch: 75\n",
      "iteration :  50, loss : 0.5240, accuracy : 85.54\n",
      "iteration : 100, loss : 0.5781, accuracy : 83.91\n",
      "iteration : 150, loss : 0.5755, accuracy : 83.99\n",
      "iteration : 200, loss : 0.5844, accuracy : 83.52\n",
      "iteration : 250, loss : 0.5796, accuracy : 83.70\n",
      "iteration : 300, loss : 0.5652, accuracy : 84.16\n",
      "Saving..\n",
      "Epoch :  75, training loss : 0.5610, training accuracy : 84.21, val loss : 0.4256, val accuracy : 86.73\n",
      "\n",
      "Epoch: 76\n",
      "iteration :  50, loss : 0.5404, accuracy : 85.65\n",
      "iteration : 100, loss : 0.5949, accuracy : 83.98\n",
      "iteration : 150, loss : 0.6033, accuracy : 83.21\n",
      "iteration : 200, loss : 0.5904, accuracy : 83.54\n",
      "iteration : 250, loss : 0.5818, accuracy : 83.77\n",
      "iteration : 300, loss : 0.5890, accuracy : 83.55\n",
      "Saving..\n",
      "Epoch :  76, training loss : 0.5888, training accuracy : 83.49, val loss : 0.4367, val accuracy : 87.04\n",
      "\n",
      "Epoch: 77\n",
      "iteration :  50, loss : 0.4810, accuracy : 85.77\n",
      "iteration : 100, loss : 0.5515, accuracy : 84.00\n",
      "iteration : 150, loss : 0.5606, accuracy : 83.94\n",
      "iteration : 200, loss : 0.5892, accuracy : 83.17\n",
      "iteration : 250, loss : 0.5848, accuracy : 83.36\n",
      "iteration : 300, loss : 0.5790, accuracy : 83.66\n",
      "Epoch :  77, training loss : 0.5879, training accuracy : 83.42, val loss : 0.4541, val accuracy : 86.84\n",
      "\n",
      "Epoch: 78\n",
      "iteration :  50, loss : 0.6051, accuracy : 83.68\n",
      "iteration : 100, loss : 0.5423, accuracy : 85.38\n",
      "iteration : 150, loss : 0.5468, accuracy : 85.13\n",
      "iteration : 200, loss : 0.5465, accuracy : 85.18\n",
      "iteration : 250, loss : 0.5329, accuracy : 85.43\n",
      "iteration : 300, loss : 0.5244, accuracy : 85.60\n",
      "Epoch :  78, training loss : 0.5283, training accuracy : 85.48, val loss : 0.4436, val accuracy : 86.52\n",
      "\n",
      "Epoch: 79\n",
      "iteration :  50, loss : 0.5963, accuracy : 82.72\n",
      "iteration : 100, loss : 0.5492, accuracy : 84.77\n",
      "iteration : 150, loss : 0.5366, accuracy : 85.02\n",
      "iteration : 200, loss : 0.5920, accuracy : 83.54\n",
      "iteration : 250, loss : 0.5724, accuracy : 84.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 300, loss : 0.5693, accuracy : 84.35\n",
      "Epoch :  79, training loss : 0.5703, training accuracy : 84.31, val loss : 0.4465, val accuracy : 86.32\n",
      "\n",
      "Epoch: 80\n",
      "iteration :  50, loss : 0.6201, accuracy : 82.39\n",
      "iteration : 100, loss : 0.6504, accuracy : 81.20\n",
      "iteration : 150, loss : 0.6240, accuracy : 82.12\n",
      "iteration : 200, loss : 0.6309, accuracy : 81.79\n",
      "iteration : 250, loss : 0.6234, accuracy : 82.04\n",
      "iteration : 300, loss : 0.6270, accuracy : 82.01\n",
      "Epoch :  80, training loss : 0.6299, training accuracy : 82.00, val loss : 0.4414, val accuracy : 86.77\n",
      "\n",
      "Epoch: 81\n",
      "iteration :  50, loss : 0.6504, accuracy : 81.09\n",
      "iteration : 100, loss : 0.6763, accuracy : 80.38\n",
      "iteration : 150, loss : 0.6700, accuracy : 80.47\n",
      "iteration : 200, loss : 0.6532, accuracy : 81.18\n",
      "iteration : 250, loss : 0.6383, accuracy : 81.73\n",
      "iteration : 300, loss : 0.6091, accuracy : 82.59\n",
      "Epoch :  81, training loss : 0.6078, training accuracy : 82.61, val loss : 0.4395, val accuracy : 86.41\n",
      "\n",
      "Epoch: 82\n",
      "iteration :  50, loss : 0.5852, accuracy : 83.57\n",
      "iteration : 100, loss : 0.5861, accuracy : 83.92\n",
      "iteration : 150, loss : 0.5771, accuracy : 84.19\n",
      "iteration : 200, loss : 0.5802, accuracy : 84.12\n",
      "iteration : 250, loss : 0.5789, accuracy : 84.00\n",
      "iteration : 300, loss : 0.5690, accuracy : 84.00\n",
      "Epoch :  82, training loss : 0.5578, training accuracy : 84.32, val loss : 0.4572, val accuracy : 85.84\n",
      "\n",
      "Epoch: 83\n",
      "iteration :  50, loss : 0.6058, accuracy : 82.00\n",
      "iteration : 100, loss : 0.5882, accuracy : 83.35\n",
      "iteration : 150, loss : 0.6336, accuracy : 81.77\n",
      "iteration : 200, loss : 0.5988, accuracy : 82.83\n",
      "iteration : 250, loss : 0.5917, accuracy : 83.17\n",
      "iteration : 300, loss : 0.5991, accuracy : 82.97\n",
      "Epoch :  83, training loss : 0.6061, training accuracy : 82.76, val loss : 0.4482, val accuracy : 86.81\n",
      "\n",
      "Epoch: 84\n",
      "iteration :  50, loss : 0.6224, accuracy : 81.87\n",
      "iteration : 100, loss : 0.5373, accuracy : 84.68\n",
      "iteration : 150, loss : 0.5728, accuracy : 83.57\n",
      "iteration : 200, loss : 0.5783, accuracy : 83.53\n",
      "iteration : 250, loss : 0.5784, accuracy : 83.49\n",
      "iteration : 300, loss : 0.5728, accuracy : 83.66\n",
      "Saving..\n",
      "Epoch :  84, training loss : 0.5711, training accuracy : 83.71, val loss : 0.4038, val accuracy : 87.36\n",
      "\n",
      "Epoch: 85\n",
      "iteration :  50, loss : 0.5733, accuracy : 84.42\n",
      "iteration : 100, loss : 0.5764, accuracy : 84.24\n",
      "iteration : 150, loss : 0.5534, accuracy : 84.50\n",
      "iteration : 200, loss : 0.5587, accuracy : 84.26\n",
      "iteration : 250, loss : 0.5404, accuracy : 84.71\n",
      "iteration : 300, loss : 0.5636, accuracy : 83.85\n",
      "Epoch :  85, training loss : 0.5578, training accuracy : 84.00, val loss : 0.4197, val accuracy : 86.95\n",
      "\n",
      "Epoch: 86\n",
      "iteration :  50, loss : 0.5115, accuracy : 84.46\n",
      "iteration : 100, loss : 0.5171, accuracy : 84.73\n",
      "iteration : 150, loss : 0.5424, accuracy : 84.19\n",
      "iteration : 200, loss : 0.5513, accuracy : 84.22\n",
      "iteration : 250, loss : 0.5693, accuracy : 83.53\n",
      "iteration : 300, loss : 0.5755, accuracy : 83.43\n",
      "Epoch :  86, training loss : 0.5756, training accuracy : 83.39, val loss : 0.4183, val accuracy : 87.27\n",
      "\n",
      "Epoch: 87\n",
      "iteration :  50, loss : 0.5780, accuracy : 82.62\n",
      "iteration : 100, loss : 0.5259, accuracy : 84.65\n",
      "iteration : 150, loss : 0.5382, accuracy : 84.47\n",
      "iteration : 200, loss : 0.5391, accuracy : 84.48\n",
      "iteration : 250, loss : 0.5465, accuracy : 84.39\n",
      "iteration : 300, loss : 0.5444, accuracy : 84.63\n",
      "Epoch :  87, training loss : 0.5436, training accuracy : 84.70, val loss : 0.4408, val accuracy : 86.35\n",
      "\n",
      "Epoch: 88\n",
      "iteration :  50, loss : 0.5689, accuracy : 83.74\n",
      "iteration : 100, loss : 0.6401, accuracy : 81.81\n",
      "iteration : 150, loss : 0.6288, accuracy : 82.24\n",
      "iteration : 200, loss : 0.5881, accuracy : 83.37\n",
      "iteration : 250, loss : 0.5940, accuracy : 83.16\n",
      "iteration : 300, loss : 0.6036, accuracy : 82.76\n",
      "Epoch :  88, training loss : 0.6023, training accuracy : 82.76, val loss : 0.4294, val accuracy : 86.49\n",
      "\n",
      "Epoch: 89\n",
      "iteration :  50, loss : 0.5668, accuracy : 84.89\n",
      "iteration : 100, loss : 0.5029, accuracy : 86.44\n",
      "iteration : 150, loss : 0.4804, accuracy : 86.96\n",
      "iteration : 200, loss : 0.4847, accuracy : 86.79\n",
      "iteration : 250, loss : 0.5175, accuracy : 85.72\n",
      "iteration : 300, loss : 0.5142, accuracy : 85.64\n",
      "Epoch :  89, training loss : 0.5099, training accuracy : 85.79, val loss : 0.4412, val accuracy : 86.09\n",
      "\n",
      "Epoch: 90\n",
      "iteration :  50, loss : 0.5958, accuracy : 82.69\n",
      "iteration : 100, loss : 0.5637, accuracy : 83.88\n",
      "iteration : 150, loss : 0.5899, accuracy : 83.46\n",
      "iteration : 200, loss : 0.5908, accuracy : 83.44\n",
      "iteration : 250, loss : 0.5797, accuracy : 83.86\n",
      "iteration : 300, loss : 0.5688, accuracy : 84.23\n",
      "Epoch :  90, training loss : 0.5682, training accuracy : 84.29, val loss : 0.4408, val accuracy : 87.04\n",
      "\n",
      "Epoch: 91\n",
      "iteration :  50, loss : 0.5283, accuracy : 85.87\n",
      "iteration : 100, loss : 0.5545, accuracy : 84.82\n",
      "iteration : 150, loss : 0.5152, accuracy : 85.87\n",
      "iteration : 200, loss : 0.5379, accuracy : 85.13\n",
      "iteration : 250, loss : 0.5259, accuracy : 85.49\n",
      "iteration : 300, loss : 0.5475, accuracy : 84.93\n",
      "Saving..\n",
      "Epoch :  91, training loss : 0.5426, training accuracy : 85.08, val loss : 0.4042, val accuracy : 87.62\n",
      "\n",
      "Epoch: 92\n",
      "iteration :  50, loss : 0.4782, accuracy : 87.32\n",
      "iteration : 100, loss : 0.5578, accuracy : 85.06\n",
      "iteration : 150, loss : 0.5296, accuracy : 85.75\n",
      "iteration : 200, loss : 0.5640, accuracy : 84.61\n",
      "iteration : 250, loss : 0.5448, accuracy : 85.22\n",
      "iteration : 300, loss : 0.5806, accuracy : 84.12\n",
      "Epoch :  92, training loss : 0.5786, training accuracy : 84.13, val loss : 0.4349, val accuracy : 86.95\n",
      "\n",
      "Epoch: 93\n",
      "iteration :  50, loss : 0.5610, accuracy : 84.67\n",
      "iteration : 100, loss : 0.5598, accuracy : 84.76\n",
      "iteration : 150, loss : 0.5993, accuracy : 83.38\n",
      "iteration : 200, loss : 0.5884, accuracy : 83.55\n",
      "iteration : 250, loss : 0.5734, accuracy : 84.13\n",
      "iteration : 300, loss : 0.5710, accuracy : 84.17\n",
      "Saving..\n",
      "Epoch :  93, training loss : 0.5710, training accuracy : 84.12, val loss : 0.3939, val accuracy : 87.92\n",
      "\n",
      "Epoch: 94\n",
      "iteration :  50, loss : 0.6449, accuracy : 82.32\n",
      "iteration : 100, loss : 0.5877, accuracy : 83.54\n",
      "iteration : 150, loss : 0.5669, accuracy : 84.09\n",
      "iteration : 200, loss : 0.5624, accuracy : 84.24\n",
      "iteration : 250, loss : 0.5867, accuracy : 83.53\n",
      "iteration : 300, loss : 0.5807, accuracy : 83.76\n",
      "Epoch :  94, training loss : 0.5789, training accuracy : 83.82, val loss : 0.4345, val accuracy : 86.85\n",
      "\n",
      "Epoch: 95\n",
      "iteration :  50, loss : 0.6599, accuracy : 82.24\n",
      "iteration : 100, loss : 0.6307, accuracy : 82.92\n",
      "iteration : 150, loss : 0.6008, accuracy : 83.79\n",
      "iteration : 200, loss : 0.6322, accuracy : 82.81\n",
      "iteration : 250, loss : 0.6148, accuracy : 83.32\n",
      "iteration : 300, loss : 0.6105, accuracy : 83.30\n",
      "Epoch :  95, training loss : 0.6020, training accuracy : 83.56, val loss : 0.4023, val accuracy : 87.75\n",
      "\n",
      "Epoch: 96\n",
      "iteration :  50, loss : 0.6220, accuracy : 82.73\n",
      "iteration : 100, loss : 0.5713, accuracy : 83.93\n",
      "iteration : 150, loss : 0.6127, accuracy : 82.72\n",
      "iteration : 200, loss : 0.6051, accuracy : 83.15\n",
      "iteration : 250, loss : 0.5860, accuracy : 83.53\n",
      "iteration : 300, loss : 0.5816, accuracy : 83.70\n",
      "Epoch :  96, training loss : 0.5865, training accuracy : 83.56, val loss : 0.4420, val accuracy : 86.97\n",
      "\n",
      "Epoch: 97\n",
      "iteration :  50, loss : 0.5445, accuracy : 84.77\n",
      "iteration : 100, loss : 0.5564, accuracy : 84.50\n",
      "iteration : 150, loss : 0.5267, accuracy : 85.33\n",
      "iteration : 200, loss : 0.5554, accuracy : 84.55\n",
      "iteration : 250, loss : 0.5489, accuracy : 84.82\n",
      "iteration : 300, loss : 0.5599, accuracy : 84.55\n",
      "Epoch :  97, training loss : 0.5594, training accuracy : 84.54, val loss : 0.4459, val accuracy : 86.68\n",
      "\n",
      "Epoch: 98\n",
      "iteration :  50, loss : 0.4851, accuracy : 86.58\n",
      "iteration : 100, loss : 0.5602, accuracy : 84.73\n",
      "iteration : 150, loss : 0.5380, accuracy : 85.04\n",
      "iteration : 200, loss : 0.5028, accuracy : 86.05\n",
      "iteration : 250, loss : 0.5294, accuracy : 85.28\n",
      "iteration : 300, loss : 0.5405, accuracy : 84.89\n",
      "Epoch :  98, training loss : 0.5355, training accuracy : 85.02, val loss : 0.4139, val accuracy : 87.36\n",
      "\n",
      "Epoch: 99\n",
      "iteration :  50, loss : 0.6057, accuracy : 83.02\n",
      "iteration : 100, loss : 0.6265, accuracy : 82.58\n",
      "iteration : 150, loss : 0.6030, accuracy : 83.26\n",
      "iteration : 200, loss : 0.5921, accuracy : 83.38\n",
      "iteration : 250, loss : 0.5779, accuracy : 83.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 300, loss : 0.5598, accuracy : 84.36\n",
      "Epoch :  99, training loss : 0.5620, training accuracy : 84.37, val loss : 0.4557, val accuracy : 86.26\n",
      "\n",
      "Epoch: 100\n",
      "iteration :  50, loss : 0.5442, accuracy : 84.64\n",
      "iteration : 100, loss : 0.5412, accuracy : 84.72\n",
      "iteration : 150, loss : 0.5250, accuracy : 85.30\n",
      "iteration : 200, loss : 0.5100, accuracy : 85.80\n",
      "iteration : 250, loss : 0.5279, accuracy : 85.24\n",
      "iteration : 300, loss : 0.5298, accuracy : 85.22\n",
      "Epoch : 100, training loss : 0.5326, training accuracy : 85.15, val loss : 0.4096, val accuracy : 87.63\n",
      "\n",
      "Epoch: 101\n",
      "iteration :  50, loss : 0.5920, accuracy : 83.66\n",
      "iteration : 100, loss : 0.5508, accuracy : 84.49\n",
      "iteration : 150, loss : 0.5852, accuracy : 83.44\n",
      "iteration : 200, loss : 0.5617, accuracy : 84.03\n",
      "iteration : 250, loss : 0.5686, accuracy : 83.71\n",
      "iteration : 300, loss : 0.5757, accuracy : 83.45\n",
      "Epoch : 101, training loss : 0.5663, training accuracy : 83.73, val loss : 0.4191, val accuracy : 87.03\n",
      "\n",
      "Epoch: 102\n",
      "iteration :  50, loss : 0.5310, accuracy : 84.71\n",
      "iteration : 100, loss : 0.6258, accuracy : 82.07\n",
      "iteration : 150, loss : 0.5553, accuracy : 84.61\n",
      "iteration : 200, loss : 0.5478, accuracy : 84.75\n",
      "iteration : 250, loss : 0.5678, accuracy : 84.23\n",
      "iteration : 300, loss : 0.5828, accuracy : 83.82\n",
      "Epoch : 102, training loss : 0.5875, training accuracy : 83.70, val loss : 0.4152, val accuracy : 87.71\n",
      "\n",
      "Epoch: 103\n",
      "iteration :  50, loss : 0.4243, accuracy : 88.65\n",
      "iteration : 100, loss : 0.4506, accuracy : 87.64\n",
      "iteration : 150, loss : 0.4901, accuracy : 86.31\n",
      "iteration : 200, loss : 0.5366, accuracy : 84.80\n",
      "iteration : 250, loss : 0.5393, accuracy : 84.73\n",
      "iteration : 300, loss : 0.5374, accuracy : 84.76\n",
      "Epoch : 103, training loss : 0.5376, training accuracy : 84.77, val loss : 0.4317, val accuracy : 87.16\n",
      "\n",
      "Epoch: 104\n",
      "iteration :  50, loss : 0.5635, accuracy : 85.26\n",
      "iteration : 100, loss : 0.5178, accuracy : 86.17\n",
      "iteration : 150, loss : 0.5131, accuracy : 86.25\n",
      "iteration : 200, loss : 0.5253, accuracy : 85.97\n",
      "iteration : 250, loss : 0.5164, accuracy : 86.07\n",
      "iteration : 300, loss : 0.5322, accuracy : 85.59\n",
      "Epoch : 104, training loss : 0.5348, training accuracy : 85.49, val loss : 0.4062, val accuracy : 87.66\n",
      "\n",
      "Epoch: 105\n",
      "iteration :  50, loss : 0.5262, accuracy : 86.27\n",
      "iteration : 100, loss : 0.5227, accuracy : 85.78\n",
      "iteration : 150, loss : 0.5710, accuracy : 84.05\n",
      "iteration : 200, loss : 0.5712, accuracy : 83.84\n",
      "iteration : 250, loss : 0.5812, accuracy : 83.61\n",
      "iteration : 300, loss : 0.5743, accuracy : 83.88\n",
      "Epoch : 105, training loss : 0.5734, training accuracy : 83.90, val loss : 0.4268, val accuracy : 87.43\n",
      "\n",
      "Epoch: 106\n",
      "iteration :  50, loss : 0.3785, accuracy : 90.14\n",
      "iteration : 100, loss : 0.5489, accuracy : 84.76\n",
      "iteration : 150, loss : 0.5282, accuracy : 85.48\n",
      "iteration : 200, loss : 0.5652, accuracy : 84.37\n",
      "iteration : 250, loss : 0.5513, accuracy : 84.77\n",
      "iteration : 300, loss : 0.5389, accuracy : 85.05\n",
      "Epoch : 106, training loss : 0.5351, training accuracy : 85.18, val loss : 0.4071, val accuracy : 87.70\n",
      "\n",
      "Epoch: 107\n",
      "iteration :  50, loss : 0.6000, accuracy : 82.61\n",
      "iteration : 100, loss : 0.5452, accuracy : 84.55\n",
      "iteration : 150, loss : 0.5462, accuracy : 84.80\n",
      "iteration : 200, loss : 0.5425, accuracy : 84.98\n",
      "iteration : 250, loss : 0.5497, accuracy : 84.75\n",
      "iteration : 300, loss : 0.5516, accuracy : 84.78\n",
      "Saving..\n",
      "Epoch : 107, training loss : 0.5503, training accuracy : 84.84, val loss : 0.4026, val accuracy : 88.06\n",
      "\n",
      "Epoch: 108\n",
      "iteration :  50, loss : 0.4400, accuracy : 87.71\n",
      "iteration : 100, loss : 0.5028, accuracy : 86.21\n",
      "iteration : 150, loss : 0.5578, accuracy : 84.52\n",
      "iteration : 200, loss : 0.5364, accuracy : 85.29\n",
      "iteration : 250, loss : 0.5631, accuracy : 84.46\n",
      "iteration : 300, loss : 0.5523, accuracy : 84.84\n",
      "Epoch : 108, training loss : 0.5489, training accuracy : 84.88, val loss : 0.4004, val accuracy : 87.91\n",
      "\n",
      "Epoch: 109\n",
      "iteration :  50, loss : 0.5638, accuracy : 84.01\n",
      "iteration : 100, loss : 0.5660, accuracy : 84.27\n",
      "iteration : 150, loss : 0.5526, accuracy : 84.77\n",
      "iteration : 200, loss : 0.5694, accuracy : 84.14\n",
      "iteration : 250, loss : 0.5591, accuracy : 84.49\n",
      "iteration : 300, loss : 0.5659, accuracy : 84.22\n",
      "Epoch : 109, training loss : 0.5577, training accuracy : 84.45, val loss : 0.4434, val accuracy : 86.85\n",
      "\n",
      "Epoch: 110\n",
      "iteration :  50, loss : 0.5094, accuracy : 86.48\n",
      "iteration : 100, loss : 0.4692, accuracy : 87.65\n",
      "iteration : 150, loss : 0.5193, accuracy : 86.31\n",
      "iteration : 200, loss : 0.5482, accuracy : 85.23\n",
      "iteration : 250, loss : 0.5713, accuracy : 84.42\n",
      "iteration : 300, loss : 0.5602, accuracy : 84.78\n",
      "Saving..\n",
      "Epoch : 110, training loss : 0.5609, training accuracy : 84.69, val loss : 0.3934, val accuracy : 88.52\n",
      "\n",
      "Epoch: 111\n",
      "iteration :  50, loss : 0.6669, accuracy : 80.70\n",
      "iteration : 100, loss : 0.6100, accuracy : 82.64\n",
      "iteration : 150, loss : 0.5762, accuracy : 83.71\n",
      "iteration : 200, loss : 0.5798, accuracy : 83.63\n",
      "iteration : 250, loss : 0.5604, accuracy : 84.32\n",
      "iteration : 300, loss : 0.5270, accuracy : 85.35\n",
      "Epoch : 111, training loss : 0.5161, training accuracy : 85.67, val loss : 0.3908, val accuracy : 87.96\n",
      "\n",
      "Epoch: 112\n",
      "iteration :  50, loss : 0.6389, accuracy : 81.85\n",
      "iteration : 100, loss : 0.6230, accuracy : 82.65\n",
      "iteration : 150, loss : 0.5896, accuracy : 83.66\n",
      "iteration : 200, loss : 0.5901, accuracy : 83.64\n",
      "iteration : 250, loss : 0.5747, accuracy : 83.95\n",
      "iteration : 300, loss : 0.5661, accuracy : 84.09\n",
      "Epoch : 112, training loss : 0.5698, training accuracy : 83.92, val loss : 0.4403, val accuracy : 86.63\n",
      "\n",
      "Epoch: 113\n",
      "iteration :  50, loss : 0.5920, accuracy : 84.14\n",
      "iteration : 100, loss : 0.5333, accuracy : 85.83\n",
      "iteration : 150, loss : 0.5289, accuracy : 85.80\n",
      "iteration : 200, loss : 0.5415, accuracy : 85.33\n",
      "iteration : 250, loss : 0.5382, accuracy : 85.51\n",
      "iteration : 300, loss : 0.5322, accuracy : 85.66\n",
      "Epoch : 113, training loss : 0.5238, training accuracy : 85.90, val loss : 0.4192, val accuracy : 87.60\n",
      "\n",
      "Epoch: 114\n",
      "iteration :  50, loss : 0.5213, accuracy : 85.73\n",
      "iteration : 100, loss : 0.5848, accuracy : 84.01\n",
      "iteration : 150, loss : 0.5577, accuracy : 84.68\n",
      "iteration : 200, loss : 0.5705, accuracy : 84.34\n",
      "iteration : 250, loss : 0.5288, accuracy : 85.62\n",
      "iteration : 300, loss : 0.5553, accuracy : 84.89\n",
      "Epoch : 114, training loss : 0.5574, training accuracy : 84.90, val loss : 0.4367, val accuracy : 86.97\n",
      "\n",
      "Epoch: 115\n",
      "iteration :  50, loss : 0.5767, accuracy : 83.85\n",
      "iteration : 100, loss : 0.5293, accuracy : 85.30\n",
      "iteration : 150, loss : 0.5175, accuracy : 85.96\n",
      "iteration : 200, loss : 0.5058, accuracy : 86.23\n",
      "iteration : 250, loss : 0.5234, accuracy : 85.90\n",
      "iteration : 300, loss : 0.5180, accuracy : 86.00\n",
      "Epoch : 115, training loss : 0.5207, training accuracy : 85.92, val loss : 0.4187, val accuracy : 87.44\n",
      "\n",
      "Epoch: 116\n",
      "iteration :  50, loss : 0.4947, accuracy : 86.12\n",
      "iteration : 100, loss : 0.5190, accuracy : 85.19\n",
      "iteration : 150, loss : 0.4965, accuracy : 86.22\n",
      "iteration : 200, loss : 0.4955, accuracy : 86.38\n",
      "iteration : 250, loss : 0.5024, accuracy : 86.11\n",
      "iteration : 300, loss : 0.5008, accuracy : 86.20\n",
      "Epoch : 116, training loss : 0.5033, training accuracy : 86.16, val loss : 0.4182, val accuracy : 87.54\n",
      "\n",
      "Epoch: 117\n",
      "iteration :  50, loss : 0.4550, accuracy : 87.19\n",
      "iteration : 100, loss : 0.4819, accuracy : 86.88\n",
      "iteration : 150, loss : 0.5172, accuracy : 85.90\n",
      "iteration : 200, loss : 0.5107, accuracy : 85.95\n",
      "iteration : 250, loss : 0.5346, accuracy : 85.16\n",
      "iteration : 300, loss : 0.5198, accuracy : 85.64\n",
      "Epoch : 117, training loss : 0.5210, training accuracy : 85.61, val loss : 0.4133, val accuracy : 87.40\n",
      "\n",
      "Epoch: 118\n",
      "iteration :  50, loss : 0.4982, accuracy : 85.84\n",
      "iteration : 100, loss : 0.4893, accuracy : 86.12\n",
      "iteration : 150, loss : 0.4789, accuracy : 86.46\n",
      "iteration : 200, loss : 0.4869, accuracy : 86.35\n",
      "iteration : 250, loss : 0.4937, accuracy : 86.18\n",
      "iteration : 300, loss : 0.4912, accuracy : 86.35\n",
      "Epoch : 118, training loss : 0.4930, training accuracy : 86.34, val loss : 0.4162, val accuracy : 87.97\n",
      "\n",
      "Epoch: 119\n",
      "iteration :  50, loss : 0.5580, accuracy : 84.58\n",
      "iteration : 100, loss : 0.5492, accuracy : 84.72\n",
      "iteration : 150, loss : 0.5315, accuracy : 85.25\n",
      "iteration : 200, loss : 0.5554, accuracy : 84.24\n",
      "iteration : 250, loss : 0.5341, accuracy : 84.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 300, loss : 0.5401, accuracy : 84.75\n",
      "Epoch : 119, training loss : 0.5415, training accuracy : 84.79, val loss : 0.4232, val accuracy : 88.02\n",
      "\n",
      "Epoch: 120\n",
      "iteration :  50, loss : 0.5430, accuracy : 85.21\n",
      "iteration : 100, loss : 0.6023, accuracy : 83.67\n",
      "iteration : 150, loss : 0.5546, accuracy : 84.64\n",
      "iteration : 200, loss : 0.5227, accuracy : 85.40\n",
      "iteration : 250, loss : 0.5443, accuracy : 84.90\n",
      "iteration : 300, loss : 0.5540, accuracy : 84.78\n",
      "Epoch : 120, training loss : 0.5499, training accuracy : 84.89, val loss : 0.4127, val accuracy : 87.82\n",
      "\n",
      "Epoch: 121\n",
      "iteration :  50, loss : 0.5957, accuracy : 83.55\n",
      "iteration : 100, loss : 0.5696, accuracy : 84.46\n",
      "iteration : 150, loss : 0.5438, accuracy : 85.03\n",
      "iteration : 200, loss : 0.5312, accuracy : 85.40\n",
      "iteration : 250, loss : 0.5406, accuracy : 84.98\n",
      "iteration : 300, loss : 0.5396, accuracy : 85.18\n",
      "Epoch : 121, training loss : 0.5375, training accuracy : 85.24, val loss : 0.4050, val accuracy : 88.05\n",
      "\n",
      "Epoch: 122\n",
      "iteration :  50, loss : 0.5574, accuracy : 85.02\n",
      "iteration : 100, loss : 0.5330, accuracy : 85.47\n",
      "iteration : 150, loss : 0.4969, accuracy : 86.40\n",
      "iteration : 200, loss : 0.5311, accuracy : 85.39\n",
      "iteration : 250, loss : 0.5401, accuracy : 85.16\n",
      "iteration : 300, loss : 0.5331, accuracy : 85.37\n",
      "Epoch : 122, training loss : 0.5301, training accuracy : 85.38, val loss : 0.4195, val accuracy : 87.34\n",
      "\n",
      "Epoch: 123\n",
      "iteration :  50, loss : 0.4500, accuracy : 87.82\n",
      "iteration : 100, loss : 0.4961, accuracy : 86.41\n",
      "iteration : 150, loss : 0.4872, accuracy : 86.86\n",
      "iteration : 200, loss : 0.5277, accuracy : 85.63\n",
      "iteration : 250, loss : 0.5183, accuracy : 85.87\n",
      "iteration : 300, loss : 0.5007, accuracy : 86.36\n",
      "Epoch : 123, training loss : 0.4953, training accuracy : 86.50, val loss : 0.4165, val accuracy : 87.78\n",
      "\n",
      "Epoch: 124\n",
      "iteration :  50, loss : 0.4761, accuracy : 87.14\n",
      "iteration : 100, loss : 0.4920, accuracy : 86.73\n",
      "iteration : 150, loss : 0.4893, accuracy : 87.10\n",
      "iteration : 200, loss : 0.4965, accuracy : 86.45\n",
      "iteration : 250, loss : 0.4929, accuracy : 86.56\n",
      "iteration : 300, loss : 0.4828, accuracy : 86.87\n",
      "Epoch : 124, training loss : 0.4868, training accuracy : 86.72, val loss : 0.4121, val accuracy : 87.94\n",
      "\n",
      "Epoch: 125\n",
      "iteration :  50, loss : 0.5262, accuracy : 85.09\n",
      "iteration : 100, loss : 0.5301, accuracy : 85.33\n",
      "iteration : 150, loss : 0.5242, accuracy : 85.56\n",
      "iteration : 200, loss : 0.5143, accuracy : 85.71\n",
      "iteration : 250, loss : 0.5275, accuracy : 85.39\n",
      "iteration : 300, loss : 0.4992, accuracy : 86.35\n",
      "Epoch : 125, training loss : 0.4977, training accuracy : 86.42, val loss : 0.4202, val accuracy : 86.92\n",
      "\n",
      "Epoch: 126\n",
      "iteration :  50, loss : 0.4791, accuracy : 86.89\n",
      "iteration : 100, loss : 0.5013, accuracy : 86.28\n",
      "iteration : 150, loss : 0.5341, accuracy : 85.26\n",
      "iteration : 200, loss : 0.5524, accuracy : 84.67\n",
      "iteration : 250, loss : 0.5430, accuracy : 85.01\n",
      "iteration : 300, loss : 0.5557, accuracy : 84.64\n",
      "Epoch : 126, training loss : 0.5483, training accuracy : 84.88, val loss : 0.4092, val accuracy : 87.72\n",
      "\n",
      "Epoch: 127\n",
      "iteration :  50, loss : 0.5581, accuracy : 84.41\n",
      "iteration : 100, loss : 0.5691, accuracy : 84.28\n",
      "iteration : 150, loss : 0.5362, accuracy : 85.29\n",
      "iteration : 200, loss : 0.5254, accuracy : 85.63\n",
      "iteration : 250, loss : 0.5210, accuracy : 85.77\n",
      "iteration : 300, loss : 0.5088, accuracy : 86.09\n",
      "Epoch : 127, training loss : 0.5111, training accuracy : 86.04, val loss : 0.4139, val accuracy : 87.93\n",
      "\n",
      "Epoch: 128\n",
      "iteration :  50, loss : 0.4612, accuracy : 87.06\n",
      "iteration : 100, loss : 0.5637, accuracy : 84.11\n",
      "iteration : 150, loss : 0.5087, accuracy : 86.12\n",
      "iteration : 200, loss : 0.5211, accuracy : 85.79\n",
      "iteration : 250, loss : 0.5466, accuracy : 85.02\n",
      "iteration : 300, loss : 0.5303, accuracy : 85.49\n",
      "Epoch : 128, training loss : 0.5355, training accuracy : 85.38, val loss : 0.4193, val accuracy : 88.01\n",
      "\n",
      "Epoch: 129\n",
      "iteration :  50, loss : 0.6497, accuracy : 81.83\n",
      "iteration : 100, loss : 0.5868, accuracy : 83.63\n",
      "iteration : 150, loss : 0.5407, accuracy : 85.26\n",
      "iteration : 200, loss : 0.5327, accuracy : 85.36\n",
      "iteration : 250, loss : 0.5373, accuracy : 85.26\n",
      "iteration : 300, loss : 0.5396, accuracy : 85.18\n",
      "Epoch : 129, training loss : 0.5356, training accuracy : 85.21, val loss : 0.4156, val accuracy : 87.54\n",
      "\n",
      "Epoch: 130\n",
      "iteration :  50, loss : 0.4742, accuracy : 88.69\n",
      "iteration : 100, loss : 0.4889, accuracy : 87.61\n",
      "iteration : 150, loss : 0.4549, accuracy : 88.32\n",
      "iteration : 200, loss : 0.4600, accuracy : 87.94\n",
      "iteration : 250, loss : 0.4874, accuracy : 87.06\n",
      "iteration : 300, loss : 0.4998, accuracy : 86.46\n",
      "Epoch : 130, training loss : 0.5021, training accuracy : 86.37, val loss : 0.4374, val accuracy : 87.27\n",
      "\n",
      "Epoch: 131\n",
      "iteration :  50, loss : 0.4310, accuracy : 88.48\n",
      "iteration : 100, loss : 0.4683, accuracy : 87.78\n",
      "iteration : 150, loss : 0.5068, accuracy : 86.66\n",
      "iteration : 200, loss : 0.4790, accuracy : 87.36\n",
      "iteration : 250, loss : 0.5009, accuracy : 86.60\n",
      "iteration : 300, loss : 0.5104, accuracy : 86.34\n",
      "Epoch : 131, training loss : 0.5170, training accuracy : 86.11, val loss : 0.4192, val accuracy : 87.84\n",
      "\n",
      "Epoch: 132\n",
      "iteration :  50, loss : 0.5606, accuracy : 84.18\n",
      "iteration : 100, loss : 0.5151, accuracy : 85.70\n",
      "iteration : 150, loss : 0.4832, accuracy : 86.99\n",
      "iteration : 200, loss : 0.4964, accuracy : 86.73\n",
      "iteration : 250, loss : 0.4790, accuracy : 87.22\n",
      "iteration : 300, loss : 0.4875, accuracy : 87.09\n",
      "Epoch : 132, training loss : 0.4846, training accuracy : 87.14, val loss : 0.4085, val accuracy : 87.74\n",
      "\n",
      "Epoch: 133\n",
      "iteration :  50, loss : 0.5043, accuracy : 85.62\n",
      "iteration : 100, loss : 0.5168, accuracy : 85.66\n",
      "iteration : 150, loss : 0.5128, accuracy : 85.93\n",
      "iteration : 200, loss : 0.5045, accuracy : 86.44\n",
      "iteration : 250, loss : 0.4890, accuracy : 86.89\n",
      "iteration : 300, loss : 0.5010, accuracy : 86.53\n",
      "Epoch : 133, training loss : 0.4996, training accuracy : 86.52, val loss : 0.4041, val accuracy : 88.31\n",
      "\n",
      "Epoch: 134\n",
      "iteration :  50, loss : 0.4791, accuracy : 87.06\n",
      "iteration : 100, loss : 0.5493, accuracy : 84.63\n",
      "iteration : 150, loss : 0.5194, accuracy : 85.36\n",
      "iteration : 200, loss : 0.4910, accuracy : 86.33\n",
      "iteration : 250, loss : 0.4946, accuracy : 86.20\n",
      "iteration : 300, loss : 0.4865, accuracy : 86.48\n",
      "Epoch : 134, training loss : 0.4848, training accuracy : 86.51, val loss : 0.4138, val accuracy : 87.79\n",
      "\n",
      "Epoch: 135\n",
      "iteration :  50, loss : 0.5424, accuracy : 85.16\n",
      "iteration : 100, loss : 0.5343, accuracy : 85.11\n",
      "iteration : 150, loss : 0.5496, accuracy : 84.65\n",
      "iteration : 200, loss : 0.5762, accuracy : 84.00\n",
      "iteration : 250, loss : 0.5515, accuracy : 84.85\n",
      "iteration : 300, loss : 0.5569, accuracy : 84.69\n",
      "Epoch : 135, training loss : 0.5498, training accuracy : 84.83, val loss : 0.4016, val accuracy : 88.22\n",
      "\n",
      "Epoch: 136\n",
      "iteration :  50, loss : 0.5494, accuracy : 84.50\n",
      "iteration : 100, loss : 0.5160, accuracy : 86.10\n",
      "iteration : 150, loss : 0.5198, accuracy : 85.72\n",
      "iteration : 200, loss : 0.5175, accuracy : 85.96\n",
      "iteration : 250, loss : 0.4966, accuracy : 86.45\n",
      "iteration : 300, loss : 0.5097, accuracy : 85.93\n",
      "Epoch : 136, training loss : 0.5160, training accuracy : 85.70, val loss : 0.4220, val accuracy : 87.60\n",
      "\n",
      "Epoch: 137\n",
      "iteration :  50, loss : 0.4540, accuracy : 87.43\n",
      "iteration : 100, loss : 0.5007, accuracy : 86.21\n",
      "iteration : 150, loss : 0.5680, accuracy : 84.17\n",
      "iteration : 200, loss : 0.5661, accuracy : 84.14\n",
      "iteration : 250, loss : 0.5555, accuracy : 84.40\n",
      "iteration : 300, loss : 0.5496, accuracy : 84.61\n",
      "Epoch : 137, training loss : 0.5418, training accuracy : 84.86, val loss : 0.4148, val accuracy : 87.17\n",
      "\n",
      "Epoch: 138\n",
      "iteration :  50, loss : 0.4060, accuracy : 89.91\n",
      "iteration : 100, loss : 0.4488, accuracy : 88.68\n",
      "iteration : 150, loss : 0.4177, accuracy : 89.23\n",
      "iteration : 200, loss : 0.4518, accuracy : 87.78\n",
      "iteration : 250, loss : 0.4743, accuracy : 87.12\n",
      "iteration : 300, loss : 0.4829, accuracy : 86.94\n",
      "Epoch : 138, training loss : 0.4848, training accuracy : 86.90, val loss : 0.4307, val accuracy : 88.01\n",
      "\n",
      "Epoch: 139\n",
      "iteration :  50, loss : 0.5154, accuracy : 86.44\n",
      "iteration : 100, loss : 0.4731, accuracy : 88.01\n",
      "iteration : 150, loss : 0.4952, accuracy : 86.86\n",
      "iteration : 200, loss : 0.5054, accuracy : 86.43\n",
      "iteration : 250, loss : 0.5119, accuracy : 86.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 300, loss : 0.5246, accuracy : 85.73\n",
      "Epoch : 139, training loss : 0.5272, training accuracy : 85.76, val loss : 0.4059, val accuracy : 88.27\n",
      "\n",
      "Epoch: 140\n",
      "iteration :  50, loss : 0.4969, accuracy : 86.77\n",
      "iteration : 100, loss : 0.4938, accuracy : 86.88\n",
      "iteration : 150, loss : 0.5068, accuracy : 86.26\n",
      "iteration : 200, loss : 0.5133, accuracy : 86.01\n",
      "iteration : 250, loss : 0.5199, accuracy : 85.67\n",
      "iteration : 300, loss : 0.5085, accuracy : 86.12\n",
      "Saving..\n",
      "Epoch : 140, training loss : 0.5170, training accuracy : 85.87, val loss : 0.4195, val accuracy : 88.54\n",
      "\n",
      "Epoch: 141\n",
      "iteration :  50, loss : 0.5456, accuracy : 85.16\n",
      "iteration : 100, loss : 0.5269, accuracy : 85.66\n",
      "iteration : 150, loss : 0.5457, accuracy : 85.13\n",
      "iteration : 200, loss : 0.5298, accuracy : 85.46\n",
      "iteration : 250, loss : 0.5402, accuracy : 85.17\n",
      "iteration : 300, loss : 0.5276, accuracy : 85.64\n",
      "Epoch : 141, training loss : 0.5386, training accuracy : 85.27, val loss : 0.4240, val accuracy : 88.38\n",
      "\n",
      "Epoch: 142\n",
      "iteration :  50, loss : 0.4683, accuracy : 86.98\n",
      "iteration : 100, loss : 0.4886, accuracy : 86.60\n",
      "iteration : 150, loss : 0.4976, accuracy : 86.30\n",
      "iteration : 200, loss : 0.5008, accuracy : 86.43\n",
      "iteration : 250, loss : 0.5050, accuracy : 86.17\n",
      "iteration : 300, loss : 0.4979, accuracy : 86.40\n",
      "Saving..\n",
      "Epoch : 142, training loss : 0.4935, training accuracy : 86.49, val loss : 0.3910, val accuracy : 88.72\n",
      "\n",
      "Epoch: 143\n",
      "iteration :  50, loss : 0.3970, accuracy : 89.70\n",
      "iteration : 100, loss : 0.4509, accuracy : 87.97\n",
      "iteration : 150, loss : 0.4580, accuracy : 87.90\n",
      "iteration : 200, loss : 0.4882, accuracy : 87.06\n",
      "iteration : 250, loss : 0.4991, accuracy : 86.79\n",
      "iteration : 300, loss : 0.5338, accuracy : 85.59\n",
      "Epoch : 143, training loss : 0.5321, training accuracy : 85.61, val loss : 0.4050, val accuracy : 88.47\n",
      "\n",
      "Epoch: 144\n",
      "iteration :  50, loss : 0.5365, accuracy : 85.81\n",
      "iteration : 100, loss : 0.4994, accuracy : 86.74\n",
      "iteration : 150, loss : 0.4487, accuracy : 88.26\n",
      "iteration : 200, loss : 0.4907, accuracy : 86.87\n",
      "iteration : 250, loss : 0.4781, accuracy : 87.13\n",
      "iteration : 300, loss : 0.4993, accuracy : 86.30\n",
      "Epoch : 144, training loss : 0.4995, training accuracy : 86.25, val loss : 0.3899, val accuracy : 88.47\n",
      "\n",
      "Epoch: 145\n",
      "iteration :  50, loss : 0.5077, accuracy : 86.60\n",
      "iteration : 100, loss : 0.5528, accuracy : 85.18\n",
      "iteration : 150, loss : 0.5310, accuracy : 85.67\n",
      "iteration : 200, loss : 0.5448, accuracy : 84.95\n",
      "iteration : 250, loss : 0.5345, accuracy : 85.36\n",
      "iteration : 300, loss : 0.5391, accuracy : 85.26\n",
      "Epoch : 145, training loss : 0.5391, training accuracy : 85.26, val loss : 0.4293, val accuracy : 88.37\n",
      "\n",
      "Epoch: 146\n",
      "iteration :  50, loss : 0.4485, accuracy : 88.41\n",
      "iteration : 100, loss : 0.4653, accuracy : 88.04\n",
      "iteration : 150, loss : 0.4514, accuracy : 88.21\n",
      "iteration : 200, loss : 0.4516, accuracy : 88.13\n",
      "iteration : 250, loss : 0.4698, accuracy : 87.62\n",
      "iteration : 300, loss : 0.4717, accuracy : 87.53\n",
      "Epoch : 146, training loss : 0.4783, training accuracy : 87.39, val loss : 0.4359, val accuracy : 87.91\n",
      "\n",
      "Epoch: 147\n",
      "iteration :  50, loss : 0.5182, accuracy : 86.08\n",
      "iteration : 100, loss : 0.5506, accuracy : 85.24\n",
      "iteration : 150, loss : 0.5162, accuracy : 86.17\n",
      "iteration : 200, loss : 0.5160, accuracy : 86.03\n",
      "iteration : 250, loss : 0.5094, accuracy : 86.38\n",
      "iteration : 300, loss : 0.5212, accuracy : 85.97\n",
      "Epoch : 147, training loss : 0.5203, training accuracy : 85.94, val loss : 0.4066, val accuracy : 87.99\n",
      "\n",
      "Epoch: 148\n",
      "iteration :  50, loss : 0.4667, accuracy : 87.61\n",
      "iteration : 100, loss : 0.5378, accuracy : 85.57\n",
      "iteration : 150, loss : 0.4623, accuracy : 87.69\n",
      "iteration : 200, loss : 0.4996, accuracy : 86.40\n",
      "iteration : 250, loss : 0.5089, accuracy : 86.16\n",
      "iteration : 300, loss : 0.4982, accuracy : 86.50\n",
      "Saving..\n",
      "Epoch : 148, training loss : 0.5024, training accuracy : 86.42, val loss : 0.3862, val accuracy : 88.92\n",
      "\n",
      "Epoch: 149\n",
      "iteration :  50, loss : 0.5253, accuracy : 86.45\n",
      "iteration : 100, loss : 0.4909, accuracy : 87.56\n",
      "iteration : 150, loss : 0.5127, accuracy : 86.91\n",
      "iteration : 200, loss : 0.5221, accuracy : 86.31\n",
      "iteration : 250, loss : 0.5276, accuracy : 86.17\n",
      "iteration : 300, loss : 0.5298, accuracy : 85.83\n",
      "Epoch : 149, training loss : 0.5205, training accuracy : 86.06, val loss : 0.3880, val accuracy : 88.61\n",
      "\n",
      "Epoch: 150\n",
      "iteration :  50, loss : 0.5793, accuracy : 84.17\n",
      "iteration : 100, loss : 0.5741, accuracy : 84.10\n",
      "iteration : 150, loss : 0.5567, accuracy : 84.28\n",
      "iteration : 200, loss : 0.5535, accuracy : 84.53\n",
      "iteration : 250, loss : 0.5556, accuracy : 84.54\n",
      "iteration : 300, loss : 0.5278, accuracy : 85.41\n",
      "Epoch : 150, training loss : 0.5240, training accuracy : 85.56, val loss : 0.4100, val accuracy : 88.28\n",
      "\n",
      "Epoch: 151\n",
      "iteration :  50, loss : 0.4268, accuracy : 88.99\n",
      "iteration : 100, loss : 0.4653, accuracy : 88.06\n",
      "iteration : 150, loss : 0.4689, accuracy : 87.85\n",
      "iteration : 200, loss : 0.4561, accuracy : 88.03\n",
      "iteration : 250, loss : 0.4537, accuracy : 88.08\n",
      "iteration : 300, loss : 0.4658, accuracy : 87.81\n",
      "Epoch : 151, training loss : 0.4647, training accuracy : 87.88, val loss : 0.3898, val accuracy : 88.27\n",
      "\n",
      "Epoch: 152\n",
      "iteration :  50, loss : 0.3994, accuracy : 90.41\n",
      "iteration : 100, loss : 0.4006, accuracy : 90.15\n",
      "iteration : 150, loss : 0.3941, accuracy : 90.16\n",
      "iteration : 200, loss : 0.3911, accuracy : 90.03\n",
      "iteration : 250, loss : 0.4085, accuracy : 89.46\n",
      "iteration : 300, loss : 0.4186, accuracy : 89.21\n",
      "Saving..\n",
      "Epoch : 152, training loss : 0.4198, training accuracy : 89.22, val loss : 0.3968, val accuracy : 88.94\n",
      "\n",
      "Epoch: 153\n",
      "iteration :  50, loss : 0.4995, accuracy : 87.16\n",
      "iteration : 100, loss : 0.5018, accuracy : 86.85\n",
      "iteration : 150, loss : 0.5139, accuracy : 86.40\n",
      "iteration : 200, loss : 0.5095, accuracy : 86.52\n",
      "iteration : 250, loss : 0.5078, accuracy : 86.73\n",
      "iteration : 300, loss : 0.5216, accuracy : 86.21\n",
      "Epoch : 153, training loss : 0.5328, training accuracy : 85.98, val loss : 0.4406, val accuracy : 88.45\n",
      "\n",
      "Epoch: 154\n",
      "iteration :  50, loss : 0.4558, accuracy : 88.13\n",
      "iteration : 100, loss : 0.4545, accuracy : 87.81\n",
      "iteration : 150, loss : 0.5110, accuracy : 86.02\n",
      "iteration : 200, loss : 0.5157, accuracy : 85.88\n",
      "iteration : 250, loss : 0.4958, accuracy : 86.37\n",
      "iteration : 300, loss : 0.4922, accuracy : 86.50\n",
      "Saving..\n",
      "Epoch : 154, training loss : 0.4913, training accuracy : 86.50, val loss : 0.3911, val accuracy : 89.08\n",
      "\n",
      "Epoch: 155\n",
      "iteration :  50, loss : 0.5932, accuracy : 82.73\n",
      "iteration : 100, loss : 0.5306, accuracy : 85.04\n",
      "iteration : 150, loss : 0.4793, accuracy : 86.71\n",
      "iteration : 200, loss : 0.4933, accuracy : 86.37\n",
      "iteration : 250, loss : 0.5126, accuracy : 85.84\n",
      "iteration : 300, loss : 0.4927, accuracy : 86.50\n",
      "Epoch : 155, training loss : 0.5038, training accuracy : 86.26, val loss : 0.4247, val accuracy : 88.29\n",
      "\n",
      "Epoch: 156\n",
      "iteration :  50, loss : 0.5144, accuracy : 85.73\n",
      "iteration : 100, loss : 0.5206, accuracy : 85.91\n",
      "iteration : 150, loss : 0.5177, accuracy : 85.94\n",
      "iteration : 200, loss : 0.5220, accuracy : 85.87\n",
      "iteration : 250, loss : 0.5307, accuracy : 85.61\n",
      "iteration : 300, loss : 0.5398, accuracy : 85.35\n",
      "Epoch : 156, training loss : 0.5360, training accuracy : 85.48, val loss : 0.4067, val accuracy : 88.68\n",
      "\n",
      "Epoch: 157\n",
      "iteration :  50, loss : 0.4824, accuracy : 87.34\n",
      "iteration : 100, loss : 0.4884, accuracy : 87.11\n",
      "iteration : 150, loss : 0.5241, accuracy : 85.79\n",
      "iteration : 200, loss : 0.5123, accuracy : 85.96\n",
      "iteration : 250, loss : 0.4981, accuracy : 86.43\n",
      "iteration : 300, loss : 0.5103, accuracy : 85.88\n",
      "Epoch : 157, training loss : 0.5088, training accuracy : 85.91, val loss : 0.3956, val accuracy : 88.86\n",
      "\n",
      "Epoch: 158\n",
      "iteration :  50, loss : 0.5219, accuracy : 86.39\n",
      "iteration : 100, loss : 0.4616, accuracy : 87.93\n",
      "iteration : 150, loss : 0.4264, accuracy : 89.01\n",
      "iteration : 200, loss : 0.4665, accuracy : 87.65\n",
      "iteration : 250, loss : 0.4742, accuracy : 87.45\n",
      "iteration : 300, loss : 0.4660, accuracy : 87.66\n",
      "Epoch : 158, training loss : 0.4651, training accuracy : 87.70, val loss : 0.3977, val accuracy : 88.31\n",
      "\n",
      "Epoch: 159\n",
      "iteration :  50, loss : 0.4998, accuracy : 86.78\n",
      "iteration : 100, loss : 0.4490, accuracy : 88.61\n",
      "iteration : 150, loss : 0.4561, accuracy : 88.23\n",
      "iteration : 200, loss : 0.5101, accuracy : 86.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 250, loss : 0.5222, accuracy : 85.74\n",
      "iteration : 300, loss : 0.5233, accuracy : 85.81\n",
      "Epoch : 159, training loss : 0.5201, training accuracy : 85.89, val loss : 0.3868, val accuracy : 88.84\n",
      "\n",
      "Epoch: 160\n",
      "iteration :  50, loss : 0.4978, accuracy : 86.67\n",
      "iteration : 100, loss : 0.5581, accuracy : 84.66\n",
      "iteration : 150, loss : 0.4991, accuracy : 86.54\n",
      "iteration : 200, loss : 0.4912, accuracy : 86.53\n",
      "iteration : 250, loss : 0.4657, accuracy : 87.24\n",
      "iteration : 300, loss : 0.4725, accuracy : 87.04\n",
      "Epoch : 160, training loss : 0.4671, training accuracy : 87.26, val loss : 0.3831, val accuracy : 88.88\n",
      "\n",
      "Epoch: 161\n",
      "iteration :  50, loss : 0.4902, accuracy : 86.94\n",
      "iteration : 100, loss : 0.4755, accuracy : 87.32\n",
      "iteration : 150, loss : 0.4952, accuracy : 86.91\n",
      "iteration : 200, loss : 0.5358, accuracy : 85.61\n",
      "iteration : 250, loss : 0.5055, accuracy : 86.38\n",
      "iteration : 300, loss : 0.5021, accuracy : 86.43\n",
      "Epoch : 161, training loss : 0.5029, training accuracy : 86.42, val loss : 0.4102, val accuracy : 88.72\n",
      "\n",
      "Epoch: 162\n",
      "iteration :  50, loss : 0.5261, accuracy : 86.34\n",
      "iteration : 100, loss : 0.5218, accuracy : 86.19\n",
      "iteration : 150, loss : 0.4947, accuracy : 86.84\n",
      "iteration : 200, loss : 0.5037, accuracy : 86.34\n",
      "iteration : 250, loss : 0.4763, accuracy : 87.15\n",
      "iteration : 300, loss : 0.4780, accuracy : 87.14\n",
      "Epoch : 162, training loss : 0.4708, training accuracy : 87.41, val loss : 0.3780, val accuracy : 89.00\n",
      "\n",
      "Epoch: 163\n",
      "iteration :  50, loss : 0.6113, accuracy : 83.14\n",
      "iteration : 100, loss : 0.5248, accuracy : 85.68\n",
      "iteration : 150, loss : 0.4980, accuracy : 86.55\n",
      "iteration : 200, loss : 0.5032, accuracy : 86.37\n",
      "iteration : 250, loss : 0.5153, accuracy : 85.93\n",
      "iteration : 300, loss : 0.5213, accuracy : 85.75\n",
      "Epoch : 163, training loss : 0.5121, training accuracy : 85.96, val loss : 0.3957, val accuracy : 88.84\n",
      "\n",
      "Epoch: 164\n",
      "iteration :  50, loss : 0.4779, accuracy : 86.99\n",
      "iteration : 100, loss : 0.4654, accuracy : 87.30\n",
      "iteration : 150, loss : 0.4927, accuracy : 86.44\n",
      "iteration : 200, loss : 0.5072, accuracy : 86.29\n",
      "iteration : 250, loss : 0.5256, accuracy : 85.78\n",
      "iteration : 300, loss : 0.5152, accuracy : 86.06\n",
      "Epoch : 164, training loss : 0.5111, training accuracy : 86.18, val loss : 0.3909, val accuracy : 89.02\n",
      "\n",
      "Epoch: 165\n",
      "iteration :  50, loss : 0.4258, accuracy : 89.85\n",
      "iteration : 100, loss : 0.3888, accuracy : 90.28\n",
      "iteration : 150, loss : 0.4267, accuracy : 88.97\n",
      "iteration : 200, loss : 0.4406, accuracy : 88.60\n",
      "iteration : 250, loss : 0.4330, accuracy : 88.82\n",
      "iteration : 300, loss : 0.4569, accuracy : 88.03\n",
      "Epoch : 165, training loss : 0.4455, training accuracy : 88.33, val loss : 0.3821, val accuracy : 88.90\n",
      "\n",
      "Epoch: 166\n",
      "iteration :  50, loss : 0.5724, accuracy : 84.56\n",
      "iteration : 100, loss : 0.5079, accuracy : 86.37\n",
      "iteration : 150, loss : 0.5091, accuracy : 86.10\n",
      "iteration : 200, loss : 0.5055, accuracy : 86.27\n",
      "iteration : 250, loss : 0.4926, accuracy : 86.63\n",
      "iteration : 300, loss : 0.4928, accuracy : 86.70\n",
      "Saving..\n",
      "Epoch : 166, training loss : 0.4832, training accuracy : 86.93, val loss : 0.3719, val accuracy : 89.35\n",
      "\n",
      "Epoch: 167\n",
      "iteration :  50, loss : 0.4914, accuracy : 87.03\n",
      "iteration : 100, loss : 0.5340, accuracy : 85.52\n",
      "iteration : 150, loss : 0.5509, accuracy : 84.77\n",
      "iteration : 200, loss : 0.5389, accuracy : 85.33\n",
      "iteration : 250, loss : 0.5047, accuracy : 86.42\n",
      "iteration : 300, loss : 0.5046, accuracy : 86.34\n",
      "Epoch : 167, training loss : 0.5049, training accuracy : 86.33, val loss : 0.3905, val accuracy : 88.96\n",
      "\n",
      "Epoch: 168\n",
      "iteration :  50, loss : 0.6412, accuracy : 81.92\n",
      "iteration : 100, loss : 0.5267, accuracy : 85.50\n",
      "iteration : 150, loss : 0.4944, accuracy : 86.55\n",
      "iteration : 200, loss : 0.4583, accuracy : 87.64\n",
      "iteration : 250, loss : 0.4362, accuracy : 88.32\n",
      "iteration : 300, loss : 0.4481, accuracy : 87.94\n",
      "Epoch : 168, training loss : 0.4540, training accuracy : 87.78, val loss : 0.4115, val accuracy : 88.69\n",
      "\n",
      "Epoch: 169\n",
      "iteration :  50, loss : 0.5216, accuracy : 85.70\n",
      "iteration : 100, loss : 0.4624, accuracy : 87.46\n",
      "iteration : 150, loss : 0.4373, accuracy : 88.38\n",
      "iteration : 200, loss : 0.4580, accuracy : 87.65\n",
      "iteration : 250, loss : 0.4831, accuracy : 86.94\n",
      "iteration : 300, loss : 0.4847, accuracy : 86.83\n",
      "Epoch : 169, training loss : 0.4977, training accuracy : 86.36, val loss : 0.3915, val accuracy : 89.12\n",
      "\n",
      "Epoch: 170\n",
      "iteration :  50, loss : 0.5818, accuracy : 84.63\n",
      "iteration : 100, loss : 0.5088, accuracy : 86.12\n",
      "iteration : 150, loss : 0.4820, accuracy : 87.13\n",
      "iteration : 200, loss : 0.4693, accuracy : 87.51\n",
      "iteration : 250, loss : 0.4810, accuracy : 87.08\n",
      "iteration : 300, loss : 0.4763, accuracy : 87.12\n",
      "Saving..\n",
      "Epoch : 170, training loss : 0.4777, training accuracy : 87.05, val loss : 0.3734, val accuracy : 89.54\n",
      "\n",
      "Epoch: 171\n",
      "iteration :  50, loss : 0.5545, accuracy : 85.64\n",
      "iteration : 100, loss : 0.5250, accuracy : 86.33\n",
      "iteration : 150, loss : 0.4982, accuracy : 87.02\n",
      "iteration : 200, loss : 0.4777, accuracy : 87.60\n",
      "iteration : 250, loss : 0.4748, accuracy : 87.63\n",
      "iteration : 300, loss : 0.4750, accuracy : 87.52\n",
      "Epoch : 171, training loss : 0.4711, training accuracy : 87.63, val loss : 0.3840, val accuracy : 88.83\n",
      "\n",
      "Epoch: 172\n",
      "iteration :  50, loss : 0.4654, accuracy : 88.36\n",
      "iteration : 100, loss : 0.4206, accuracy : 89.24\n",
      "iteration : 150, loss : 0.4546, accuracy : 87.98\n",
      "iteration : 200, loss : 0.4505, accuracy : 88.00\n",
      "iteration : 250, loss : 0.4502, accuracy : 87.96\n",
      "iteration : 300, loss : 0.4616, accuracy : 87.61\n",
      "Epoch : 172, training loss : 0.4623, training accuracy : 87.62, val loss : 0.3989, val accuracy : 88.71\n",
      "\n",
      "Epoch: 173\n",
      "iteration :  50, loss : 0.5058, accuracy : 86.55\n",
      "iteration : 100, loss : 0.5059, accuracy : 86.38\n",
      "iteration : 150, loss : 0.5008, accuracy : 86.57\n",
      "iteration : 200, loss : 0.4854, accuracy : 87.08\n",
      "iteration : 250, loss : 0.4843, accuracy : 86.97\n",
      "iteration : 300, loss : 0.4778, accuracy : 87.08\n",
      "Epoch : 173, training loss : 0.4751, training accuracy : 87.16, val loss : 0.3958, val accuracy : 88.63\n",
      "\n",
      "Epoch: 174\n",
      "iteration :  50, loss : 0.4092, accuracy : 89.41\n",
      "iteration : 100, loss : 0.5328, accuracy : 85.20\n",
      "iteration : 150, loss : 0.5061, accuracy : 86.23\n",
      "iteration : 200, loss : 0.5193, accuracy : 85.70\n",
      "iteration : 250, loss : 0.5355, accuracy : 85.21\n",
      "iteration : 300, loss : 0.5207, accuracy : 85.76\n",
      "Epoch : 174, training loss : 0.5164, training accuracy : 85.90, val loss : 0.3660, val accuracy : 89.46\n",
      "\n",
      "Epoch: 175\n",
      "iteration :  50, loss : 0.4548, accuracy : 87.99\n",
      "iteration : 100, loss : 0.4818, accuracy : 87.34\n",
      "iteration : 150, loss : 0.4521, accuracy : 88.03\n",
      "iteration : 200, loss : 0.4599, accuracy : 87.59\n",
      "iteration : 250, loss : 0.4615, accuracy : 87.58\n",
      "iteration : 300, loss : 0.4521, accuracy : 87.81\n",
      "Epoch : 175, training loss : 0.4533, training accuracy : 87.80, val loss : 0.3824, val accuracy : 89.22\n",
      "\n",
      "Epoch: 176\n",
      "iteration :  50, loss : 0.4340, accuracy : 88.59\n",
      "iteration : 100, loss : 0.3928, accuracy : 90.14\n",
      "iteration : 150, loss : 0.4017, accuracy : 89.74\n",
      "iteration : 200, loss : 0.4242, accuracy : 89.15\n",
      "iteration : 250, loss : 0.4536, accuracy : 88.04\n",
      "iteration : 300, loss : 0.4304, accuracy : 88.71\n",
      "Epoch : 176, training loss : 0.4255, training accuracy : 88.83, val loss : 0.3904, val accuracy : 88.84\n",
      "\n",
      "Epoch: 177\n",
      "iteration :  50, loss : 0.3942, accuracy : 89.62\n",
      "iteration : 100, loss : 0.4424, accuracy : 88.31\n",
      "iteration : 150, loss : 0.4028, accuracy : 89.58\n",
      "iteration : 200, loss : 0.4291, accuracy : 88.58\n",
      "iteration : 250, loss : 0.4152, accuracy : 89.05\n",
      "iteration : 300, loss : 0.4196, accuracy : 88.90\n",
      "Epoch : 177, training loss : 0.4258, training accuracy : 88.73, val loss : 0.3872, val accuracy : 89.52\n",
      "\n",
      "Epoch: 178\n",
      "iteration :  50, loss : 0.4831, accuracy : 87.37\n",
      "iteration : 100, loss : 0.4448, accuracy : 88.16\n",
      "iteration : 150, loss : 0.4248, accuracy : 88.91\n",
      "iteration : 200, loss : 0.3950, accuracy : 89.70\n",
      "iteration : 250, loss : 0.4530, accuracy : 88.01\n",
      "iteration : 300, loss : 0.4605, accuracy : 87.94\n",
      "Epoch : 178, training loss : 0.4590, training accuracy : 87.97, val loss : 0.3982, val accuracy : 89.20\n",
      "\n",
      "Epoch: 179\n",
      "iteration :  50, loss : 0.4379, accuracy : 88.72\n",
      "iteration : 100, loss : 0.4848, accuracy : 87.19\n",
      "iteration : 150, loss : 0.4520, accuracy : 88.22\n",
      "iteration : 200, loss : 0.4884, accuracy : 86.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 250, loss : 0.4887, accuracy : 86.94\n",
      "iteration : 300, loss : 0.4923, accuracy : 86.84\n",
      "Epoch : 179, training loss : 0.4789, training accuracy : 87.24, val loss : 0.3893, val accuracy : 89.04\n",
      "\n",
      "Epoch: 180\n",
      "iteration :  50, loss : 0.3616, accuracy : 90.23\n",
      "iteration : 100, loss : 0.3797, accuracy : 90.08\n",
      "iteration : 150, loss : 0.3684, accuracy : 90.31\n",
      "iteration : 200, loss : 0.4223, accuracy : 88.73\n",
      "iteration : 250, loss : 0.4426, accuracy : 87.98\n",
      "iteration : 300, loss : 0.4485, accuracy : 87.89\n",
      "Epoch : 180, training loss : 0.4444, training accuracy : 88.05, val loss : 0.3862, val accuracy : 89.17\n",
      "\n",
      "Epoch: 181\n",
      "iteration :  50, loss : 0.4906, accuracy : 86.95\n",
      "iteration : 100, loss : 0.4920, accuracy : 86.76\n",
      "iteration : 150, loss : 0.5153, accuracy : 86.06\n",
      "iteration : 200, loss : 0.4925, accuracy : 87.03\n",
      "iteration : 250, loss : 0.5002, accuracy : 86.64\n",
      "iteration : 300, loss : 0.4938, accuracy : 86.91\n",
      "Epoch : 181, training loss : 0.4937, training accuracy : 86.90, val loss : 0.3974, val accuracy : 89.19\n",
      "\n",
      "Epoch: 182\n",
      "iteration :  50, loss : 0.4076, accuracy : 89.83\n",
      "iteration : 100, loss : 0.4713, accuracy : 87.48\n",
      "iteration : 150, loss : 0.4535, accuracy : 87.92\n",
      "iteration : 200, loss : 0.4594, accuracy : 87.63\n",
      "iteration : 250, loss : 0.4738, accuracy : 87.14\n",
      "iteration : 300, loss : 0.5051, accuracy : 86.19\n",
      "Epoch : 182, training loss : 0.5103, training accuracy : 86.16, val loss : 0.3917, val accuracy : 89.43\n",
      "\n",
      "Epoch: 183\n",
      "iteration :  50, loss : 0.4411, accuracy : 87.94\n",
      "iteration : 100, loss : 0.4533, accuracy : 87.20\n",
      "iteration : 150, loss : 0.4350, accuracy : 87.97\n",
      "iteration : 200, loss : 0.4465, accuracy : 87.81\n",
      "iteration : 250, loss : 0.4970, accuracy : 86.32\n",
      "iteration : 300, loss : 0.4897, accuracy : 86.56\n",
      "Saving..\n",
      "Epoch : 183, training loss : 0.4863, training accuracy : 86.65, val loss : 0.3747, val accuracy : 89.68\n",
      "\n",
      "Epoch: 184\n",
      "iteration :  50, loss : 0.5411, accuracy : 84.96\n",
      "iteration : 100, loss : 0.4866, accuracy : 86.85\n",
      "iteration : 150, loss : 0.4835, accuracy : 87.18\n",
      "iteration : 200, loss : 0.4697, accuracy : 87.83\n",
      "iteration : 250, loss : 0.4534, accuracy : 88.21\n",
      "iteration : 300, loss : 0.4676, accuracy : 87.73\n",
      "Epoch : 184, training loss : 0.4606, training accuracy : 87.89, val loss : 0.3741, val accuracy : 89.54\n",
      "\n",
      "Epoch: 185\n",
      "iteration :  50, loss : 0.4426, accuracy : 87.95\n",
      "iteration : 100, loss : 0.4743, accuracy : 87.38\n",
      "iteration : 150, loss : 0.4550, accuracy : 88.19\n",
      "iteration : 200, loss : 0.4592, accuracy : 88.14\n",
      "iteration : 250, loss : 0.4407, accuracy : 88.74\n",
      "iteration : 300, loss : 0.4450, accuracy : 88.58\n",
      "Epoch : 185, training loss : 0.4460, training accuracy : 88.59, val loss : 0.4064, val accuracy : 88.77\n",
      "\n",
      "Epoch: 186\n",
      "iteration :  50, loss : 0.5692, accuracy : 85.06\n",
      "iteration : 100, loss : 0.5330, accuracy : 86.43\n",
      "iteration : 150, loss : 0.4851, accuracy : 87.69\n",
      "iteration : 200, loss : 0.4708, accuracy : 87.96\n",
      "iteration : 250, loss : 0.4719, accuracy : 88.03\n",
      "iteration : 300, loss : 0.4738, accuracy : 87.93\n",
      "Epoch : 186, training loss : 0.4688, training accuracy : 88.05, val loss : 0.3786, val accuracy : 89.32\n",
      "\n",
      "Epoch: 187\n",
      "iteration :  50, loss : 0.4907, accuracy : 86.96\n",
      "iteration : 100, loss : 0.4612, accuracy : 87.89\n",
      "iteration : 150, loss : 0.4418, accuracy : 88.51\n",
      "iteration : 200, loss : 0.4548, accuracy : 88.12\n",
      "iteration : 250, loss : 0.4452, accuracy : 88.42\n",
      "iteration : 300, loss : 0.4450, accuracy : 88.31\n",
      "Epoch : 187, training loss : 0.4496, training accuracy : 88.21, val loss : 0.4106, val accuracy : 89.50\n",
      "\n",
      "Epoch: 188\n",
      "iteration :  50, loss : 0.3878, accuracy : 89.76\n",
      "iteration : 100, loss : 0.4091, accuracy : 89.19\n",
      "iteration : 150, loss : 0.4500, accuracy : 88.21\n",
      "iteration : 200, loss : 0.4491, accuracy : 88.26\n",
      "iteration : 250, loss : 0.4655, accuracy : 87.53\n",
      "iteration : 300, loss : 0.4466, accuracy : 88.07\n",
      "Epoch : 188, training loss : 0.4437, training accuracy : 88.21, val loss : 0.3768, val accuracy : 89.54\n",
      "\n",
      "Epoch: 189\n",
      "iteration :  50, loss : 0.3750, accuracy : 90.74\n",
      "iteration : 100, loss : 0.4157, accuracy : 89.80\n",
      "iteration : 150, loss : 0.4337, accuracy : 89.22\n",
      "iteration : 200, loss : 0.4474, accuracy : 88.74\n",
      "iteration : 250, loss : 0.4342, accuracy : 88.95\n",
      "iteration : 300, loss : 0.4306, accuracy : 88.95\n",
      "Epoch : 189, training loss : 0.4334, training accuracy : 88.89, val loss : 0.3889, val accuracy : 89.02\n",
      "\n",
      "Epoch: 190\n",
      "iteration :  50, loss : 0.4677, accuracy : 87.71\n",
      "iteration : 100, loss : 0.4129, accuracy : 88.83\n",
      "iteration : 150, loss : 0.4159, accuracy : 88.96\n",
      "iteration : 200, loss : 0.4091, accuracy : 89.42\n",
      "iteration : 250, loss : 0.4099, accuracy : 89.36\n",
      "iteration : 300, loss : 0.4090, accuracy : 89.42\n",
      "Epoch : 190, training loss : 0.4066, training accuracy : 89.50, val loss : 0.3806, val accuracy : 89.23\n",
      "\n",
      "Epoch: 191\n",
      "iteration :  50, loss : 0.4373, accuracy : 89.22\n",
      "iteration : 100, loss : 0.3891, accuracy : 89.85\n",
      "iteration : 150, loss : 0.3972, accuracy : 89.69\n",
      "iteration : 200, loss : 0.3835, accuracy : 90.11\n",
      "iteration : 250, loss : 0.4141, accuracy : 89.16\n",
      "iteration : 300, loss : 0.4256, accuracy : 88.82\n",
      "Epoch : 191, training loss : 0.4253, training accuracy : 88.80, val loss : 0.3894, val accuracy : 89.14\n",
      "\n",
      "Epoch: 192\n",
      "iteration :  50, loss : 0.4327, accuracy : 88.86\n",
      "iteration : 100, loss : 0.4112, accuracy : 89.83\n",
      "iteration : 150, loss : 0.3860, accuracy : 90.45\n",
      "iteration : 200, loss : 0.3765, accuracy : 90.69\n",
      "iteration : 250, loss : 0.4031, accuracy : 89.94\n",
      "iteration : 300, loss : 0.4078, accuracy : 89.61\n",
      "Saving..\n",
      "Epoch : 192, training loss : 0.4147, training accuracy : 89.37, val loss : 0.3661, val accuracy : 89.72\n",
      "\n",
      "Epoch: 193\n",
      "iteration :  50, loss : 0.3797, accuracy : 90.01\n",
      "iteration : 100, loss : 0.4902, accuracy : 85.98\n",
      "iteration : 150, loss : 0.4837, accuracy : 86.54\n",
      "iteration : 200, loss : 0.4846, accuracy : 86.73\n",
      "iteration : 250, loss : 0.4954, accuracy : 86.63\n",
      "iteration : 300, loss : 0.4744, accuracy : 87.23\n",
      "Epoch : 193, training loss : 0.4677, training accuracy : 87.48, val loss : 0.3919, val accuracy : 89.38\n",
      "\n",
      "Epoch: 194\n",
      "iteration :  50, loss : 0.3905, accuracy : 90.00\n",
      "iteration : 100, loss : 0.3991, accuracy : 89.77\n",
      "iteration : 150, loss : 0.4013, accuracy : 90.09\n",
      "iteration : 200, loss : 0.3745, accuracy : 90.71\n",
      "iteration : 250, loss : 0.3575, accuracy : 91.21\n",
      "iteration : 300, loss : 0.3898, accuracy : 90.28\n",
      "Epoch : 194, training loss : 0.3934, training accuracy : 90.19, val loss : 0.3881, val accuracy : 89.47\n",
      "\n",
      "Epoch: 195\n",
      "iteration :  50, loss : 0.3661, accuracy : 90.84\n",
      "iteration : 100, loss : 0.3604, accuracy : 90.85\n",
      "iteration : 150, loss : 0.4035, accuracy : 89.65\n",
      "iteration : 200, loss : 0.4082, accuracy : 89.61\n",
      "iteration : 250, loss : 0.3846, accuracy : 90.35\n",
      "iteration : 300, loss : 0.4041, accuracy : 89.69\n",
      "Epoch : 195, training loss : 0.4039, training accuracy : 89.62, val loss : 0.3868, val accuracy : 89.49\n",
      "\n",
      "Epoch: 196\n",
      "iteration :  50, loss : 0.4399, accuracy : 87.97\n",
      "iteration : 100, loss : 0.4997, accuracy : 86.72\n",
      "iteration : 150, loss : 0.5032, accuracy : 86.55\n",
      "iteration : 200, loss : 0.4637, accuracy : 87.64\n",
      "iteration : 250, loss : 0.4695, accuracy : 87.52\n",
      "iteration : 300, loss : 0.4616, accuracy : 87.79\n",
      "Epoch : 196, training loss : 0.4590, training accuracy : 87.85, val loss : 0.3803, val accuracy : 89.62\n",
      "\n",
      "Epoch: 197\n",
      "iteration :  50, loss : 0.4606, accuracy : 87.63\n",
      "iteration : 100, loss : 0.4394, accuracy : 88.38\n",
      "iteration : 150, loss : 0.4220, accuracy : 88.69\n",
      "iteration : 200, loss : 0.3944, accuracy : 89.55\n",
      "iteration : 250, loss : 0.4063, accuracy : 89.18\n",
      "iteration : 300, loss : 0.3913, accuracy : 89.76\n",
      "Saving..\n",
      "Epoch : 197, training loss : 0.3933, training accuracy : 89.77, val loss : 0.3792, val accuracy : 89.79\n",
      "\n",
      "Epoch: 198\n",
      "iteration :  50, loss : 0.4551, accuracy : 86.79\n",
      "iteration : 100, loss : 0.3758, accuracy : 89.88\n",
      "iteration : 150, loss : 0.3437, accuracy : 90.89\n",
      "iteration : 200, loss : 0.3606, accuracy : 90.52\n",
      "iteration : 250, loss : 0.3625, accuracy : 90.57\n",
      "iteration : 300, loss : 0.3981, accuracy : 89.29\n",
      "Saving..\n",
      "Epoch : 198, training loss : 0.3997, training accuracy : 89.30, val loss : 0.3720, val accuracy : 90.28\n",
      "\n",
      "Epoch: 199\n",
      "iteration :  50, loss : 0.4342, accuracy : 88.81\n",
      "iteration : 100, loss : 0.4577, accuracy : 88.41\n",
      "iteration : 150, loss : 0.4247, accuracy : 89.04\n",
      "iteration : 200, loss : 0.4106, accuracy : 89.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 250, loss : 0.4349, accuracy : 88.61\n",
      "iteration : 300, loss : 0.4573, accuracy : 87.95\n",
      "Epoch : 199, training loss : 0.4591, training accuracy : 87.90, val loss : 0.3865, val accuracy : 89.46\n",
      "\n",
      "Epoch: 200\n",
      "iteration :  50, loss : 0.5013, accuracy : 86.68\n",
      "iteration : 100, loss : 0.5067, accuracy : 86.55\n",
      "iteration : 150, loss : 0.4846, accuracy : 86.87\n",
      "iteration : 200, loss : 0.4620, accuracy : 87.47\n",
      "iteration : 250, loss : 0.4584, accuracy : 87.60\n",
      "iteration : 300, loss : 0.4528, accuracy : 87.76\n",
      "Epoch : 200, training loss : 0.4461, training accuracy : 87.93, val loss : 0.3658, val accuracy : 90.13\n",
      "\n",
      "Epoch: 201\n",
      "iteration :  50, loss : 0.4764, accuracy : 88.06\n",
      "iteration : 100, loss : 0.4764, accuracy : 87.37\n",
      "iteration : 150, loss : 0.4358, accuracy : 88.78\n",
      "iteration : 200, loss : 0.4583, accuracy : 88.22\n",
      "iteration : 250, loss : 0.4471, accuracy : 88.61\n",
      "iteration : 300, loss : 0.4569, accuracy : 88.30\n",
      "Epoch : 201, training loss : 0.4533, training accuracy : 88.40, val loss : 0.3868, val accuracy : 89.54\n",
      "\n",
      "Epoch: 202\n",
      "iteration :  50, loss : 0.4838, accuracy : 86.46\n",
      "iteration : 100, loss : 0.4377, accuracy : 88.60\n",
      "iteration : 150, loss : 0.4010, accuracy : 89.69\n",
      "iteration : 200, loss : 0.4145, accuracy : 89.27\n",
      "iteration : 250, loss : 0.4303, accuracy : 88.71\n",
      "iteration : 300, loss : 0.4427, accuracy : 88.33\n",
      "Epoch : 202, training loss : 0.4347, training accuracy : 88.60, val loss : 0.3710, val accuracy : 89.77\n",
      "\n",
      "Epoch: 203\n",
      "iteration :  50, loss : 0.3982, accuracy : 90.44\n",
      "iteration : 100, loss : 0.3466, accuracy : 91.92\n",
      "iteration : 150, loss : 0.3582, accuracy : 91.59\n",
      "iteration : 200, loss : 0.3775, accuracy : 90.79\n",
      "iteration : 250, loss : 0.4163, accuracy : 89.43\n",
      "iteration : 300, loss : 0.4124, accuracy : 89.47\n",
      "Epoch : 203, training loss : 0.4073, training accuracy : 89.67, val loss : 0.3696, val accuracy : 89.82\n",
      "\n",
      "Epoch: 204\n",
      "iteration :  50, loss : 0.5219, accuracy : 85.16\n",
      "iteration : 100, loss : 0.4462, accuracy : 87.75\n",
      "iteration : 150, loss : 0.4396, accuracy : 87.86\n",
      "iteration : 200, loss : 0.4623, accuracy : 87.28\n",
      "iteration : 250, loss : 0.4386, accuracy : 88.08\n",
      "iteration : 300, loss : 0.4324, accuracy : 88.41\n",
      "Epoch : 204, training loss : 0.4269, training accuracy : 88.65, val loss : 0.3693, val accuracy : 90.08\n",
      "\n",
      "Epoch: 205\n",
      "iteration :  50, loss : 0.3893, accuracy : 90.36\n",
      "iteration : 100, loss : 0.4443, accuracy : 88.32\n",
      "iteration : 150, loss : 0.4519, accuracy : 88.16\n",
      "iteration : 200, loss : 0.4351, accuracy : 88.74\n",
      "iteration : 250, loss : 0.4266, accuracy : 89.04\n",
      "iteration : 300, loss : 0.4405, accuracy : 88.48\n",
      "Saving..\n",
      "Epoch : 205, training loss : 0.4338, training accuracy : 88.60, val loss : 0.3600, val accuracy : 90.32\n",
      "\n",
      "Epoch: 206\n",
      "iteration :  50, loss : 0.4075, accuracy : 90.31\n",
      "iteration : 100, loss : 0.3862, accuracy : 90.31\n",
      "iteration : 150, loss : 0.3866, accuracy : 90.03\n",
      "iteration : 200, loss : 0.3902, accuracy : 89.85\n",
      "iteration : 250, loss : 0.3614, accuracy : 90.66\n",
      "iteration : 300, loss : 0.3606, accuracy : 90.62\n",
      "Epoch : 206, training loss : 0.3658, training accuracy : 90.45, val loss : 0.3792, val accuracy : 89.92\n",
      "\n",
      "Epoch: 207\n",
      "iteration :  50, loss : 0.5684, accuracy : 84.01\n",
      "iteration : 100, loss : 0.5715, accuracy : 84.47\n",
      "iteration : 150, loss : 0.4999, accuracy : 86.63\n",
      "iteration : 200, loss : 0.4831, accuracy : 87.31\n",
      "iteration : 250, loss : 0.4832, accuracy : 87.08\n",
      "iteration : 300, loss : 0.4585, accuracy : 87.75\n",
      "Epoch : 207, training loss : 0.4536, training accuracy : 87.90, val loss : 0.3625, val accuracy : 90.08\n",
      "\n",
      "Epoch: 208\n",
      "iteration :  50, loss : 0.3481, accuracy : 91.93\n",
      "iteration : 100, loss : 0.4260, accuracy : 89.50\n",
      "iteration : 150, loss : 0.4055, accuracy : 90.00\n",
      "iteration : 200, loss : 0.3871, accuracy : 90.30\n",
      "iteration : 250, loss : 0.3978, accuracy : 89.81\n",
      "iteration : 300, loss : 0.3878, accuracy : 90.10\n",
      "Saving..\n",
      "Epoch : 208, training loss : 0.3830, training accuracy : 90.29, val loss : 0.3627, val accuracy : 90.33\n",
      "\n",
      "Epoch: 209\n",
      "iteration :  50, loss : 0.5488, accuracy : 85.31\n",
      "iteration : 100, loss : 0.5054, accuracy : 86.53\n",
      "iteration : 150, loss : 0.4850, accuracy : 87.26\n",
      "iteration : 200, loss : 0.4384, accuracy : 88.69\n",
      "iteration : 250, loss : 0.4366, accuracy : 88.62\n",
      "iteration : 300, loss : 0.4384, accuracy : 88.66\n",
      "Epoch : 209, training loss : 0.4414, training accuracy : 88.48, val loss : 0.3788, val accuracy : 89.72\n",
      "\n",
      "Epoch: 210\n",
      "iteration :  50, loss : 0.4073, accuracy : 90.00\n",
      "iteration : 100, loss : 0.3526, accuracy : 91.21\n",
      "iteration : 150, loss : 0.3805, accuracy : 90.11\n",
      "iteration : 200, loss : 0.3851, accuracy : 90.02\n",
      "iteration : 250, loss : 0.4122, accuracy : 89.24\n",
      "iteration : 300, loss : 0.4033, accuracy : 89.47\n",
      "Saving..\n",
      "Epoch : 210, training loss : 0.3936, training accuracy : 89.76, val loss : 0.3678, val accuracy : 90.36\n",
      "\n",
      "Epoch: 211\n",
      "iteration :  50, loss : 0.2321, accuracy : 94.47\n",
      "iteration : 100, loss : 0.3174, accuracy : 92.07\n",
      "iteration : 150, loss : 0.3554, accuracy : 90.90\n",
      "iteration : 200, loss : 0.4089, accuracy : 89.18\n",
      "iteration : 250, loss : 0.4290, accuracy : 88.70\n",
      "iteration : 300, loss : 0.4226, accuracy : 89.06\n",
      "Epoch : 211, training loss : 0.4242, training accuracy : 89.01, val loss : 0.3789, val accuracy : 90.05\n",
      "\n",
      "Epoch: 212\n",
      "iteration :  50, loss : 0.3087, accuracy : 92.48\n",
      "iteration : 100, loss : 0.3049, accuracy : 92.89\n",
      "iteration : 150, loss : 0.3619, accuracy : 91.17\n",
      "iteration : 200, loss : 0.3862, accuracy : 90.45\n",
      "iteration : 250, loss : 0.3871, accuracy : 90.51\n",
      "iteration : 300, loss : 0.3922, accuracy : 90.33\n",
      "Epoch : 212, training loss : 0.4059, training accuracy : 89.80, val loss : 0.3920, val accuracy : 90.20\n",
      "\n",
      "Epoch: 213\n",
      "iteration :  50, loss : 0.4020, accuracy : 90.20\n",
      "iteration : 100, loss : 0.4123, accuracy : 89.32\n",
      "iteration : 150, loss : 0.4036, accuracy : 89.68\n",
      "iteration : 200, loss : 0.4500, accuracy : 88.22\n",
      "iteration : 250, loss : 0.4554, accuracy : 87.98\n",
      "iteration : 300, loss : 0.4423, accuracy : 88.43\n",
      "Epoch : 213, training loss : 0.4519, training accuracy : 88.20, val loss : 0.4098, val accuracy : 89.66\n",
      "\n",
      "Epoch: 214\n",
      "iteration :  50, loss : 0.4326, accuracy : 88.78\n",
      "iteration : 100, loss : 0.4180, accuracy : 89.49\n",
      "iteration : 150, loss : 0.4067, accuracy : 90.01\n",
      "iteration : 200, loss : 0.4251, accuracy : 89.45\n",
      "iteration : 250, loss : 0.4499, accuracy : 88.59\n",
      "iteration : 300, loss : 0.4574, accuracy : 88.36\n",
      "Epoch : 214, training loss : 0.4541, training accuracy : 88.52, val loss : 0.3780, val accuracy : 89.83\n",
      "\n",
      "Epoch: 215\n",
      "iteration :  50, loss : 0.3753, accuracy : 90.63\n",
      "iteration : 100, loss : 0.4355, accuracy : 88.32\n",
      "iteration : 150, loss : 0.4663, accuracy : 87.53\n",
      "iteration : 200, loss : 0.4427, accuracy : 88.25\n",
      "iteration : 250, loss : 0.4325, accuracy : 88.44\n",
      "iteration : 300, loss : 0.4370, accuracy : 88.32\n",
      "Epoch : 215, training loss : 0.4378, training accuracy : 88.39, val loss : 0.3787, val accuracy : 90.35\n",
      "\n",
      "Epoch: 216\n",
      "iteration :  50, loss : 0.3536, accuracy : 91.28\n",
      "iteration : 100, loss : 0.3239, accuracy : 92.15\n",
      "iteration : 150, loss : 0.3810, accuracy : 90.48\n",
      "iteration : 200, loss : 0.3960, accuracy : 89.88\n",
      "iteration : 250, loss : 0.4040, accuracy : 89.60\n",
      "iteration : 300, loss : 0.4195, accuracy : 89.02\n",
      "Epoch : 216, training loss : 0.4282, training accuracy : 88.83, val loss : 0.3943, val accuracy : 90.02\n",
      "\n",
      "Epoch: 217\n",
      "iteration :  50, loss : 0.4151, accuracy : 89.28\n",
      "iteration : 100, loss : 0.4668, accuracy : 87.22\n",
      "iteration : 150, loss : 0.4150, accuracy : 88.89\n",
      "iteration : 200, loss : 0.4180, accuracy : 88.88\n",
      "iteration : 250, loss : 0.4092, accuracy : 89.03\n",
      "iteration : 300, loss : 0.4110, accuracy : 88.95\n",
      "Saving..\n",
      "Epoch : 217, training loss : 0.4159, training accuracy : 88.86, val loss : 0.3624, val accuracy : 90.47\n",
      "\n",
      "Epoch: 218\n",
      "iteration :  50, loss : 0.5368, accuracy : 84.67\n",
      "iteration : 100, loss : 0.5127, accuracy : 86.08\n",
      "iteration : 150, loss : 0.5180, accuracy : 85.92\n",
      "iteration : 200, loss : 0.4844, accuracy : 87.10\n",
      "iteration : 250, loss : 0.4423, accuracy : 88.35\n",
      "iteration : 300, loss : 0.4466, accuracy : 88.17\n",
      "Epoch : 218, training loss : 0.4508, training accuracy : 88.03, val loss : 0.3882, val accuracy : 90.27\n",
      "\n",
      "Epoch: 219\n",
      "iteration :  50, loss : 0.5133, accuracy : 85.85\n",
      "iteration : 100, loss : 0.4496, accuracy : 88.14\n",
      "iteration : 150, loss : 0.4314, accuracy : 88.51\n",
      "iteration : 200, loss : 0.4372, accuracy : 88.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 250, loss : 0.4308, accuracy : 88.52\n",
      "iteration : 300, loss : 0.3967, accuracy : 89.50\n",
      "Epoch : 219, training loss : 0.4014, training accuracy : 89.39, val loss : 0.3637, val accuracy : 90.32\n",
      "\n",
      "Epoch: 220\n",
      "iteration :  50, loss : 0.3474, accuracy : 91.19\n",
      "iteration : 100, loss : 0.3791, accuracy : 90.92\n",
      "iteration : 150, loss : 0.3977, accuracy : 90.00\n",
      "iteration : 200, loss : 0.3828, accuracy : 90.44\n",
      "iteration : 250, loss : 0.3753, accuracy : 90.66\n",
      "iteration : 300, loss : 0.3594, accuracy : 91.04\n",
      "Saving..\n",
      "Epoch : 220, training loss : 0.3689, training accuracy : 90.67, val loss : 0.3519, val accuracy : 90.64\n",
      "\n",
      "Epoch: 221\n",
      "iteration :  50, loss : 0.3344, accuracy : 91.28\n",
      "iteration : 100, loss : 0.3788, accuracy : 89.62\n",
      "iteration : 150, loss : 0.3764, accuracy : 90.12\n",
      "iteration : 200, loss : 0.3899, accuracy : 90.00\n",
      "iteration : 250, loss : 0.3898, accuracy : 89.97\n",
      "iteration : 300, loss : 0.3911, accuracy : 90.03\n",
      "Epoch : 221, training loss : 0.3950, training accuracy : 89.92, val loss : 0.3760, val accuracy : 90.21\n",
      "\n",
      "Epoch: 222\n",
      "iteration :  50, loss : 0.5405, accuracy : 86.11\n",
      "iteration : 100, loss : 0.4776, accuracy : 87.69\n",
      "iteration : 150, loss : 0.4386, accuracy : 88.61\n",
      "iteration : 200, loss : 0.4317, accuracy : 88.84\n",
      "iteration : 250, loss : 0.4216, accuracy : 88.96\n",
      "iteration : 300, loss : 0.4297, accuracy : 88.78\n",
      "Saving..\n",
      "Epoch : 222, training loss : 0.4298, training accuracy : 88.78, val loss : 0.3604, val accuracy : 90.69\n",
      "\n",
      "Epoch: 223\n",
      "iteration :  50, loss : 0.3302, accuracy : 91.58\n",
      "iteration : 100, loss : 0.4045, accuracy : 89.64\n",
      "iteration : 150, loss : 0.4225, accuracy : 88.62\n",
      "iteration : 200, loss : 0.4081, accuracy : 89.35\n",
      "iteration : 250, loss : 0.4175, accuracy : 89.06\n",
      "iteration : 300, loss : 0.4255, accuracy : 88.85\n",
      "Epoch : 223, training loss : 0.4273, training accuracy : 88.83, val loss : 0.3791, val accuracy : 90.29\n",
      "\n",
      "Epoch: 224\n",
      "iteration :  50, loss : 0.4441, accuracy : 87.90\n",
      "iteration : 100, loss : 0.4441, accuracy : 87.59\n",
      "iteration : 150, loss : 0.5020, accuracy : 85.91\n",
      "iteration : 200, loss : 0.4879, accuracy : 86.44\n",
      "iteration : 250, loss : 0.5178, accuracy : 85.61\n",
      "iteration : 300, loss : 0.5180, accuracy : 85.59\n",
      "Epoch : 224, training loss : 0.5154, training accuracy : 85.75, val loss : 0.3557, val accuracy : 90.68\n",
      "\n",
      "Epoch: 225\n",
      "iteration :  50, loss : 0.4465, accuracy : 88.98\n",
      "iteration : 100, loss : 0.4902, accuracy : 87.65\n",
      "iteration : 150, loss : 0.4436, accuracy : 88.96\n",
      "iteration : 200, loss : 0.4370, accuracy : 88.83\n",
      "iteration : 250, loss : 0.4213, accuracy : 89.25\n",
      "iteration : 300, loss : 0.4221, accuracy : 89.35\n",
      "Epoch : 225, training loss : 0.4220, training accuracy : 89.38, val loss : 0.3621, val accuracy : 90.42\n",
      "\n",
      "Epoch: 226\n",
      "iteration :  50, loss : 0.3820, accuracy : 89.76\n",
      "iteration : 100, loss : 0.3823, accuracy : 89.80\n",
      "iteration : 150, loss : 0.3985, accuracy : 89.52\n",
      "iteration : 200, loss : 0.3936, accuracy : 89.57\n",
      "iteration : 250, loss : 0.3700, accuracy : 90.44\n",
      "iteration : 300, loss : 0.3884, accuracy : 89.91\n",
      "Epoch : 226, training loss : 0.3950, training accuracy : 89.72, val loss : 0.3832, val accuracy : 90.17\n",
      "\n",
      "Epoch: 227\n",
      "iteration :  50, loss : 0.3424, accuracy : 91.95\n",
      "iteration : 100, loss : 0.3712, accuracy : 90.53\n",
      "iteration : 150, loss : 0.4025, accuracy : 89.70\n",
      "iteration : 200, loss : 0.4011, accuracy : 89.75\n",
      "iteration : 250, loss : 0.4013, accuracy : 89.73\n",
      "iteration : 300, loss : 0.3987, accuracy : 89.74\n",
      "Epoch : 227, training loss : 0.4002, training accuracy : 89.66, val loss : 0.3589, val accuracy : 90.59\n",
      "\n",
      "Epoch: 228\n",
      "iteration :  50, loss : 0.4567, accuracy : 88.39\n",
      "iteration : 100, loss : 0.4372, accuracy : 88.66\n",
      "iteration : 150, loss : 0.4144, accuracy : 89.18\n",
      "iteration : 200, loss : 0.4125, accuracy : 89.22\n",
      "iteration : 250, loss : 0.4229, accuracy : 88.87\n",
      "iteration : 300, loss : 0.4156, accuracy : 89.11\n",
      "Saving..\n",
      "Epoch : 228, training loss : 0.4245, training accuracy : 88.95, val loss : 0.3726, val accuracy : 90.71\n",
      "\n",
      "Epoch: 229\n",
      "iteration :  50, loss : 0.3468, accuracy : 90.96\n",
      "iteration : 100, loss : 0.4500, accuracy : 87.76\n",
      "iteration : 150, loss : 0.4055, accuracy : 89.20\n",
      "iteration : 200, loss : 0.4077, accuracy : 89.14\n",
      "iteration : 250, loss : 0.4016, accuracy : 89.18\n",
      "iteration : 300, loss : 0.4087, accuracy : 89.21\n",
      "Epoch : 229, training loss : 0.4069, training accuracy : 89.29, val loss : 0.3726, val accuracy : 90.28\n",
      "\n",
      "Epoch: 230\n",
      "iteration :  50, loss : 0.3454, accuracy : 91.71\n",
      "iteration : 100, loss : 0.3467, accuracy : 91.75\n",
      "iteration : 150, loss : 0.3407, accuracy : 91.88\n",
      "iteration : 200, loss : 0.3882, accuracy : 90.44\n",
      "iteration : 250, loss : 0.3832, accuracy : 90.70\n",
      "iteration : 300, loss : 0.3844, accuracy : 90.54\n",
      "Epoch : 230, training loss : 0.3925, training accuracy : 90.24, val loss : 0.3658, val accuracy : 90.54\n",
      "\n",
      "Epoch: 231\n",
      "iteration :  50, loss : 0.4478, accuracy : 87.45\n",
      "iteration : 100, loss : 0.4248, accuracy : 88.58\n",
      "iteration : 150, loss : 0.4255, accuracy : 88.75\n",
      "iteration : 200, loss : 0.4091, accuracy : 89.43\n",
      "iteration : 250, loss : 0.3922, accuracy : 89.91\n",
      "iteration : 300, loss : 0.4107, accuracy : 89.26\n",
      "Epoch : 231, training loss : 0.4193, training accuracy : 88.94, val loss : 0.3799, val accuracy : 90.71\n",
      "\n",
      "Epoch: 232\n",
      "iteration :  50, loss : 0.4698, accuracy : 88.17\n",
      "iteration : 100, loss : 0.4656, accuracy : 87.89\n",
      "iteration : 150, loss : 0.4700, accuracy : 87.65\n",
      "iteration : 200, loss : 0.4651, accuracy : 87.45\n",
      "iteration : 250, loss : 0.4424, accuracy : 88.03\n",
      "iteration : 300, loss : 0.4595, accuracy : 87.43\n",
      "Saving..\n",
      "Epoch : 232, training loss : 0.4530, training accuracy : 87.62, val loss : 0.3507, val accuracy : 90.82\n",
      "\n",
      "Epoch: 233\n",
      "iteration :  50, loss : 0.4813, accuracy : 86.04\n",
      "iteration : 100, loss : 0.4981, accuracy : 86.17\n",
      "iteration : 150, loss : 0.4396, accuracy : 88.13\n",
      "iteration : 200, loss : 0.4198, accuracy : 88.80\n",
      "iteration : 250, loss : 0.4364, accuracy : 88.31\n",
      "iteration : 300, loss : 0.4399, accuracy : 88.39\n",
      "Epoch : 233, training loss : 0.4413, training accuracy : 88.37, val loss : 0.3662, val accuracy : 90.53\n",
      "\n",
      "Epoch: 234\n",
      "iteration :  50, loss : 0.3012, accuracy : 92.54\n",
      "iteration : 100, loss : 0.3383, accuracy : 91.27\n",
      "iteration : 150, loss : 0.3390, accuracy : 91.39\n",
      "iteration : 200, loss : 0.3677, accuracy : 90.48\n",
      "iteration : 250, loss : 0.3619, accuracy : 90.68\n",
      "iteration : 300, loss : 0.3633, accuracy : 90.65\n",
      "Saving..\n",
      "Epoch : 234, training loss : 0.3568, training accuracy : 90.84, val loss : 0.3522, val accuracy : 91.07\n",
      "\n",
      "Epoch: 235\n",
      "iteration :  50, loss : 0.3950, accuracy : 90.03\n",
      "iteration : 100, loss : 0.4166, accuracy : 89.17\n",
      "iteration : 150, loss : 0.4466, accuracy : 87.83\n",
      "iteration : 200, loss : 0.4529, accuracy : 87.84\n",
      "iteration : 250, loss : 0.4351, accuracy : 88.52\n",
      "iteration : 300, loss : 0.4210, accuracy : 89.13\n",
      "Epoch : 235, training loss : 0.4248, training accuracy : 89.07, val loss : 0.3510, val accuracy : 91.00\n",
      "\n",
      "Epoch: 236\n",
      "iteration :  50, loss : 0.3051, accuracy : 92.32\n",
      "iteration : 100, loss : 0.3444, accuracy : 90.68\n",
      "iteration : 150, loss : 0.3864, accuracy : 89.49\n",
      "iteration : 200, loss : 0.3713, accuracy : 89.95\n",
      "iteration : 250, loss : 0.3682, accuracy : 90.26\n",
      "iteration : 300, loss : 0.3691, accuracy : 90.40\n",
      "Epoch : 236, training loss : 0.3830, training accuracy : 89.98, val loss : 0.3668, val accuracy : 90.92\n",
      "\n",
      "Epoch: 237\n",
      "iteration :  50, loss : 0.4272, accuracy : 89.33\n",
      "iteration : 100, loss : 0.3667, accuracy : 90.92\n",
      "iteration : 150, loss : 0.3539, accuracy : 91.32\n",
      "iteration : 200, loss : 0.3579, accuracy : 91.08\n",
      "iteration : 250, loss : 0.3691, accuracy : 90.85\n",
      "iteration : 300, loss : 0.3811, accuracy : 90.41\n",
      "Epoch : 237, training loss : 0.3856, training accuracy : 90.25, val loss : 0.3514, val accuracy : 90.85\n",
      "\n",
      "Epoch: 238\n",
      "iteration :  50, loss : 0.3920, accuracy : 89.19\n",
      "iteration : 100, loss : 0.4290, accuracy : 88.66\n",
      "iteration : 150, loss : 0.4088, accuracy : 89.27\n",
      "iteration : 200, loss : 0.3885, accuracy : 89.75\n",
      "iteration : 250, loss : 0.4116, accuracy : 89.07\n",
      "iteration : 300, loss : 0.4216, accuracy : 88.64\n",
      "Epoch : 238, training loss : 0.4329, training accuracy : 88.30, val loss : 0.3781, val accuracy : 90.66\n",
      "\n",
      "Epoch: 239\n",
      "iteration :  50, loss : 0.4808, accuracy : 87.75\n",
      "iteration : 100, loss : 0.4632, accuracy : 87.71\n",
      "iteration : 150, loss : 0.4437, accuracy : 88.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 200, loss : 0.4524, accuracy : 88.17\n",
      "iteration : 250, loss : 0.4403, accuracy : 88.60\n",
      "iteration : 300, loss : 0.4441, accuracy : 88.65\n",
      "Epoch : 239, training loss : 0.4387, training accuracy : 88.83, val loss : 0.3549, val accuracy : 90.77\n",
      "\n",
      "Epoch: 240\n",
      "iteration :  50, loss : 0.4054, accuracy : 89.50\n",
      "iteration : 100, loss : 0.4577, accuracy : 87.67\n",
      "iteration : 150, loss : 0.4098, accuracy : 89.34\n",
      "iteration : 200, loss : 0.4079, accuracy : 89.39\n",
      "iteration : 250, loss : 0.4028, accuracy : 89.66\n",
      "iteration : 300, loss : 0.4038, accuracy : 89.65\n",
      "Epoch : 240, training loss : 0.3988, training accuracy : 89.80, val loss : 0.3436, val accuracy : 90.94\n",
      "\n",
      "Epoch: 241\n",
      "iteration :  50, loss : 0.4259, accuracy : 88.15\n",
      "iteration : 100, loss : 0.4193, accuracy : 88.33\n",
      "iteration : 150, loss : 0.4094, accuracy : 89.08\n",
      "iteration : 200, loss : 0.4254, accuracy : 88.60\n",
      "iteration : 250, loss : 0.4175, accuracy : 88.78\n",
      "iteration : 300, loss : 0.4097, accuracy : 88.93\n",
      "Saving..\n",
      "Epoch : 241, training loss : 0.4005, training accuracy : 89.23, val loss : 0.3437, val accuracy : 91.24\n",
      "\n",
      "Epoch: 242\n",
      "iteration :  50, loss : 0.3296, accuracy : 92.98\n",
      "iteration : 100, loss : 0.3178, accuracy : 92.70\n",
      "iteration : 150, loss : 0.3794, accuracy : 90.84\n",
      "iteration : 200, loss : 0.3766, accuracy : 90.70\n",
      "iteration : 250, loss : 0.3664, accuracy : 90.97\n",
      "iteration : 300, loss : 0.3450, accuracy : 91.50\n",
      "Epoch : 242, training loss : 0.3554, training accuracy : 91.14, val loss : 0.3633, val accuracy : 90.61\n",
      "\n",
      "Epoch: 243\n",
      "iteration :  50, loss : 0.3718, accuracy : 90.88\n",
      "iteration : 100, loss : 0.3902, accuracy : 90.23\n",
      "iteration : 150, loss : 0.3864, accuracy : 90.23\n",
      "iteration : 200, loss : 0.4097, accuracy : 89.55\n",
      "iteration : 250, loss : 0.4161, accuracy : 89.48\n",
      "iteration : 300, loss : 0.4081, accuracy : 89.60\n",
      "Epoch : 243, training loss : 0.4118, training accuracy : 89.51, val loss : 0.3640, val accuracy : 90.73\n",
      "\n",
      "Epoch: 244\n",
      "iteration :  50, loss : 0.2919, accuracy : 93.14\n",
      "iteration : 100, loss : 0.3199, accuracy : 92.28\n",
      "iteration : 150, loss : 0.3598, accuracy : 91.28\n",
      "iteration : 200, loss : 0.3937, accuracy : 90.05\n",
      "iteration : 250, loss : 0.3963, accuracy : 89.93\n",
      "iteration : 300, loss : 0.3958, accuracy : 89.79\n",
      "Saving..\n",
      "Epoch : 244, training loss : 0.3925, training accuracy : 89.90, val loss : 0.3382, val accuracy : 91.34\n",
      "\n",
      "Epoch: 245\n",
      "iteration :  50, loss : 0.3895, accuracy : 90.12\n",
      "iteration : 100, loss : 0.3484, accuracy : 91.11\n",
      "iteration : 150, loss : 0.3764, accuracy : 90.37\n",
      "iteration : 200, loss : 0.3837, accuracy : 90.07\n",
      "iteration : 250, loss : 0.3995, accuracy : 89.52\n",
      "iteration : 300, loss : 0.4220, accuracy : 88.99\n",
      "Epoch : 245, training loss : 0.4197, training accuracy : 89.08, val loss : 0.3496, val accuracy : 91.31\n",
      "\n",
      "Epoch: 246\n",
      "iteration :  50, loss : 0.4027, accuracy : 89.84\n",
      "iteration : 100, loss : 0.4542, accuracy : 87.65\n",
      "iteration : 150, loss : 0.4586, accuracy : 87.79\n",
      "iteration : 200, loss : 0.4354, accuracy : 88.44\n",
      "iteration : 250, loss : 0.4597, accuracy : 88.00\n",
      "iteration : 300, loss : 0.4539, accuracy : 88.12\n",
      "Epoch : 246, training loss : 0.4532, training accuracy : 88.09, val loss : 0.3530, val accuracy : 90.94\n",
      "\n",
      "Epoch: 247\n",
      "iteration :  50, loss : 0.4474, accuracy : 88.03\n",
      "iteration : 100, loss : 0.4275, accuracy : 88.50\n",
      "iteration : 150, loss : 0.4408, accuracy : 88.18\n",
      "iteration : 200, loss : 0.4361, accuracy : 88.44\n",
      "iteration : 250, loss : 0.4302, accuracy : 88.68\n",
      "iteration : 300, loss : 0.4162, accuracy : 89.16\n",
      "Epoch : 247, training loss : 0.4132, training accuracy : 89.23, val loss : 0.3441, val accuracy : 91.18\n",
      "\n",
      "Epoch: 248\n",
      "iteration :  50, loss : 0.4181, accuracy : 89.01\n",
      "iteration : 100, loss : 0.4476, accuracy : 88.46\n",
      "iteration : 150, loss : 0.4260, accuracy : 89.40\n",
      "iteration : 200, loss : 0.4221, accuracy : 89.48\n",
      "iteration : 250, loss : 0.4302, accuracy : 89.16\n",
      "iteration : 300, loss : 0.4270, accuracy : 89.16\n",
      "Epoch : 248, training loss : 0.4245, training accuracy : 89.26, val loss : 0.3461, val accuracy : 91.15\n",
      "\n",
      "Epoch: 249\n",
      "iteration :  50, loss : 0.4854, accuracy : 85.87\n",
      "iteration : 100, loss : 0.3936, accuracy : 89.19\n",
      "iteration : 150, loss : 0.3614, accuracy : 90.56\n",
      "iteration : 200, loss : 0.3634, accuracy : 90.62\n",
      "iteration : 250, loss : 0.3725, accuracy : 90.41\n",
      "iteration : 300, loss : 0.3777, accuracy : 90.35\n",
      "Epoch : 249, training loss : 0.3715, training accuracy : 90.59, val loss : 0.3397, val accuracy : 91.21\n",
      "\n",
      "Epoch: 250\n",
      "iteration :  50, loss : 0.4042, accuracy : 89.77\n",
      "iteration : 100, loss : 0.4582, accuracy : 87.62\n",
      "iteration : 150, loss : 0.4325, accuracy : 88.57\n",
      "iteration : 200, loss : 0.4242, accuracy : 88.75\n",
      "iteration : 250, loss : 0.4404, accuracy : 88.48\n",
      "iteration : 300, loss : 0.4262, accuracy : 88.89\n",
      "Epoch : 250, training loss : 0.4189, training accuracy : 89.17, val loss : 0.3448, val accuracy : 91.13\n",
      "\n",
      "Epoch: 251\n",
      "iteration :  50, loss : 0.4349, accuracy : 89.06\n",
      "iteration : 100, loss : 0.3943, accuracy : 90.07\n",
      "iteration : 150, loss : 0.3599, accuracy : 91.19\n",
      "iteration : 200, loss : 0.3711, accuracy : 90.86\n",
      "iteration : 250, loss : 0.3740, accuracy : 90.57\n",
      "iteration : 300, loss : 0.3661, accuracy : 90.67\n",
      "Epoch : 251, training loss : 0.3709, training accuracy : 90.50, val loss : 0.3472, val accuracy : 91.04\n",
      "\n",
      "Epoch: 252\n",
      "iteration :  50, loss : 0.4561, accuracy : 88.65\n",
      "iteration : 100, loss : 0.3796, accuracy : 90.63\n",
      "iteration : 150, loss : 0.3395, accuracy : 91.65\n",
      "iteration : 200, loss : 0.3390, accuracy : 91.58\n",
      "iteration : 250, loss : 0.3397, accuracy : 91.67\n",
      "iteration : 300, loss : 0.3491, accuracy : 91.43\n",
      "Epoch : 252, training loss : 0.3643, training accuracy : 91.01, val loss : 0.3731, val accuracy : 91.12\n",
      "\n",
      "Epoch: 253\n",
      "iteration :  50, loss : 0.4170, accuracy : 89.76\n",
      "iteration : 100, loss : 0.3908, accuracy : 90.36\n",
      "iteration : 150, loss : 0.3865, accuracy : 90.44\n",
      "iteration : 200, loss : 0.4130, accuracy : 89.61\n",
      "iteration : 250, loss : 0.4011, accuracy : 89.98\n",
      "iteration : 300, loss : 0.4234, accuracy : 89.15\n",
      "Epoch : 253, training loss : 0.4210, training accuracy : 89.25, val loss : 0.3498, val accuracy : 91.28\n",
      "\n",
      "Epoch: 254\n",
      "iteration :  50, loss : 0.2897, accuracy : 92.49\n",
      "iteration : 100, loss : 0.3703, accuracy : 89.99\n",
      "iteration : 150, loss : 0.3854, accuracy : 89.52\n",
      "iteration : 200, loss : 0.3935, accuracy : 89.43\n",
      "iteration : 250, loss : 0.3936, accuracy : 89.65\n",
      "iteration : 300, loss : 0.4138, accuracy : 88.87\n",
      "Saving..\n",
      "Epoch : 254, training loss : 0.4088, training accuracy : 89.04, val loss : 0.3384, val accuracy : 91.42\n",
      "\n",
      "Epoch: 255\n",
      "iteration :  50, loss : 0.3929, accuracy : 90.55\n",
      "iteration : 100, loss : 0.4454, accuracy : 89.06\n",
      "iteration : 150, loss : 0.4178, accuracy : 89.55\n",
      "iteration : 200, loss : 0.4114, accuracy : 89.57\n",
      "iteration : 250, loss : 0.3894, accuracy : 90.35\n",
      "iteration : 300, loss : 0.4035, accuracy : 89.87\n",
      "Epoch : 255, training loss : 0.4086, training accuracy : 89.67, val loss : 0.3601, val accuracy : 91.10\n",
      "\n",
      "Epoch: 256\n",
      "iteration :  50, loss : 0.3745, accuracy : 89.68\n",
      "iteration : 100, loss : 0.4462, accuracy : 87.63\n",
      "iteration : 150, loss : 0.4409, accuracy : 88.15\n",
      "iteration : 200, loss : 0.4434, accuracy : 88.23\n",
      "iteration : 250, loss : 0.4370, accuracy : 88.51\n",
      "iteration : 300, loss : 0.4510, accuracy : 88.22\n",
      "Epoch : 256, training loss : 0.4507, training accuracy : 88.29, val loss : 0.3606, val accuracy : 90.87\n",
      "\n",
      "Epoch: 257\n",
      "iteration :  50, loss : 0.3257, accuracy : 91.83\n",
      "iteration : 100, loss : 0.3398, accuracy : 91.01\n",
      "iteration : 150, loss : 0.3807, accuracy : 89.87\n",
      "iteration : 200, loss : 0.3830, accuracy : 89.97\n",
      "iteration : 250, loss : 0.3860, accuracy : 89.86\n",
      "iteration : 300, loss : 0.3984, accuracy : 89.55\n",
      "Epoch : 257, training loss : 0.3973, training accuracy : 89.63, val loss : 0.3573, val accuracy : 90.92\n",
      "\n",
      "Epoch: 258\n",
      "iteration :  50, loss : 0.3814, accuracy : 90.84\n",
      "iteration : 100, loss : 0.4400, accuracy : 88.70\n",
      "iteration : 150, loss : 0.4423, accuracy : 88.22\n",
      "iteration : 200, loss : 0.4343, accuracy : 88.33\n",
      "iteration : 250, loss : 0.4334, accuracy : 88.51\n",
      "iteration : 300, loss : 0.4095, accuracy : 89.26\n",
      "Epoch : 258, training loss : 0.4144, training accuracy : 89.16, val loss : 0.3511, val accuracy : 91.12\n",
      "\n",
      "Epoch: 259\n",
      "iteration :  50, loss : 0.4866, accuracy : 86.64\n",
      "iteration : 100, loss : 0.4324, accuracy : 88.56\n",
      "iteration : 150, loss : 0.4209, accuracy : 88.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 200, loss : 0.4252, accuracy : 88.49\n",
      "iteration : 250, loss : 0.4290, accuracy : 88.44\n",
      "iteration : 300, loss : 0.4185, accuracy : 88.87\n",
      "Epoch : 259, training loss : 0.4119, training accuracy : 89.11, val loss : 0.3406, val accuracy : 91.31\n",
      "\n",
      "Epoch: 260\n",
      "iteration :  50, loss : 0.3788, accuracy : 90.61\n",
      "iteration : 100, loss : 0.3690, accuracy : 90.82\n",
      "iteration : 150, loss : 0.3258, accuracy : 91.84\n",
      "iteration : 200, loss : 0.3515, accuracy : 90.89\n",
      "iteration : 250, loss : 0.3671, accuracy : 90.43\n",
      "iteration : 300, loss : 0.3540, accuracy : 90.93\n",
      "Epoch : 260, training loss : 0.3608, training accuracy : 90.82, val loss : 0.3607, val accuracy : 90.94\n",
      "\n",
      "Epoch: 261\n",
      "iteration :  50, loss : 0.3849, accuracy : 90.38\n",
      "iteration : 100, loss : 0.4005, accuracy : 89.36\n",
      "iteration : 150, loss : 0.4256, accuracy : 88.71\n",
      "iteration : 200, loss : 0.4467, accuracy : 88.03\n",
      "iteration : 250, loss : 0.4326, accuracy : 88.67\n",
      "iteration : 300, loss : 0.4329, accuracy : 88.56\n",
      "Saving..\n",
      "Epoch : 261, training loss : 0.4299, training accuracy : 88.66, val loss : 0.3372, val accuracy : 91.71\n",
      "\n",
      "Epoch: 262\n",
      "iteration :  50, loss : 0.4457, accuracy : 88.72\n",
      "iteration : 100, loss : 0.3831, accuracy : 90.27\n",
      "iteration : 150, loss : 0.3796, accuracy : 90.16\n",
      "iteration : 200, loss : 0.3940, accuracy : 89.44\n",
      "iteration : 250, loss : 0.3910, accuracy : 89.62\n",
      "iteration : 300, loss : 0.3984, accuracy : 89.46\n",
      "Epoch : 262, training loss : 0.4041, training accuracy : 89.30, val loss : 0.3493, val accuracy : 91.02\n",
      "\n",
      "Epoch: 263\n",
      "iteration :  50, loss : 0.3893, accuracy : 89.76\n",
      "iteration : 100, loss : 0.3758, accuracy : 89.82\n",
      "iteration : 150, loss : 0.3770, accuracy : 89.79\n",
      "iteration : 200, loss : 0.3666, accuracy : 90.37\n",
      "iteration : 250, loss : 0.3691, accuracy : 90.55\n",
      "iteration : 300, loss : 0.3773, accuracy : 90.17\n",
      "Epoch : 263, training loss : 0.3765, training accuracy : 90.21, val loss : 0.3422, val accuracy : 91.21\n",
      "\n",
      "Epoch: 264\n",
      "iteration :  50, loss : 0.4058, accuracy : 90.11\n",
      "iteration : 100, loss : 0.3652, accuracy : 91.09\n",
      "iteration : 150, loss : 0.3641, accuracy : 90.74\n",
      "iteration : 200, loss : 0.3741, accuracy : 90.71\n",
      "iteration : 250, loss : 0.3932, accuracy : 90.28\n",
      "iteration : 300, loss : 0.3901, accuracy : 90.42\n",
      "Epoch : 264, training loss : 0.3934, training accuracy : 90.31, val loss : 0.3547, val accuracy : 91.03\n",
      "\n",
      "Epoch: 265\n",
      "iteration :  50, loss : 0.4647, accuracy : 87.48\n",
      "iteration : 100, loss : 0.4083, accuracy : 89.61\n",
      "iteration : 150, loss : 0.3935, accuracy : 89.89\n",
      "iteration : 200, loss : 0.3877, accuracy : 90.13\n",
      "iteration : 250, loss : 0.3949, accuracy : 90.00\n",
      "iteration : 300, loss : 0.3971, accuracy : 89.87\n",
      "Epoch : 265, training loss : 0.3961, training accuracy : 89.95, val loss : 0.3354, val accuracy : 91.37\n",
      "\n",
      "Epoch: 266\n",
      "iteration :  50, loss : 0.4116, accuracy : 89.15\n",
      "iteration : 100, loss : 0.3734, accuracy : 90.07\n",
      "iteration : 150, loss : 0.3829, accuracy : 89.89\n",
      "iteration : 200, loss : 0.4135, accuracy : 88.96\n",
      "iteration : 250, loss : 0.4260, accuracy : 88.63\n",
      "iteration : 300, loss : 0.4068, accuracy : 89.36\n",
      "Epoch : 266, training loss : 0.4045, training accuracy : 89.51, val loss : 0.3516, val accuracy : 90.90\n",
      "\n",
      "Epoch: 267\n",
      "iteration :  50, loss : 0.5049, accuracy : 85.47\n",
      "iteration : 100, loss : 0.4786, accuracy : 86.46\n",
      "iteration : 150, loss : 0.4065, accuracy : 88.79\n",
      "iteration : 200, loss : 0.4117, accuracy : 88.93\n",
      "iteration : 250, loss : 0.4079, accuracy : 89.14\n",
      "iteration : 300, loss : 0.4044, accuracy : 89.39\n",
      "Epoch : 267, training loss : 0.3963, training accuracy : 89.63, val loss : 0.3474, val accuracy : 91.04\n",
      "\n",
      "Epoch: 268\n",
      "iteration :  50, loss : 0.3183, accuracy : 92.14\n",
      "iteration : 100, loss : 0.3059, accuracy : 92.70\n",
      "iteration : 150, loss : 0.3340, accuracy : 91.78\n",
      "iteration : 200, loss : 0.3616, accuracy : 91.07\n",
      "iteration : 250, loss : 0.3755, accuracy : 90.52\n",
      "iteration : 300, loss : 0.3869, accuracy : 90.19\n",
      "Epoch : 268, training loss : 0.3910, training accuracy : 90.02, val loss : 0.3392, val accuracy : 91.33\n",
      "\n",
      "Epoch: 269\n",
      "iteration :  50, loss : 0.4345, accuracy : 88.20\n",
      "iteration : 100, loss : 0.3850, accuracy : 90.18\n",
      "iteration : 150, loss : 0.3629, accuracy : 91.04\n",
      "iteration : 200, loss : 0.3812, accuracy : 90.44\n",
      "iteration : 250, loss : 0.3651, accuracy : 90.90\n",
      "iteration : 300, loss : 0.3664, accuracy : 90.85\n",
      "Epoch : 269, training loss : 0.3618, training accuracy : 91.01, val loss : 0.3371, val accuracy : 91.38\n",
      "\n",
      "Epoch: 270\n",
      "iteration :  50, loss : 0.3217, accuracy : 91.60\n",
      "iteration : 100, loss : 0.3461, accuracy : 91.10\n",
      "iteration : 150, loss : 0.3203, accuracy : 92.13\n",
      "iteration : 200, loss : 0.3439, accuracy : 91.46\n",
      "iteration : 250, loss : 0.3785, accuracy : 90.24\n",
      "iteration : 300, loss : 0.3957, accuracy : 89.78\n",
      "Epoch : 270, training loss : 0.3955, training accuracy : 89.74, val loss : 0.3389, val accuracy : 91.29\n",
      "\n",
      "Epoch: 271\n",
      "iteration :  50, loss : 0.4268, accuracy : 88.46\n",
      "iteration : 100, loss : 0.3700, accuracy : 90.39\n",
      "iteration : 150, loss : 0.4000, accuracy : 89.54\n",
      "iteration : 200, loss : 0.4308, accuracy : 88.55\n",
      "iteration : 250, loss : 0.3903, accuracy : 89.85\n",
      "iteration : 300, loss : 0.3951, accuracy : 89.47\n",
      "Epoch : 271, training loss : 0.3981, training accuracy : 89.44, val loss : 0.3391, val accuracy : 91.40\n",
      "\n",
      "Epoch: 272\n",
      "iteration :  50, loss : 0.3538, accuracy : 90.70\n",
      "iteration : 100, loss : 0.3786, accuracy : 90.37\n",
      "iteration : 150, loss : 0.3686, accuracy : 90.65\n",
      "iteration : 200, loss : 0.3890, accuracy : 90.13\n",
      "iteration : 250, loss : 0.3838, accuracy : 90.17\n",
      "iteration : 300, loss : 0.3830, accuracy : 90.16\n",
      "Epoch : 272, training loss : 0.3787, training accuracy : 90.27, val loss : 0.3370, val accuracy : 91.24\n",
      "\n",
      "Epoch: 273\n",
      "iteration :  50, loss : 0.4333, accuracy : 88.88\n",
      "iteration : 100, loss : 0.4846, accuracy : 86.70\n",
      "iteration : 150, loss : 0.4453, accuracy : 87.83\n",
      "iteration : 200, loss : 0.4211, accuracy : 88.72\n",
      "iteration : 250, loss : 0.4036, accuracy : 89.32\n",
      "iteration : 300, loss : 0.3918, accuracy : 89.62\n",
      "Epoch : 273, training loss : 0.4027, training accuracy : 89.42, val loss : 0.3592, val accuracy : 91.21\n",
      "\n",
      "Epoch: 274\n",
      "iteration :  50, loss : 0.4018, accuracy : 89.07\n",
      "iteration : 100, loss : 0.3998, accuracy : 89.36\n",
      "iteration : 150, loss : 0.4489, accuracy : 88.08\n",
      "iteration : 200, loss : 0.4626, accuracy : 87.70\n",
      "iteration : 250, loss : 0.4579, accuracy : 88.07\n",
      "iteration : 300, loss : 0.4537, accuracy : 88.19\n",
      "Epoch : 274, training loss : 0.4495, training accuracy : 88.26, val loss : 0.3288, val accuracy : 91.59\n",
      "\n",
      "Epoch: 275\n",
      "iteration :  50, loss : 0.4775, accuracy : 87.64\n",
      "iteration : 100, loss : 0.4533, accuracy : 88.09\n",
      "iteration : 150, loss : 0.4202, accuracy : 89.14\n",
      "iteration : 200, loss : 0.3955, accuracy : 89.84\n",
      "iteration : 250, loss : 0.4075, accuracy : 89.51\n",
      "iteration : 300, loss : 0.3858, accuracy : 90.05\n",
      "Epoch : 275, training loss : 0.3758, training accuracy : 90.33, val loss : 0.3447, val accuracy : 91.69\n",
      "\n",
      "Epoch: 276\n",
      "iteration :  50, loss : 0.4698, accuracy : 86.43\n",
      "iteration : 100, loss : 0.4379, accuracy : 88.11\n",
      "iteration : 150, loss : 0.4428, accuracy : 87.98\n",
      "iteration : 200, loss : 0.4353, accuracy : 88.33\n",
      "iteration : 250, loss : 0.4447, accuracy : 88.08\n",
      "iteration : 300, loss : 0.4080, accuracy : 89.23\n",
      "Epoch : 276, training loss : 0.4023, training accuracy : 89.39, val loss : 0.3442, val accuracy : 91.57\n",
      "\n",
      "Epoch: 277\n",
      "iteration :  50, loss : 0.4156, accuracy : 88.94\n",
      "iteration : 100, loss : 0.4284, accuracy : 88.64\n",
      "iteration : 150, loss : 0.3979, accuracy : 89.60\n",
      "iteration : 200, loss : 0.3905, accuracy : 89.76\n",
      "iteration : 250, loss : 0.3958, accuracy : 89.85\n",
      "iteration : 300, loss : 0.3888, accuracy : 90.03\n",
      "Epoch : 277, training loss : 0.3835, training accuracy : 90.14, val loss : 0.3407, val accuracy : 91.38\n",
      "\n",
      "Epoch: 278\n",
      "iteration :  50, loss : 0.1918, accuracy : 95.81\n",
      "iteration : 100, loss : 0.3186, accuracy : 92.17\n",
      "iteration : 150, loss : 0.3232, accuracy : 92.05\n",
      "iteration : 200, loss : 0.3276, accuracy : 91.74\n",
      "iteration : 250, loss : 0.3474, accuracy : 91.21\n",
      "iteration : 300, loss : 0.3831, accuracy : 90.15\n",
      "Epoch : 278, training loss : 0.3775, training accuracy : 90.27, val loss : 0.3472, val accuracy : 91.28\n",
      "\n",
      "Epoch: 279\n",
      "iteration :  50, loss : 0.3556, accuracy : 91.63\n",
      "iteration : 100, loss : 0.3385, accuracy : 91.71\n",
      "iteration : 150, loss : 0.3534, accuracy : 91.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 200, loss : 0.3588, accuracy : 91.20\n",
      "iteration : 250, loss : 0.3719, accuracy : 90.85\n",
      "iteration : 300, loss : 0.3874, accuracy : 90.29\n",
      "Epoch : 279, training loss : 0.3916, training accuracy : 90.17, val loss : 0.3438, val accuracy : 91.32\n",
      "\n",
      "Epoch: 280\n",
      "iteration :  50, loss : 0.3355, accuracy : 92.10\n",
      "iteration : 100, loss : 0.3094, accuracy : 92.93\n",
      "iteration : 150, loss : 0.3284, accuracy : 92.18\n",
      "iteration : 200, loss : 0.3372, accuracy : 91.90\n",
      "iteration : 250, loss : 0.3397, accuracy : 91.69\n",
      "iteration : 300, loss : 0.3335, accuracy : 91.98\n",
      "Epoch : 280, training loss : 0.3481, training accuracy : 91.61, val loss : 0.3571, val accuracy : 91.19\n",
      "\n",
      "Epoch: 281\n",
      "iteration :  50, loss : 0.4316, accuracy : 89.21\n",
      "iteration : 100, loss : 0.4136, accuracy : 89.49\n",
      "iteration : 150, loss : 0.4112, accuracy : 89.61\n",
      "iteration : 200, loss : 0.4003, accuracy : 90.05\n",
      "iteration : 250, loss : 0.4143, accuracy : 89.64\n",
      "iteration : 300, loss : 0.3989, accuracy : 90.01\n",
      "Epoch : 281, training loss : 0.3973, training accuracy : 90.09, val loss : 0.3449, val accuracy : 91.33\n",
      "\n",
      "Epoch: 282\n",
      "iteration :  50, loss : 0.3886, accuracy : 90.14\n",
      "iteration : 100, loss : 0.3973, accuracy : 89.64\n",
      "iteration : 150, loss : 0.4386, accuracy : 88.09\n",
      "iteration : 200, loss : 0.4256, accuracy : 88.50\n",
      "iteration : 250, loss : 0.4196, accuracy : 88.73\n",
      "iteration : 300, loss : 0.4100, accuracy : 89.20\n",
      "Epoch : 282, training loss : 0.4163, training accuracy : 88.97, val loss : 0.3558, val accuracy : 91.33\n",
      "\n",
      "Epoch: 283\n",
      "iteration :  50, loss : 0.3907, accuracy : 90.07\n",
      "iteration : 100, loss : 0.3582, accuracy : 91.11\n",
      "iteration : 150, loss : 0.3404, accuracy : 91.47\n",
      "iteration : 200, loss : 0.3782, accuracy : 90.13\n",
      "iteration : 250, loss : 0.4076, accuracy : 89.18\n",
      "iteration : 300, loss : 0.4093, accuracy : 89.17\n",
      "Epoch : 283, training loss : 0.4082, training accuracy : 89.16, val loss : 0.3465, val accuracy : 91.29\n",
      "\n",
      "Epoch: 284\n",
      "iteration :  50, loss : 0.4797, accuracy : 87.61\n",
      "iteration : 100, loss : 0.4547, accuracy : 88.00\n",
      "iteration : 150, loss : 0.4434, accuracy : 88.05\n",
      "iteration : 200, loss : 0.4633, accuracy : 87.61\n",
      "iteration : 250, loss : 0.4606, accuracy : 87.80\n",
      "iteration : 300, loss : 0.4492, accuracy : 87.95\n",
      "Epoch : 284, training loss : 0.4588, training accuracy : 87.75, val loss : 0.3502, val accuracy : 91.15\n",
      "\n",
      "Epoch: 285\n",
      "iteration :  50, loss : 0.4179, accuracy : 88.87\n",
      "iteration : 100, loss : 0.3542, accuracy : 90.66\n",
      "iteration : 150, loss : 0.3485, accuracy : 91.14\n",
      "iteration : 200, loss : 0.3661, accuracy : 90.61\n",
      "iteration : 250, loss : 0.3567, accuracy : 90.88\n",
      "iteration : 300, loss : 0.3746, accuracy : 90.38\n",
      "Epoch : 285, training loss : 0.3695, training accuracy : 90.53, val loss : 0.3434, val accuracy : 91.47\n",
      "\n",
      "Epoch: 286\n",
      "iteration :  50, loss : 0.4474, accuracy : 89.34\n",
      "iteration : 100, loss : 0.4710, accuracy : 88.24\n",
      "iteration : 150, loss : 0.4461, accuracy : 88.95\n",
      "iteration : 200, loss : 0.4337, accuracy : 89.03\n",
      "iteration : 250, loss : 0.4146, accuracy : 89.56\n",
      "iteration : 300, loss : 0.4158, accuracy : 89.39\n",
      "Epoch : 286, training loss : 0.4128, training accuracy : 89.51, val loss : 0.3480, val accuracy : 91.07\n",
      "\n",
      "Epoch: 287\n",
      "iteration :  50, loss : 0.3859, accuracy : 89.80\n",
      "iteration : 100, loss : 0.3435, accuracy : 91.09\n",
      "iteration : 150, loss : 0.4143, accuracy : 89.16\n",
      "iteration : 200, loss : 0.4408, accuracy : 88.56\n",
      "iteration : 250, loss : 0.4363, accuracy : 88.75\n",
      "iteration : 300, loss : 0.4426, accuracy : 88.49\n",
      "Epoch : 287, training loss : 0.4384, training accuracy : 88.57, val loss : 0.3299, val accuracy : 91.45\n",
      "\n",
      "Epoch: 288\n",
      "iteration :  50, loss : 0.4015, accuracy : 89.76\n",
      "iteration : 100, loss : 0.4030, accuracy : 89.49\n",
      "iteration : 150, loss : 0.3680, accuracy : 90.66\n",
      "iteration : 200, loss : 0.3717, accuracy : 90.58\n",
      "iteration : 250, loss : 0.3729, accuracy : 90.67\n",
      "iteration : 300, loss : 0.3596, accuracy : 91.11\n",
      "Epoch : 288, training loss : 0.3658, training accuracy : 90.94, val loss : 0.3469, val accuracy : 91.12\n",
      "\n",
      "Epoch: 289\n",
      "iteration :  50, loss : 0.3819, accuracy : 90.39\n",
      "iteration : 100, loss : 0.4193, accuracy : 88.95\n",
      "iteration : 150, loss : 0.3939, accuracy : 90.00\n",
      "iteration : 200, loss : 0.4264, accuracy : 88.71\n",
      "iteration : 250, loss : 0.4394, accuracy : 88.38\n",
      "iteration : 300, loss : 0.4427, accuracy : 88.05\n",
      "Epoch : 289, training loss : 0.4403, training accuracy : 88.13, val loss : 0.3430, val accuracy : 91.20\n",
      "\n",
      "Epoch: 290\n",
      "iteration :  50, loss : 0.4915, accuracy : 87.31\n",
      "iteration : 100, loss : 0.4631, accuracy : 87.73\n",
      "iteration : 150, loss : 0.4234, accuracy : 88.71\n",
      "iteration : 200, loss : 0.4135, accuracy : 89.28\n",
      "iteration : 250, loss : 0.4214, accuracy : 89.02\n",
      "iteration : 300, loss : 0.4094, accuracy : 89.40\n",
      "Epoch : 290, training loss : 0.4085, training accuracy : 89.48, val loss : 0.3364, val accuracy : 91.18\n",
      "\n",
      "Epoch: 291\n",
      "iteration :  50, loss : 0.3841, accuracy : 90.36\n",
      "iteration : 100, loss : 0.3672, accuracy : 90.98\n",
      "iteration : 150, loss : 0.3762, accuracy : 90.30\n",
      "iteration : 200, loss : 0.3576, accuracy : 90.88\n",
      "iteration : 250, loss : 0.3668, accuracy : 90.58\n",
      "iteration : 300, loss : 0.3698, accuracy : 90.47\n",
      "Epoch : 291, training loss : 0.3731, training accuracy : 90.36, val loss : 0.3401, val accuracy : 91.31\n",
      "\n",
      "Epoch: 292\n",
      "iteration :  50, loss : 0.5409, accuracy : 84.70\n",
      "iteration : 100, loss : 0.4609, accuracy : 87.49\n",
      "iteration : 150, loss : 0.4118, accuracy : 88.98\n",
      "iteration : 200, loss : 0.4052, accuracy : 89.57\n",
      "iteration : 250, loss : 0.3980, accuracy : 89.50\n",
      "iteration : 300, loss : 0.3776, accuracy : 90.06\n",
      "Epoch : 292, training loss : 0.3787, training accuracy : 90.08, val loss : 0.3383, val accuracy : 91.56\n",
      "\n",
      "Epoch: 293\n",
      "iteration :  50, loss : 0.4870, accuracy : 86.53\n",
      "iteration : 100, loss : 0.4592, accuracy : 87.84\n",
      "iteration : 150, loss : 0.4382, accuracy : 88.50\n",
      "iteration : 200, loss : 0.4237, accuracy : 88.99\n",
      "iteration : 250, loss : 0.4012, accuracy : 89.62\n",
      "iteration : 300, loss : 0.4106, accuracy : 89.35\n",
      "Epoch : 293, training loss : 0.4222, training accuracy : 88.98, val loss : 0.3521, val accuracy : 91.16\n",
      "\n",
      "Epoch: 294\n",
      "iteration :  50, loss : 0.3613, accuracy : 91.60\n",
      "iteration : 100, loss : 0.2754, accuracy : 93.43\n",
      "iteration : 150, loss : 0.3383, accuracy : 91.54\n",
      "iteration : 200, loss : 0.3544, accuracy : 91.27\n",
      "iteration : 250, loss : 0.3685, accuracy : 90.96\n",
      "iteration : 300, loss : 0.3765, accuracy : 90.63\n",
      "Epoch : 294, training loss : 0.3862, training accuracy : 90.29, val loss : 0.3497, val accuracy : 91.58\n",
      "\n",
      "Epoch: 295\n",
      "iteration :  50, loss : 0.5044, accuracy : 85.37\n",
      "iteration : 100, loss : 0.4747, accuracy : 86.57\n",
      "iteration : 150, loss : 0.4256, accuracy : 88.12\n",
      "iteration : 200, loss : 0.4233, accuracy : 88.47\n",
      "iteration : 250, loss : 0.4239, accuracy : 88.49\n",
      "iteration : 300, loss : 0.4259, accuracy : 88.45\n",
      "Epoch : 295, training loss : 0.4391, training accuracy : 88.11, val loss : 0.3750, val accuracy : 91.18\n",
      "\n",
      "Epoch: 296\n",
      "iteration :  50, loss : 0.3249, accuracy : 91.79\n",
      "iteration : 100, loss : 0.3720, accuracy : 90.71\n",
      "iteration : 150, loss : 0.3676, accuracy : 90.98\n",
      "iteration : 200, loss : 0.3825, accuracy : 90.37\n",
      "iteration : 250, loss : 0.3743, accuracy : 90.56\n",
      "iteration : 300, loss : 0.3809, accuracy : 90.29\n",
      "Epoch : 296, training loss : 0.3825, training accuracy : 90.16, val loss : 0.3396, val accuracy : 91.30\n",
      "\n",
      "Epoch: 297\n",
      "iteration :  50, loss : 0.2244, accuracy : 94.20\n",
      "iteration : 100, loss : 0.3371, accuracy : 91.31\n",
      "iteration : 150, loss : 0.3915, accuracy : 89.69\n",
      "iteration : 200, loss : 0.3951, accuracy : 89.64\n",
      "iteration : 250, loss : 0.4081, accuracy : 89.31\n",
      "iteration : 300, loss : 0.4090, accuracy : 89.37\n",
      "Epoch : 297, training loss : 0.4090, training accuracy : 89.34, val loss : 0.3444, val accuracy : 91.23\n",
      "\n",
      "Epoch: 298\n",
      "iteration :  50, loss : 0.2721, accuracy : 93.77\n",
      "iteration : 100, loss : 0.3987, accuracy : 90.06\n",
      "iteration : 150, loss : 0.4056, accuracy : 89.61\n",
      "iteration : 200, loss : 0.4128, accuracy : 89.41\n",
      "iteration : 250, loss : 0.4066, accuracy : 89.43\n",
      "iteration : 300, loss : 0.3883, accuracy : 90.00\n",
      "Epoch : 298, training loss : 0.3891, training accuracy : 89.96, val loss : 0.3354, val accuracy : 91.42\n",
      "\n",
      "Epoch: 299\n",
      "iteration :  50, loss : 0.3507, accuracy : 90.90\n",
      "iteration : 100, loss : 0.3602, accuracy : 90.63\n",
      "iteration : 150, loss : 0.3662, accuracy : 90.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 200, loss : 0.3829, accuracy : 90.06\n",
      "iteration : 250, loss : 0.3770, accuracy : 90.40\n",
      "iteration : 300, loss : 0.3859, accuracy : 90.13\n",
      "Epoch : 299, training loss : 0.3866, training accuracy : 90.12, val loss : 0.3380, val accuracy : 91.71\n"
     ]
    }
   ],
   "source": [
    "# Learning rate part\n",
    "config = {'lr': 0.05, \n",
    "          'momentum': 0.9, \n",
    "          'weight_decay': 1e-4,\n",
    "          'alpha': 0.2}\n",
    "\n",
    "net = MobileNet().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
    "                     momentum=config['momentum'], \n",
    "                     weight_decay=config['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300, eta_min=0)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "best_val = -1.0\n",
    "best_cnn_model = None\n",
    "# learning_rate = []\n",
    "for epoch in range(300):\n",
    "    train_loss_, train_acc_ = train(epoch, net, criterion, trainloader, scheduler=scheduler, alpha=config['alpha'])\n",
    "    val_loss_, val_acc_ = val(epoch, net, criterion, valloader)\n",
    "    # new_lr = scheduler.get_last_lr()\n",
    "    \n",
    "    train_loss.append(train_loss_)\n",
    "    val_loss.append(val_loss_)\n",
    "    train_acc.append(train_acc_/100)\n",
    "    val_acc.append(val_acc_/100)\n",
    "    # learning_rate.append(new_lr)\n",
    "\n",
    "    if val_acc_ > best_val:\n",
    "        best_val = val_acc_\n",
    "        best_cnn_model = net\n",
    "        save_checkpoint(best_cnn_model, best_val, epoch)\n",
    "    \n",
    "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, val loss \" + \\\n",
    "      \": %0.4f, val accuracy : %2.2f\") % (epoch, train_loss_, train_acc_, val_loss_, val_acc_))\n",
    "    # print(\"The updated learning rate is: \", new_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ea3ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3394060706413245, 91.64)\n"
     ]
    }
   ],
   "source": [
    "val_test_acc = val(epoch, best_cnn_model, criterion, valloader)\n",
    "print(val_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d212e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy of the best model is:  (0.333588894084096, 91.57)\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(epoch, best_cnn_model, criterion, testloader)\n",
    "print(\"The test accuracy of the best model is: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bb6502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_loss_acc\n",
    "\n",
    "ta = [t.cpu().numpy() for t in train_acc]\n",
    "\n",
    "plot_loss_acc(train_loss, val_loss, ta, val_acc, \"loss_acc_diagram_mixup\")\n",
    "\n",
    "#plt.plot(range(len(train_loss)), train_loss, 'b')\n",
    "#plt.plot(range(len(test_loss)), test_loss, 'r')\n",
    "#plt.xlabel(\"Number of epochs\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "#plt.title(\"Logistic Regression: Loss vs Number of epochs\")\n",
    "#plt.legend(['train', 'test'])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2076fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot_learning_rate(learning_rate, figname):\n",
    "    x = np.arange(len(learning_rate))\n",
    "    max_lr = 1.0\n",
    "    min_lr = 0.0\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"learning rate\")\n",
    "    plt.plot(x, learning_rate, label=\"learning rate curve\")\n",
    "    plt.title(figname)\n",
    "    plt.savefig(os.path.join('./', figname))\n",
    "\n",
    "plt.clf()\n",
    "plot_learning_rate(learning_rate, \"lr_diagram_cosine_schedule_300_epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2726385e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is:  [1.9380317934024067, 1.6475613140069638, 1.560358148032484, 1.4024820049730733, 1.3128777940433247, 1.253766577655134, 1.150853529144019, 1.1231219340056275, 1.0638390836624292, 1.0043195134725054, 1.0155985690534306, 0.9646279515740209, 0.9246122246733108, 0.92717438364943, 0.8787821355147865, 0.8894828356112154, 0.8739283360040988, 0.8445886195467683, 0.8498075768208733, 0.8601226619066903, 0.7670159915003913, 0.8142044987446203, 0.7682687590202204, 0.741738436225885, 0.7513336721605386, 0.7295574149765527, 0.7067025245759434, 0.8154496096384031, 0.759247512244188, 0.7368058099533422, 0.7338492307133568, 0.7140759309164633, 0.7228552980449634, 0.705282554078026, 0.6711295556050901, 0.710843091955581, 0.7448324011728025, 0.6448681313104142, 0.6691442753274601, 0.672795738560704, 0.6879534619018293, 0.717868373845332, 0.7073381217999961, 0.6875257927198379, 0.6888261439796454, 0.6688861362279033, 0.655400006035075, 0.6621934574918625, 0.6403262472381226, 0.6360070123649634, 0.6675636886169736, 0.6573795811912884, 0.6234066752008737, 0.6427489180391589, 0.6151577932908893, 0.6158527494571842, 0.6354822835411889, 0.5908531197867455, 0.6051015006467557, 0.6010310204027179, 0.6177194622663644, 0.60635742407066, 0.640846801118348, 0.626517969222305, 0.6094420184009373, 0.6639218320147678, 0.5641124317535577, 0.5866914653121093, 0.6069591062280316, 0.5778825595356024, 0.6004864788188721, 0.6196036257159215, 0.5683456827133608, 0.5571330003083323, 0.573184117317771, 0.5609859842985583, 0.5887907195491151, 0.5879362331221278, 0.5283205455151229, 0.5703352128212064, 0.6299081922958072, 0.6077869445704424, 0.5578425250733241, 0.6060513277618459, 0.5710555067458473, 0.5577732720694983, 0.5756454675461347, 0.5436206364783996, 0.6022959535661787, 0.5098964916155361, 0.568235636304933, 0.5426363185905039, 0.5786088824795839, 0.5710012762667462, 0.5788552646105662, 0.6020434453083684, 0.5864665277849752, 0.5593647278202608, 0.5354613495496706, 0.5620040415812986, 0.5325971636742639, 0.5663294921548793, 0.5875414111339056, 0.5375850258139185, 0.5347685319975543, 0.5734107590759524, 0.5350641827470959, 0.5503083163449844, 0.5489339529277798, 0.5577012329769973, 0.5609040638794914, 0.5161347853632781, 0.5697900040771443, 0.5238429933976823, 0.5574212811482601, 0.5206611314937234, 0.5033390183335009, 0.5209977906970932, 0.4930072644600472, 0.5414650503653117, 0.5499164997852934, 0.5374587137882892, 0.53011711102467, 0.4952581250510467, 0.4868233087249457, 0.4976887833624602, 0.5482763593284466, 0.5111250429036328, 0.5354835964001405, 0.535566463899879, 0.5021014492066143, 0.5169971532191331, 0.4845933559746407, 0.4996115119812397, 0.48479218383471423, 0.5497924353237065, 0.5159923792266237, 0.5418350524998035, 0.4847994057384257, 0.5271856969537826, 0.5169662425633722, 0.5385778578170858, 0.4935174086354316, 0.5321284233571623, 0.499488515892444, 0.5391151558893462, 0.4783305276554233, 0.5203268811547052, 0.5024429817740529, 0.520506001068666, 0.5239680245066413, 0.4646734365681633, 0.4197883911299915, 0.5328260017826725, 0.4913225013334245, 0.5037667215459596, 0.5359555417284988, 0.508827614773529, 0.4650799448128802, 0.5200720101047438, 0.4671299329902322, 0.5029434511384454, 0.47079539809899684, 0.5120643368853738, 0.5111224126255217, 0.4454799669024091, 0.4831962439621361, 0.5049445170361679, 0.4540449393312105, 0.4976967037075196, 0.477660450545815, 0.4711382586788684, 0.4622528389001045, 0.47505055726788487, 0.516384147289105, 0.4532914961369845, 0.4254787778690124, 0.4258251278646314, 0.4590166845945076, 0.47894543260722733, 0.4444023870577375, 0.49372237812751496, 0.5103110560415366, 0.48630314245492745, 0.460604647290521, 0.4460340492522564, 0.468753137456152, 0.44959931236992295, 0.44370956803127504, 0.43343847491942084, 0.4066362362741805, 0.4252830820854384, 0.4147208066785131, 0.4676713396351749, 0.3933653541477605, 0.4038601034326842, 0.4590215625259252, 0.3932893311563201, 0.39968969362130324, 0.45906215056646077, 0.4460519747734189, 0.45329473271873744, 0.4347227949785967, 0.40729346422526164, 0.42685138069378875, 0.4337951912270924, 0.3658408474031324, 0.4536435554382769, 0.3830165768030305, 0.4413831475936174, 0.39364795197104924, 0.4241926320330106, 0.4059024784779444, 0.4519255662384553, 0.4540797206543434, 0.43777912316364814, 0.42821903082619484, 0.41586394533410836, 0.4507649618003089, 0.4013924671749111, 0.3689107134143789, 0.3949942708033295, 0.42984974449247526, 0.42725547796082547, 0.5154012506693816, 0.4220304718641296, 0.39502386935278416, 0.4002364448304422, 0.42453451302220907, 0.4068987449473395, 0.3925251608495764, 0.41926557260800523, 0.4529804381759117, 0.44127188948840024, 0.3567761150188744, 0.4247819368538563, 0.3829690974165563, 0.3855611677549351, 0.43292565605106254, 0.43870794674688085, 0.3988378502535649, 0.400549412205125, 0.35541387424230003, 0.4118225970848526, 0.39245030732926567, 0.41972654810571036, 0.453165734628626, 0.4132006184239619, 0.424511046490272, 0.37150199834581693, 0.41889693249112214, 0.37089057079183096, 0.3643439156665339, 0.4210440965983779, 0.40880971714279973, 0.40863080164149834, 0.45071369304080694, 0.397269363402755, 0.4144032842318971, 0.4118637954191373, 0.3607817814877322, 0.4298964553741744, 0.40407721258955787, 0.37651550229311515, 0.393434839171414, 0.3961456038530904, 0.4045016986738962, 0.3962529219817906, 0.39104444370456276, 0.3617934753282216, 0.39545561704255, 0.39808994295527805, 0.3787089437536431, 0.4026602355900188, 0.44949730838866186, 0.37579519176831877, 0.4022704895367078, 0.38354269757244985, 0.3775296810286911, 0.3915506010516562, 0.348072382047945, 0.3973150129464214, 0.4163465163664529, 0.40820612906799697, 0.45875902635048327, 0.369529472661767, 0.41276455244879584, 0.43841005800166927, 0.36584121874122977, 0.4402594669259335, 0.4084671486612445, 0.37308712461784765, 0.37873962083861185, 0.4222104964344896, 0.3862310626236097, 0.43912913813032567, 0.382503849376118, 0.40896206596470513, 0.38914094881910366, 0.3865681897994643] , with a min value of:  0.348072382047945\n",
      "Val loss is:  [1.5947484366501434, 1.4430042429815364, 1.2923106407817406, 1.1721098083484023, 1.0596059968199911, 0.9680381598351877, 0.9586841056618509, 0.8655452637732783, 0.8265978512884695, 0.7494181575654428, 0.7296242736562898, 0.726686621014076, 0.6800512365902527, 0.6508850383607647, 0.6515887688986862, 0.6031098422370379, 0.6270980857595613, 0.6253798358802554, 0.5694309465492828, 0.6379570029204404, 0.5658430668372142, 0.5961158151113535, 0.5579157833811603, 0.5285578875602046, 0.6174970793573162, 0.5310279821293263, 0.5526220168493972, 0.5344446248646024, 0.5689093968536281, 0.4900521217267724, 0.5479200274883946, 0.4891197451307804, 0.4779430982432788, 0.5159557639043543, 0.4873145528231995, 0.48978849189190926, 0.5257837285723868, 0.5028136949750441, 0.47350059393086014, 0.4642875134190427, 0.5164336799820767, 0.47285957804209067, 0.5097242703166189, 0.44339273370142224, 0.48916160683088666, 0.5226309039170229, 0.4323835950109023, 0.4635681887216206, 0.44590744715702685, 0.4726672725209707, 0.4728468720671497, 0.4957408954071093, 0.4563391055864624, 0.4538551048387455, 0.47257169062578225, 0.4590596794327603, 0.49165653163873696, 0.42153536085086535, 0.4629199033296561, 0.48219560897803004, 0.44055509529536285, 0.4377558220036422, 0.4831619564490982, 0.43513466285753855, 0.4303996989244147, 0.4736573918710781, 0.44315397852583777, 0.49754724661006205, 0.46725685498382474, 0.4610384591395342, 0.4469354379026196, 0.4355598318425915, 0.42898509132711193, 0.462551128260697, 0.43884646741649774, 0.4256340001202837, 0.43668871891649463, 0.4540630752527261, 0.4436234933666036, 0.44652954234352593, 0.44143981571438945, 0.43947023077856134, 0.4572366762764846, 0.4482319426687458, 0.4038099009024946, 0.41968813406515726, 0.418266048537025, 0.4407875854757768, 0.42940457108654556, 0.4411810598795927, 0.44082337712185293, 0.4042053279242938, 0.4348972681202466, 0.39390127323096313, 0.4344510251208197, 0.4023207490202747, 0.4419588556018057, 0.44591839290872404, 0.4138675250961811, 0.4556690850589849, 0.40963145354880565, 0.4190580957109415, 0.41523690578303757, 0.4316852703879151, 0.4062448247701307, 0.426788606975652, 0.4071109309981141, 0.4026195007034495, 0.40039428106591674, 0.4434168040752411, 0.393409535477433, 0.39080131450031375, 0.4403184639124931, 0.41919248956668226, 0.4367000394606892, 0.4187097417402871, 0.41816980835003187, 0.41330984253672104, 0.41616355090201657, 0.42318868278702604, 0.41270033562485176, 0.40503157165986076, 0.41947425147400624, 0.4164752456583554, 0.4121486601195758, 0.4202491659906846, 0.40915204528011856, 0.41387366418597066, 0.4192551638506636, 0.41560716040526763, 0.4374131952659993, 0.4191752783482588, 0.408497383417208, 0.40410949968838994, 0.41376992437658433, 0.4015634082540681, 0.42199914968466457, 0.41478122186057176, 0.4307159032625488, 0.40590347708026064, 0.4195087716172013, 0.4240230917930603, 0.3909891640083699, 0.4050479325689847, 0.38986060777796977, 0.42932113269461863, 0.4359373952014537, 0.406636134355883, 0.38623878718176974, 0.3879536873177637, 0.4100072744149196, 0.3897848548013953, 0.39682750573641135, 0.44059668462487717, 0.3911020618073548, 0.42470686005640634, 0.4066688333508335, 0.39556698010692115, 0.39767803251743317, 0.3868324745304977, 0.3831110466507417, 0.4101603038703339, 0.37797422575045236, 0.3957104716874376, 0.39087679472905174, 0.38209838165512566, 0.37191647423219076, 0.3904566117857076, 0.4115323073501828, 0.39152156325835213, 0.3734227649018734, 0.38398897930791104, 0.39894700899154323, 0.3958158704298961, 0.36604195587997196, 0.38243624832056744, 0.3903874155086807, 0.3871714826437491, 0.3982352818114848, 0.3893259214826777, 0.38624517506436457, 0.3973762134585199, 0.3917189327221883, 0.3747289057208013, 0.37414661068705063, 0.40639787667159794, 0.37862370738500284, 0.41061228494855423, 0.3768002945411054, 0.38890445345564734, 0.38061220698718784, 0.3894334303427346, 0.3661086506481412, 0.3919064464825618, 0.38808567991739584, 0.3868426188260694, 0.38026346832136565, 0.3791559555485279, 0.3720289327298539, 0.38650395749490474, 0.3657913507917259, 0.3867930249322819, 0.37101593179793296, 0.3695888062821159, 0.3693412949767294, 0.35999977324582355, 0.37918736191490027, 0.3624654447164717, 0.36272966012924535, 0.37882929210421407, 0.3678250821142257, 0.37893561622764493, 0.3920362276744239, 0.40977979196777825, 0.3779631737100927, 0.37873312623440464, 0.3942865329452708, 0.3623688164391095, 0.38815157285219504, 0.3637352920597113, 0.3519259298904033, 0.3760031991744343, 0.3604348433923118, 0.37912770464450496, 0.35574886929008026, 0.3621348333132418, 0.3831908636832539, 0.3588577431591251, 0.3726145879754537, 0.3725675089827067, 0.3657918955329098, 0.37992769894720635, 0.35067143262941625, 0.3662307158678393, 0.3521531361945068, 0.35104056664660005, 0.36681897598731367, 0.35137689434274844, 0.37808359989637064, 0.35490556381925753, 0.3435688763856888, 0.3436845872975603, 0.3632828540439847, 0.36400012294702894, 0.3381603685549543, 0.3495966305461111, 0.35296691293957866, 0.3441125222399265, 0.3460694232696219, 0.3396620044979868, 0.3447973594258103, 0.3472085721507857, 0.37306552012509936, 0.3497647629885734, 0.33839101542400407, 0.36009681507756436, 0.3605621151531799, 0.3572670445789265, 0.3510795071532455, 0.3405804067284246, 0.3607225214378743, 0.3371599067615557, 0.34925223396548744, 0.3421986838684806, 0.35468976671182656, 0.3354090708720533, 0.35158661913268174, 0.34738044289848474, 0.3392051884645148, 0.33706349511689776, 0.33885642282570466, 0.33905418933946874, 0.3369722995388357, 0.35921946960159495, 0.32875689392602897, 0.3446504201716449, 0.34424509881417964, 0.3407450144803977, 0.347241808153406, 0.3438498202381255, 0.35707759611968753, 0.34487522336878357, 0.35579028140894975, 0.34652603842035123, 0.35019391850580145, 0.3433745216719712, 0.3479574748986884, 0.3298928958328464, 0.34694965655290627, 0.3429841693443588, 0.3363993846917454, 0.3400512788491913, 0.33830890606475783, 0.352086907512025, 0.34966015796872635, 0.3749800392344028, 0.33960814245893983, 0.3444409470392179, 0.3354255653257611, 0.3380198425884488] , with a min value of:  0.32875689392602897\n",
      "Train accuracy is:  [array(0.29363808, dtype=float32), array(0.42277867, dtype=float32), array(0.48525202, dtype=float32), array(0.53643054, dtype=float32), array(0.5812519, dtype=float32), array(0.60731274, dtype=float32), array(0.63582844, dtype=float32), array(0.66138935, dtype=float32), array(0.67273617, dtype=float32), array(0.6954643, dtype=float32), array(0.712125, dtype=float32), array(0.71454626, dtype=float32), array(0.7127639, dtype=float32), array(0.7263365, dtype=float32), array(0.7337734, dtype=float32), array(0.745769, dtype=float32), array(0.7465995, dtype=float32), array(0.74622643, dtype=float32), array(0.7381728, dtype=float32), array(0.76693684, dtype=float32), array(0.76629, dtype=float32), array(0.7662248, dtype=float32), array(0.7567733, dtype=float32), array(0.77773964, dtype=float32), array(0.76199675, dtype=float32), array(0.7796576, dtype=float32), array(0.7768087, dtype=float32), array(0.79064655, dtype=float32), array(0.7908055, dtype=float32), array(0.790572, dtype=float32), array(0.7915795, dtype=float32), array(0.78532994, dtype=float32), array(0.7806868, dtype=float32), array(0.7900574, dtype=float32), array(0.795633, dtype=float32), array(0.7896517, dtype=float32), array(0.7907675, dtype=float32), array(0.80830234, dtype=float32), array(0.800401, dtype=float32), array(0.8031931, dtype=float32), array(0.79781234, dtype=float32), array(0.8082288, dtype=float32), array(0.81364083, dtype=float32), array(0.8043231, dtype=float32), array(0.81730163, dtype=float32), array(0.8046862, dtype=float32), array(0.819369, dtype=float32), array(0.81412464, dtype=float32), array(0.8239873, dtype=float32), array(0.8097951, dtype=float32), array(0.81762403, dtype=float32), array(0.8193836, dtype=float32), array(0.81512314, dtype=float32), array(0.8217108, dtype=float32), array(0.8199571, dtype=float32), array(0.8230137, dtype=float32), array(0.8153371, dtype=float32), array(0.82053417, dtype=float32), array(0.82962704, dtype=float32), array(0.82384497, dtype=float32), array(0.81986487, dtype=float32), array(0.82472205, dtype=float32), array(0.81739205, dtype=float32), array(0.8269829, dtype=float32), array(0.815205, dtype=float32), array(0.81551105, dtype=float32), array(0.82783705, dtype=float32), array(0.82883, dtype=float32), array(0.8341663, dtype=float32), array(0.8348845, dtype=float32), array(0.847405, dtype=float32), array(0.83317906, dtype=float32), array(0.8277108, dtype=float32), array(0.82872146, dtype=float32), array(0.8250461, dtype=float32), array(0.8339326, dtype=float32), array(0.8230096, dtype=float32), array(0.84333706, dtype=float32), array(0.8350295, dtype=float32), array(0.85044134, dtype=float32), array(0.84080845, dtype=float32), array(0.86296415, dtype=float32), array(0.8463005, dtype=float32), array(0.83630294, dtype=float32), array(0.83512425, dtype=float32), array(0.840213, dtype=float32), array(0.8365312, dtype=float32), array(0.83557355, dtype=float32), array(0.85572124, dtype=float32), array(0.8537108, dtype=float32), array(0.8451022, dtype=float32), array(0.8451941, dtype=float32), array(0.85914546, dtype=float32), array(0.84316975, dtype=float32), array(0.8513707, dtype=float32), array(0.84905803, dtype=float32), array(0.84301716, dtype=float32), array(0.835233, dtype=float32), array(0.8469327, dtype=float32), array(0.84468627, dtype=float32), array(0.84557205, dtype=float32), array(0.83450603, dtype=float32), array(0.85052997, dtype=float32), array(0.8480655, dtype=float32), array(0.8505798, dtype=float32), array(0.8512843, dtype=float32), array(0.8459223, dtype=float32), array(0.84414566, dtype=float32), array(0.86792725, dtype=float32), array(0.8396244, dtype=float32), array(0.8435041, dtype=float32), array(0.8666993, dtype=float32), array(0.8550152, dtype=float32), array(0.8536165, dtype=float32), array(0.85751766, dtype=float32), array(0.84147286, dtype=float32), array(0.8481095, dtype=float32), array(0.85182846, dtype=float32), array(0.8435639, dtype=float32), array(0.8649418, dtype=float32), array(0.8521258, dtype=float32), array(0.8595337, dtype=float32), array(0.8696092, dtype=float32), array(0.8453733, dtype=float32), array(0.8488526, dtype=float32), array(0.8607682, dtype=float32), array(0.86397916, dtype=float32), array(0.8516941, dtype=float32), array(0.86069703, dtype=float32), array(0.8611569, dtype=float32), array(0.85715365, dtype=float32), array(0.8612276, dtype=float32), array(0.8695128, dtype=float32), array(0.86619854, dtype=float32), array(0.8634141, dtype=float32), array(0.87091726, dtype=float32), array(0.8709701, dtype=float32), array(0.8651151, dtype=float32), array(0.85932773, dtype=float32), array(0.85712147, dtype=float32), array(0.8808788, dtype=float32), array(0.8668732, dtype=float32), array(0.8764995, dtype=float32), array(0.87295604, dtype=float32), array(0.8799983, dtype=float32), array(0.8627108, dtype=float32), array(0.8609138, dtype=float32), array(0.8692243, dtype=float32), array(0.86489666, dtype=float32), array(0.88034093, dtype=float32), array(0.88002217, dtype=float32), array(0.8639941, dtype=float32), array(0.8672488, dtype=float32), array(0.86623174, dtype=float32), array(0.8717979, dtype=float32), array(0.8532385, dtype=float32), array(0.8811584, dtype=float32), array(0.8769812, dtype=float32), array(0.87028337, dtype=float32), array(0.86857617, dtype=float32), array(0.8692684, dtype=float32), array(0.8689753, dtype=float32), array(0.8606715, dtype=float32), array(0.86128086, dtype=float32), array(0.86389005, dtype=float32), array(0.8614715, dtype=float32), array(0.8590713, dtype=float32), array(0.87544644, dtype=float32), array(0.8829076, dtype=float32), array(0.8782147, dtype=float32), array(0.86895835, dtype=float32), array(0.8901584, dtype=float32), array(0.88007575, dtype=float32), array(0.8773673, dtype=float32), array(0.87948716, dtype=float32), array(0.87276304, dtype=float32), array(0.88079756, dtype=float32), array(0.8924906, dtype=float32), array(0.8854396, dtype=float32), array(0.89313954, dtype=float32), array(0.8896447, dtype=float32), array(0.8810333, dtype=float32), array(0.8709239, dtype=float32), array(0.8720579, dtype=float32), array(0.8846088, dtype=float32), array(0.8806276, dtype=float32), array(0.87452507, dtype=float32), array(0.86076695, dtype=float32), array(0.9013918, dtype=float32), array(0.89527154, dtype=float32), array(0.8761018, dtype=float32), array(0.8942972, dtype=float32), array(0.887975, dtype=float32), array(0.89436084, dtype=float32), array(0.89184284, dtype=float32), array(0.8750259, dtype=float32), array(0.8820805, dtype=float32), array(0.8839523, dtype=float32), array(0.88142216, dtype=float32), array(0.8850837, dtype=float32), array(0.89213085, dtype=float32), array(0.8859693, dtype=float32), array(0.8862945, dtype=float32), array(0.89832526, dtype=float32), array(0.87828934, dtype=float32), array(0.8637013, dtype=float32), array(0.8815847, dtype=float32), array(0.8989122, dtype=float32), array(0.88424563, dtype=float32), array(0.89612585, dtype=float32), array(0.9026569, dtype=float32), array(0.8744598, dtype=float32), array(0.88995856, dtype=float32), array(0.88713413, dtype=float32), array(0.9000701, dtype=float32), array(0.89407545, dtype=float32), array(0.8981044, dtype=float32), array(0.89179593, dtype=float32), array(0.897144, dtype=float32), array(0.88805664, dtype=float32), array(0.87538373, dtype=float32), array(0.8790383, dtype=float32), array(0.885504, dtype=float32), array(0.8903992, dtype=float32), array(0.8852889, dtype=float32), array(0.87584156, dtype=float32), array(0.8943828, dtype=float32), array(0.8793636, dtype=float32), array(0.8867875, dtype=float32), array(0.9030169, dtype=float32), array(0.88161075, dtype=float32), array(0.89031506, dtype=float32), array(0.9020026, dtype=float32), array(0.8930107, dtype=float32), array(0.8845616, dtype=float32), array(0.89099133, dtype=float32), array(0.88068694, dtype=float32), array(0.88390183, dtype=float32), array(0.8948262, dtype=float32), array(0.88958144, dtype=float32), array(0.88931763, dtype=float32), array(0.91156125, dtype=float32), array(0.9065027, dtype=float32), array(0.89652145, dtype=float32), array(0.90450656, dtype=float32), array(0.89957947, dtype=float32), array(0.89609, dtype=float32), array(0.8965503, dtype=float32), array(0.88303983, dtype=float32), array(0.8891586, dtype=float32), array(0.9064748, dtype=float32), array(0.8958413, dtype=float32), array(0.8892201, dtype=float32), array(0.89953405, dtype=float32), array(0.9034001, dtype=float32), array(0.8897968, dtype=float32), array(0.9086043, dtype=float32), array(0.90245795, dtype=float32), array(0.89612985, dtype=float32), array(0.88198364, dtype=float32), array(0.8868896, dtype=float32), array(0.8891092, dtype=float32), array(0.8874074, dtype=float32), array(0.89538765, dtype=float32), array(0.8944975, dtype=float32), array(0.89309233, dtype=float32), array(0.91409683, dtype=float32), array(0.8954099, dtype=float32), array(0.89306486, dtype=float32), array(0.9040108, dtype=float32), array(0.8804477, dtype=float32), array(0.89530903, dtype=float32), array(0.9043116, dtype=float32), array(0.89949965, dtype=float32), array(0.90088767, dtype=float32), array(0.88563734, dtype=float32), array(0.90768445, dtype=float32), array(0.90949947, dtype=float32), array(0.8939531, dtype=float32), array(0.8851896, dtype=float32), array(0.8952571, dtype=float32), array(0.89104575, dtype=float32), array(0.8946078, dtype=float32), array(0.89081824, dtype=float32), array(0.8958595, dtype=float32), array(0.91161793, dtype=float32), array(0.8905024, dtype=float32), array(0.8874909, dtype=float32), array(0.88971883, dtype=float32), array(0.89707136, dtype=float32), array(0.90555626, dtype=float32), array(0.8749265, dtype=float32), array(0.89941055, dtype=float32), array(0.9017605, dtype=float32), array(0.8812182, dtype=float32), array(0.8970306, dtype=float32), array(0.8918716, dtype=float32), array(0.9054947, dtype=float32), array(0.89846855, dtype=float32), array(0.8913488, dtype=float32)] , with a max value of:  0.91409683\n",
      "Val accuracy is:  [0.4129, 0.4806, 0.5357999999999999, 0.5911, 0.6332, 0.6744, 0.6801999999999999, 0.7137, 0.7201000000000001, 0.7518, 0.7578, 0.7584000000000001, 0.7775, 0.7863, 0.7867000000000001, 0.8003, 0.7934, 0.7957, 0.8164, 0.7972, 0.8148000000000001, 0.8114, 0.8199, 0.8295, 0.7955, 0.826, 0.8222, 0.8321999999999999, 0.8341, 0.8440000000000001, 0.8239, 0.8358, 0.8442000000000001, 0.8428, 0.8431000000000001, 0.841, 0.8351000000000001, 0.8405, 0.8445, 0.8478, 0.8366, 0.8537, 0.8556, 0.8593000000000001, 0.8438, 0.8473, 0.8606, 0.8545999999999999, 0.8576, 0.851, 0.847, 0.8434999999999999, 0.8588, 0.8559, 0.8499, 0.8565, 0.8561, 0.8637, 0.8551000000000001, 0.8573000000000001, 0.8623999999999999, 0.8613, 0.8499, 0.8658, 0.865, 0.8502, 0.8599, 0.8486, 0.8593000000000001, 0.8645999999999999, 0.8625, 0.863, 0.867, 0.8526, 0.8667, 0.8673000000000001, 0.8704000000000001, 0.8684000000000001, 0.8652, 0.8632, 0.8676999999999999, 0.8641, 0.8584, 0.8681, 0.8736, 0.8695, 0.8726999999999999, 0.8634999999999999, 0.8649, 0.8609, 0.8704000000000001, 0.8762000000000001, 0.8695, 0.8792, 0.8684999999999999, 0.8775, 0.8697, 0.8668, 0.8736, 0.8626, 0.8763, 0.8703, 0.8771, 0.8715999999999999, 0.8765999999999999, 0.8743000000000001, 0.877, 0.8806, 0.8791, 0.8684999999999999, 0.8852, 0.8795999999999999, 0.8663, 0.8759999999999999, 0.8697, 0.8744, 0.8754000000000001, 0.8740000000000001, 0.8797, 0.8802, 0.8782, 0.8805, 0.8734000000000001, 0.8778, 0.8794, 0.8692, 0.8772, 0.8793000000000001, 0.8801000000000001, 0.8754000000000001, 0.8726999999999999, 0.8784000000000001, 0.8774, 0.8831, 0.8779, 0.8822, 0.8759999999999999, 0.8717, 0.8801000000000001, 0.8826999999999999, 0.8854000000000001, 0.8837999999999999, 0.8872, 0.8847, 0.8847, 0.8837, 0.8791, 0.8798999999999999, 0.8892, 0.8861, 0.8828, 0.8826999999999999, 0.8894, 0.8845000000000001, 0.8908, 0.8829, 0.8868, 0.8886, 0.8831, 0.8884000000000001, 0.8887999999999999, 0.8872, 0.89, 0.8884000000000001, 0.8902, 0.889, 0.8935, 0.8896, 0.8869, 0.8912, 0.8954000000000001, 0.8883, 0.8870999999999999, 0.8863, 0.8946, 0.8922, 0.8884000000000001, 0.8952, 0.892, 0.8904000000000001, 0.8917, 0.8919, 0.8943000000000001, 0.8968, 0.8954000000000001, 0.8876999999999999, 0.8931999999999999, 0.895, 0.8954000000000001, 0.8902, 0.8923000000000001, 0.8914, 0.8972, 0.8937999999999999, 0.8946999999999999, 0.8948999999999999, 0.8962, 0.8979, 0.9028, 0.8946, 0.9013, 0.8954000000000001, 0.8976999999999999, 0.8981999999999999, 0.9007999999999999, 0.9031999999999999, 0.8992, 0.9007999999999999, 0.9033, 0.8972, 0.9036, 0.9005, 0.902, 0.8966, 0.8983, 0.9035, 0.9002, 0.9047, 0.9027, 0.9031999999999999, 0.9064, 0.9020999999999999, 0.9068999999999999, 0.9029, 0.9068, 0.9042, 0.9017000000000001, 0.9059, 0.9070999999999999, 0.9028, 0.9054000000000001, 0.9070999999999999, 0.9081999999999999, 0.9053, 0.9107, 0.91, 0.9092, 0.9085, 0.9066, 0.9077, 0.9094, 0.9124, 0.9061, 0.9073, 0.9134, 0.9131, 0.9094, 0.9118, 0.9115000000000001, 0.9120999999999999, 0.9113, 0.9104000000000001, 0.9112, 0.9128000000000001, 0.9142, 0.9109999999999999, 0.9087000000000001, 0.9092, 0.9112, 0.9131, 0.9094, 0.9170999999999999, 0.9102, 0.9120999999999999, 0.9103, 0.9137000000000001, 0.909, 0.9104000000000001, 0.9133, 0.9138, 0.9129, 0.914, 0.9124, 0.9120999999999999, 0.9159, 0.9168999999999999, 0.9157, 0.9138, 0.9128000000000001, 0.9131999999999999, 0.9118999999999999, 0.9133, 0.9133, 0.9129, 0.9115000000000001, 0.9147, 0.9107, 0.9145, 0.9112, 0.912, 0.9118, 0.9131, 0.9156, 0.9116, 0.9158, 0.9118, 0.9129999999999999, 0.9123, 0.9142, 0.9170999999999999] , with a max value of:  0.9170999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loss is: \", train_loss, \", with a min value of: \", min(train_loss))\n",
    "print(\"Val loss is: \", val_loss, \", with a min value of: \", min(val_loss))\n",
    "print(\"Train accuracy is: \", ta, \", with a max value of: \", max(ta))\n",
    "print(\"Val accuracy is: \", val_acc, \", with a max value of: \", max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73b290e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb15a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
